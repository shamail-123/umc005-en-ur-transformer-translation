{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba458b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e0e518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All praise be to Allah alone , the Sustainer of all the worlds .\n",
      "Most Compassionate , Ever - Merciful .\n",
      "Master of the Day of Judgment .\n",
      "( O Allah ! ) You alone do we worship and to You alone we look for help .\n",
      "Show us the straight path .\n",
      "The path of those upon whom You have bestowed Your favours .\n",
      "Not of those who have been afflicted with wrath and nor of those who have gone astray .\n",
      "Alif , Lam , Mim . ( Only Allah and the Messenger know the real meaning . )\n",
      "( This is ) the Glorious Book in whi\n",
      "﻿سب تعریفیں اللہ ہی کے لئے ہیں جو تمام جہانوں کی پرورش فرمانے والا ہے ۔\n",
      "نہایت مہربان بہت رحم فرمانے والا ہے ۔\n",
      "روز جزا کا مالک ہے ۔\n",
      "اے اللہ ! ہم تیری ہی عبادت کرتے ہیں اور ہم تجھ ہی سے مدد چاہتے ہیں ۔\n",
      "ہمیں سیدھا راستہ دکھا ۔\n",
      "ان لوگوں کا راستہ جن پر تو نے انعام فرمایا ۔\n",
      "ان لوگوں کا نہیں جن پر غضب کیا گیا ہے اور نہ ہی گمراہوں کا ۔\n",
      "الف لام میم حقیقی معنی اﷲ اور رسول صلی اللہ علیہ وآلہ وسلم ہی بہتر جانتے ہیں ۔\n",
      "یہ وہ عظیم کتاب ہے جس میں کسی شک کی گنجائش نہیں ، یہ پرہیزگاروں کے لئے ہدایت ہے ۔\n",
      "جو غیب پر\n",
      "﻿The book of the generation of Jesus Christ , the son of David , the son of Abraham .\n",
      "Abraham begat Isaac ; and Isaac begat Jacob ; and Jacob begat Judas and his brethren .\n",
      "And Judas begat Phares and Zara of Thamar ; and Phares begat Esrom ; and Esrom begat Aram .\n",
      "And Aram begat Aminadab ; and Aminadab begat Naasson ; and Naasson begat Salmon .\n",
      "And Salmon begat Booz of Rachab ; and Booz begat Obed of Ruth ; and Obed begat Jesse .\n",
      "And Jesse begat David the king ; and David the king begat Solomon \n",
      "﻿سب تعریفیں اللہ ہی کے لئے ہیں جو تمام جہانوں کی پرورش فرمانے والا ہے ۔\n",
      "نہایت مہربان بہت رحم فرمانے والا ہے ۔\n",
      "روز جزا کا مالک ہے ۔\n",
      "اے اللہ ! ہم تیری ہی عبادت کرتے ہیں اور ہم تجھ ہی سے مدد چاہتے ہیں ۔\n",
      "ہمیں سیدھا راستہ دکھا ۔\n",
      "ان لوگوں کا راستہ جن پر تو نے انعام فرمایا ۔\n",
      "ان لوگوں کا نہیں جن پر غضب کیا گیا ہے اور نہ ہی گمراہوں کا ۔\n",
      "الف لام میم حقیقی معنی اﷲ اور رسول صلی اللہ علیہ وآلہ وسلم ہی بہتر جانتے ہیں ۔\n",
      "یہ وہ عظیم کتاب ہے جس میں کسی شک کی گنجائش نہیں ، یہ پرہیزگاروں کے لئے ہدایت ہے ۔\n",
      "جو غیب پر\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Base data folder\n",
    "base_path = 'C:\\\\Users\\\\aamir\\\\Downloads\\\\umc005-corpus'\n",
    "\n",
    "# Subfolders\n",
    "folders = ['quran', 'bible']\n",
    "\n",
    "# File names\n",
    "files = ['train.en', 'train.ur', 'test.en', 'test.ur', 'dev.en', 'dev.ur']\n",
    "\n",
    "# Dictionary to store the data\n",
    "data = {}\n",
    "\n",
    "# Loop through each folder and file\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    data[folder] = {}\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            data[folder][file_name] = content\n",
    "        else:\n",
    "            print(f\"Warning: {file_path} not found.\")\n",
    "\n",
    "# Example: accessing Quran's training English data\n",
    "print(data['quran']['train.en'][:500])  # Print first 500 characters\n",
    "print(data['quran']['train.ur'][:500])  \n",
    "print(data['bible']['train.en'][:500])  # Print first 500 characters\n",
    "print(data['quran']['train.ur'][:500])  # Print first 500 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6373bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Quran English Example: ['▁All', '▁praise', '▁be', '▁to', '▁Allah', '▁a', 'lone', '▁,', '▁the', '▁S', 'us', 't', 'ain', 'er', '▁of', '▁all', '▁the', '▁world', 's', '▁.']\n",
      "Tokenized Quran Urdu Example: ['▁سب', '▁تعریف', 'یں', '▁ال', 'لہ', '▁ہی', '▁کے', '▁ل', 'ئے', '▁ہیں', '▁جو', '▁تم', 'ام', '▁جہان', 'وں', '▁کی', '▁پر', 'ور', 'ش', '▁فرما', 'نے', '▁والا', '▁ہے', '▁۔']\n",
      "Tokenized Bible English Example: ['▁The', '▁bo', 'ok', '▁of', '▁the', '▁generation', '▁of', '▁Jes', 'us', '▁Christ', '▁,', '▁the', '▁son', '▁of', '▁', 'D', 'avi', 'd', '▁,', '▁the', '▁son', '▁of', '▁Abraham', '▁.']\n",
      "Tokenized Bible Urdu Example: ['▁یسو', 'ع', '▁مسیح', '▁اب', 'ن', '▁د', 'ا', 'ود', '▁اب', 'ن', '▁اب', 'ر', 'ہ', 'ام', '▁کا', '▁نسب', '▁نام', 'ہ']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Base data folder\n",
    "base_path = 'C:\\\\Users\\\\aamir\\\\Downloads\\\\umc005-corpus'\n",
    "folders = ['quran', 'bible']\n",
    "files = ['train.en', 'train.ur', 'test.en', 'test.ur', 'dev.en', 'dev.ur']\n",
    "\n",
    "# Dictionary to store the data\n",
    "data = {}\n",
    "\n",
    "# Reading and cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.strip()  # Remove leading/trailing spaces\n",
    "    text = text.replace('\\u200c', '')  # Remove zero-width non-joiner (Urdu issue)\n",
    "    text = text.replace('\\n', ' ')  # Replace newlines with space\n",
    "    text = ' '.join(text.split())  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Load all files\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    data[folder] = {}\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "            lines = [clean_text(line) for line in lines]\n",
    "            data[folder][file_name] = lines\n",
    "        else:\n",
    "            print(f\"Warning: {file_path} not found.\")\n",
    "\n",
    "# Write temporary files for BPE training\n",
    "os.makedirs('temp', exist_ok=True)\n",
    "for lang in ['en', 'ur']:\n",
    "    combined_data = []\n",
    "    for folder in folders:\n",
    "        for split in ['train', 'test', 'dev']:\n",
    "            combined_data += data[folder][f'{split}.{lang}']\n",
    "    with open(f'temp/combined_{lang}.txt', 'w', encoding='utf-8') as f:\n",
    "        for line in combined_data:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "\n",
    "# Train BPE model (one for English, one for Urdu) - vocab size adjusted!\n",
    "def get_vocab_limit(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    unique_chars = set(''.join(lines))   # Approximation (characters/subwords)\n",
    "    return max(1000, int(len(unique_chars) * 2))  # Simple logic: 2x unique chars\n",
    "\n",
    "vocab_size_en = min(5000, get_vocab_limit('temp/combined_en.txt'))\n",
    "vocab_size_ur = min(4500, get_vocab_limit('temp/combined_ur.txt'))\n",
    "\n",
    "spm.SentencePieceTrainer.train(input='temp/combined_en.txt', model_prefix='bpe_en', vocab_size=vocab_size_en)\n",
    "spm.SentencePieceTrainer.train(input='temp/combined_ur.txt', model_prefix='bpe_ur', vocab_size=vocab_size_ur)\n",
    "\n",
    "\n",
    "# Load trained tokenizers\n",
    "tokenizer_en = spm.SentencePieceProcessor(model_file='bpe_en.model')\n",
    "tokenizer_ur = spm.SentencePieceProcessor(model_file='bpe_ur.model')\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_data = {}\n",
    "\n",
    "for folder in folders:\n",
    "    tokenized_data[folder] = {}\n",
    "    for split in ['train', 'test', 'dev']:\n",
    "        en_lines = data[folder][f'{split}.en']\n",
    "        ur_lines = data[folder][f'{split}.ur']\n",
    "        \n",
    "        # Ensure alignment\n",
    "        assert len(en_lines) == len(ur_lines), f\"Mismatch in {folder}/{split}\"\n",
    "        \n",
    "        tokenized_en = [tokenizer_en.encode(line, out_type=str) for line in en_lines]\n",
    "        tokenized_ur = [tokenizer_ur.encode(line, out_type=str) for line in ur_lines]\n",
    "        \n",
    "        tokenized_data[folder][f'{split}.en'] = tokenized_en\n",
    "        tokenized_data[folder][f'{split}.ur'] = tokenized_ur\n",
    "\n",
    "# Example: print first tokenized sentence\n",
    "print(\"Tokenized Quran English Example:\", tokenized_data['quran']['train.en'][0])\n",
    "print(\"Tokenized Quran Urdu Example:\", tokenized_data['quran']['train.ur'][0])\n",
    "print(\"Tokenized Bible English Example:\", tokenized_data['bible']['train.en'][0])\n",
    "print(\"Tokenized Bible Urdu Example:\", tokenized_data['bible']['train.ur'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da02b148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "572b8c99",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35a9ca0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15\n",
      "  Batch 1/210 - Loss: 7.1009\n",
      "  Batch 2/210 - Loss: 6.8162\n",
      "  Batch 3/210 - Loss: 6.6317\n",
      "  Batch 4/210 - Loss: 6.5375\n",
      "  Batch 5/210 - Loss: 6.4502\n",
      "  Batch 6/210 - Loss: 6.3997\n",
      "  Batch 7/210 - Loss: 6.3508\n",
      "  Batch 8/210 - Loss: 6.3275\n",
      "  Batch 9/210 - Loss: 6.2932\n",
      "  Batch 10/210 - Loss: 6.2760\n",
      "  Batch 11/210 - Loss: 6.2782\n",
      "  Batch 12/210 - Loss: 6.2384\n",
      "  Batch 13/210 - Loss: 6.2619\n",
      "  Batch 14/210 - Loss: 6.2240\n",
      "  Batch 15/210 - Loss: 6.1458\n",
      "  Batch 16/210 - Loss: 6.1652\n",
      "  Batch 17/210 - Loss: 6.1257\n",
      "  Batch 18/210 - Loss: 6.1259\n",
      "  Batch 19/210 - Loss: 6.1302\n",
      "  Batch 20/210 - Loss: 6.0971\n",
      "  Batch 21/210 - Loss: 6.0919\n",
      "  Batch 22/210 - Loss: 6.0332\n",
      "  Batch 23/210 - Loss: 6.0520\n",
      "  Batch 24/210 - Loss: 6.0212\n",
      "  Batch 25/210 - Loss: 5.9839\n",
      "  Batch 26/210 - Loss: 6.0013\n",
      "  Batch 27/210 - Loss: 5.9597\n",
      "  Batch 28/210 - Loss: 5.9726\n",
      "  Batch 29/210 - Loss: 5.8964\n",
      "  Batch 30/210 - Loss: 5.9275\n",
      "  Batch 31/210 - Loss: 5.9127\n",
      "  Batch 32/210 - Loss: 5.8933\n",
      "  Batch 33/210 - Loss: 5.8966\n",
      "  Batch 34/210 - Loss: 5.9010\n",
      "  Batch 35/210 - Loss: 5.8347\n",
      "  Batch 36/210 - Loss: 5.8470\n",
      "  Batch 37/210 - Loss: 5.8012\n",
      "  Batch 38/210 - Loss: 5.8555\n",
      "  Batch 39/210 - Loss: 5.8197\n",
      "  Batch 40/210 - Loss: 5.7632\n",
      "  Batch 41/210 - Loss: 5.7742\n",
      "  Batch 42/210 - Loss: 5.7440\n",
      "  Batch 43/210 - Loss: 5.7776\n",
      "  Batch 44/210 - Loss: 5.7525\n",
      "  Batch 45/210 - Loss: 5.7057\n",
      "  Batch 46/210 - Loss: 5.7092\n",
      "  Batch 47/210 - Loss: 5.6324\n",
      "  Batch 48/210 - Loss: 5.6424\n",
      "  Batch 49/210 - Loss: 5.6484\n",
      "  Batch 50/210 - Loss: 5.6114\n",
      "  Batch 51/210 - Loss: 5.5716\n",
      "  Batch 52/210 - Loss: 5.6273\n",
      "  Batch 53/210 - Loss: 5.6139\n",
      "  Batch 54/210 - Loss: 5.6666\n",
      "  Batch 55/210 - Loss: 5.6108\n",
      "  Batch 56/210 - Loss: 5.5517\n",
      "  Batch 57/210 - Loss: 5.5865\n",
      "  Batch 58/210 - Loss: 5.5072\n",
      "  Batch 59/210 - Loss: 5.5485\n",
      "  Batch 60/210 - Loss: 5.5337\n",
      "  Batch 61/210 - Loss: 5.4260\n",
      "  Batch 62/210 - Loss: 5.4724\n",
      "  Batch 63/210 - Loss: 5.4862\n",
      "  Batch 64/210 - Loss: 5.4269\n",
      "  Batch 65/210 - Loss: 5.4384\n",
      "  Batch 66/210 - Loss: 5.3634\n",
      "  Batch 67/210 - Loss: 5.4296\n",
      "  Batch 68/210 - Loss: 5.4391\n",
      "  Batch 69/210 - Loss: 5.3997\n",
      "  Batch 70/210 - Loss: 5.3501\n",
      "  Batch 71/210 - Loss: 5.4303\n",
      "  Batch 72/210 - Loss: 5.3567\n",
      "  Batch 73/210 - Loss: 5.3823\n",
      "  Batch 74/210 - Loss: 5.2877\n",
      "  Batch 75/210 - Loss: 5.3082\n",
      "  Batch 76/210 - Loss: 5.3193\n",
      "  Batch 77/210 - Loss: 5.3089\n",
      "  Batch 78/210 - Loss: 5.3251\n",
      "  Batch 79/210 - Loss: 5.2226\n",
      "  Batch 80/210 - Loss: 5.2357\n",
      "  Batch 81/210 - Loss: 5.2314\n",
      "  Batch 82/210 - Loss: 5.2191\n",
      "  Batch 83/210 - Loss: 5.2112\n",
      "  Batch 84/210 - Loss: 5.1853\n",
      "  Batch 85/210 - Loss: 5.2192\n",
      "  Batch 86/210 - Loss: 5.2111\n",
      "  Batch 87/210 - Loss: 5.1445\n",
      "  Batch 88/210 - Loss: 5.2205\n",
      "  Batch 89/210 - Loss: 5.1504\n",
      "  Batch 90/210 - Loss: 5.1480\n",
      "  Batch 91/210 - Loss: 5.1582\n",
      "  Batch 92/210 - Loss: 5.2034\n",
      "  Batch 93/210 - Loss: 5.1623\n",
      "  Batch 94/210 - Loss: 5.1164\n",
      "  Batch 95/210 - Loss: 5.0893\n",
      "  Batch 96/210 - Loss: 5.1034\n",
      "  Batch 97/210 - Loss: 5.0776\n",
      "  Batch 98/210 - Loss: 5.1360\n",
      "  Batch 99/210 - Loss: 5.1288\n",
      "  Batch 100/210 - Loss: 5.0135\n",
      "  Batch 101/210 - Loss: 5.0742\n",
      "  Batch 102/210 - Loss: 5.0080\n",
      "  Batch 103/210 - Loss: 5.0654\n",
      "  Batch 104/210 - Loss: 5.0094\n",
      "  Batch 105/210 - Loss: 5.0408\n",
      "  Batch 106/210 - Loss: 5.0361\n",
      "  Batch 107/210 - Loss: 4.9760\n",
      "  Batch 108/210 - Loss: 5.1154\n",
      "  Batch 109/210 - Loss: 5.0303\n",
      "  Batch 110/210 - Loss: 4.9883\n",
      "  Batch 111/210 - Loss: 4.9909\n",
      "  Batch 112/210 - Loss: 5.0357\n",
      "  Batch 113/210 - Loss: 4.9138\n",
      "  Batch 114/210 - Loss: 4.8904\n",
      "  Batch 115/210 - Loss: 4.9764\n",
      "  Batch 116/210 - Loss: 4.9315\n",
      "  Batch 117/210 - Loss: 4.9004\n",
      "  Batch 118/210 - Loss: 4.8501\n",
      "  Batch 119/210 - Loss: 4.8994\n",
      "  Batch 120/210 - Loss: 4.8771\n",
      "  Batch 121/210 - Loss: 4.8930\n",
      "  Batch 122/210 - Loss: 4.9218\n",
      "  Batch 123/210 - Loss: 4.8263\n",
      "  Batch 124/210 - Loss: 4.9247\n",
      "  Batch 125/210 - Loss: 4.8781\n",
      "  Batch 126/210 - Loss: 4.9501\n",
      "  Batch 127/210 - Loss: 4.8712\n",
      "  Batch 128/210 - Loss: 4.8153\n",
      "  Batch 129/210 - Loss: 4.7915\n",
      "  Batch 130/210 - Loss: 4.8576\n",
      "  Batch 131/210 - Loss: 4.9174\n",
      "  Batch 132/210 - Loss: 4.7649\n",
      "  Batch 133/210 - Loss: 4.8479\n",
      "  Batch 134/210 - Loss: 4.7751\n",
      "  Batch 135/210 - Loss: 4.7425\n",
      "  Batch 136/210 - Loss: 4.8329\n",
      "  Batch 137/210 - Loss: 4.8849\n",
      "  Batch 138/210 - Loss: 4.8254\n",
      "  Batch 139/210 - Loss: 4.7795\n",
      "  Batch 140/210 - Loss: 4.8097\n",
      "  Batch 141/210 - Loss: 4.7627\n",
      "  Batch 142/210 - Loss: 4.8507\n",
      "  Batch 143/210 - Loss: 4.6896\n",
      "  Batch 144/210 - Loss: 4.7038\n",
      "  Batch 145/210 - Loss: 4.6794\n",
      "  Batch 146/210 - Loss: 4.7629\n",
      "  Batch 147/210 - Loss: 4.7133\n",
      "  Batch 148/210 - Loss: 4.6813\n",
      "  Batch 149/210 - Loss: 4.6469\n",
      "  Batch 150/210 - Loss: 4.7347\n",
      "  Batch 151/210 - Loss: 4.6789\n",
      "  Batch 152/210 - Loss: 4.7419\n",
      "  Batch 153/210 - Loss: 4.6854\n",
      "  Batch 154/210 - Loss: 4.6769\n",
      "  Batch 155/210 - Loss: 4.6966\n",
      "  Batch 156/210 - Loss: 4.7373\n",
      "  Batch 157/210 - Loss: 4.6604\n",
      "  Batch 158/210 - Loss: 4.6623\n",
      "  Batch 159/210 - Loss: 4.7101\n",
      "  Batch 160/210 - Loss: 4.6674\n",
      "  Batch 161/210 - Loss: 4.6604\n",
      "  Batch 162/210 - Loss: 4.6993\n",
      "  Batch 163/210 - Loss: 4.6511\n",
      "  Batch 164/210 - Loss: 4.6034\n",
      "  Batch 165/210 - Loss: 4.6928\n",
      "  Batch 166/210 - Loss: 4.5878\n",
      "  Batch 167/210 - Loss: 4.6136\n",
      "  Batch 168/210 - Loss: 4.6622\n",
      "  Batch 169/210 - Loss: 4.6394\n",
      "  Batch 170/210 - Loss: 4.6419\n",
      "  Batch 171/210 - Loss: 4.7009\n",
      "  Batch 172/210 - Loss: 4.6807\n",
      "  Batch 173/210 - Loss: 4.5692\n",
      "  Batch 174/210 - Loss: 4.6435\n",
      "  Batch 175/210 - Loss: 4.6138\n",
      "  Batch 176/210 - Loss: 4.5695\n",
      "  Batch 177/210 - Loss: 4.5589\n",
      "  Batch 178/210 - Loss: 4.6022\n",
      "  Batch 179/210 - Loss: 4.6163\n",
      "  Batch 180/210 - Loss: 4.5732\n",
      "  Batch 181/210 - Loss: 4.6037\n",
      "  Batch 182/210 - Loss: 4.5447\n",
      "  Batch 183/210 - Loss: 4.6533\n",
      "  Batch 184/210 - Loss: 4.5816\n",
      "  Batch 185/210 - Loss: 4.4886\n",
      "  Batch 186/210 - Loss: 4.5627\n",
      "  Batch 187/210 - Loss: 4.5075\n",
      "  Batch 188/210 - Loss: 4.4752\n",
      "  Batch 189/210 - Loss: 4.5324\n",
      "  Batch 190/210 - Loss: 4.5497\n",
      "  Batch 191/210 - Loss: 4.5393\n",
      "  Batch 192/210 - Loss: 4.5087\n",
      "  Batch 193/210 - Loss: 4.5455\n",
      "  Batch 194/210 - Loss: 4.5158\n",
      "  Batch 195/210 - Loss: 4.5001\n",
      "  Batch 196/210 - Loss: 4.5175\n",
      "  Batch 197/210 - Loss: 4.5869\n",
      "  Batch 198/210 - Loss: 4.5352\n",
      "  Batch 199/210 - Loss: 4.5275\n",
      "  Batch 200/210 - Loss: 4.5337\n",
      "  Batch 201/210 - Loss: 4.3872\n",
      "  Batch 202/210 - Loss: 4.3839\n",
      "  Batch 203/210 - Loss: 4.4052\n",
      "  Batch 204/210 - Loss: 4.5002\n",
      "  Batch 205/210 - Loss: 4.5474\n",
      "  Batch 206/210 - Loss: 4.4110\n",
      "  Batch 207/210 - Loss: 4.4347\n",
      "  Batch 208/210 - Loss: 4.3925\n",
      "  Batch 209/210 - Loss: 4.4687\n",
      "  Batch 210/210 - Loss: 4.4404\n",
      "Epoch 1 Completed. Train Loss: 5.1772, Val Loss: 4.4587\n",
      "\n",
      "Epoch 2/15\n",
      "  Batch 1/210 - Loss: 4.4685\n",
      "  Batch 2/210 - Loss: 4.4228\n",
      "  Batch 3/210 - Loss: 4.3828\n",
      "  Batch 4/210 - Loss: 4.3841\n",
      "  Batch 5/210 - Loss: 4.4070\n",
      "  Batch 6/210 - Loss: 4.4275\n",
      "  Batch 7/210 - Loss: 4.2998\n",
      "  Batch 8/210 - Loss: 4.3829\n",
      "  Batch 9/210 - Loss: 4.4018\n",
      "  Batch 10/210 - Loss: 4.4032\n",
      "  Batch 11/210 - Loss: 4.3504\n",
      "  Batch 12/210 - Loss: 4.3577\n",
      "  Batch 13/210 - Loss: 4.3645\n",
      "  Batch 14/210 - Loss: 4.3908\n",
      "  Batch 15/210 - Loss: 4.3232\n",
      "  Batch 16/210 - Loss: 4.3671\n",
      "  Batch 17/210 - Loss: 4.4154\n",
      "  Batch 18/210 - Loss: 4.3092\n",
      "  Batch 19/210 - Loss: 4.3428\n",
      "  Batch 20/210 - Loss: 4.2904\n",
      "  Batch 21/210 - Loss: 4.3814\n",
      "  Batch 22/210 - Loss: 4.2903\n",
      "  Batch 23/210 - Loss: 4.2877\n",
      "  Batch 24/210 - Loss: 4.3243\n",
      "  Batch 25/210 - Loss: 4.3746\n",
      "  Batch 26/210 - Loss: 4.2896\n",
      "  Batch 27/210 - Loss: 4.2266\n",
      "  Batch 28/210 - Loss: 4.2831\n",
      "  Batch 29/210 - Loss: 4.3111\n",
      "  Batch 30/210 - Loss: 4.2890\n",
      "  Batch 31/210 - Loss: 4.2874\n",
      "  Batch 32/210 - Loss: 4.2439\n",
      "  Batch 33/210 - Loss: 4.2948\n",
      "  Batch 34/210 - Loss: 4.3096\n",
      "  Batch 35/210 - Loss: 4.2541\n",
      "  Batch 36/210 - Loss: 4.2380\n",
      "  Batch 37/210 - Loss: 4.2732\n",
      "  Batch 38/210 - Loss: 4.3059\n",
      "  Batch 39/210 - Loss: 4.2181\n",
      "  Batch 40/210 - Loss: 4.2644\n",
      "  Batch 41/210 - Loss: 4.1958\n",
      "  Batch 42/210 - Loss: 4.2160\n",
      "  Batch 43/210 - Loss: 4.2761\n",
      "  Batch 44/210 - Loss: 4.2241\n",
      "  Batch 45/210 - Loss: 4.1719\n",
      "  Batch 46/210 - Loss: 4.2425\n",
      "  Batch 47/210 - Loss: 4.2842\n",
      "  Batch 48/210 - Loss: 4.2040\n",
      "  Batch 49/210 - Loss: 4.1909\n",
      "  Batch 50/210 - Loss: 4.1745\n",
      "  Batch 51/210 - Loss: 4.1806\n",
      "  Batch 52/210 - Loss: 4.1749\n",
      "  Batch 53/210 - Loss: 4.2185\n",
      "  Batch 54/210 - Loss: 4.1905\n",
      "  Batch 55/210 - Loss: 4.2039\n",
      "  Batch 56/210 - Loss: 4.2191\n",
      "  Batch 57/210 - Loss: 4.1651\n",
      "  Batch 58/210 - Loss: 4.2407\n",
      "  Batch 59/210 - Loss: 4.3110\n",
      "  Batch 60/210 - Loss: 4.2022\n",
      "  Batch 61/210 - Loss: 4.2076\n",
      "  Batch 62/210 - Loss: 4.1314\n",
      "  Batch 63/210 - Loss: 4.1866\n",
      "  Batch 64/210 - Loss: 4.1981\n",
      "  Batch 65/210 - Loss: 4.2430\n",
      "  Batch 66/210 - Loss: 4.1911\n",
      "  Batch 67/210 - Loss: 4.2050\n",
      "  Batch 68/210 - Loss: 4.2182\n",
      "  Batch 69/210 - Loss: 4.0481\n",
      "  Batch 70/210 - Loss: 4.1686\n",
      "  Batch 71/210 - Loss: 4.0240\n",
      "  Batch 72/210 - Loss: 4.1080\n",
      "  Batch 73/210 - Loss: 4.1662\n",
      "  Batch 74/210 - Loss: 4.1528\n",
      "  Batch 75/210 - Loss: 4.0693\n",
      "  Batch 76/210 - Loss: 4.1205\n",
      "  Batch 77/210 - Loss: 4.1290\n",
      "  Batch 78/210 - Loss: 4.0336\n",
      "  Batch 79/210 - Loss: 4.1961\n",
      "  Batch 80/210 - Loss: 4.2414\n",
      "  Batch 81/210 - Loss: 4.1191\n",
      "  Batch 82/210 - Loss: 4.0846\n",
      "  Batch 83/210 - Loss: 4.0530\n",
      "  Batch 84/210 - Loss: 4.0211\n",
      "  Batch 85/210 - Loss: 4.0445\n",
      "  Batch 86/210 - Loss: 4.0742\n",
      "  Batch 87/210 - Loss: 4.1130\n",
      "  Batch 88/210 - Loss: 4.0400\n",
      "  Batch 89/210 - Loss: 4.1673\n",
      "  Batch 90/210 - Loss: 4.1045\n",
      "  Batch 91/210 - Loss: 4.0683\n",
      "  Batch 92/210 - Loss: 3.9955\n",
      "  Batch 93/210 - Loss: 4.1061\n",
      "  Batch 94/210 - Loss: 4.0339\n",
      "  Batch 95/210 - Loss: 3.9657\n",
      "  Batch 96/210 - Loss: 4.0824\n",
      "  Batch 97/210 - Loss: 4.0209\n",
      "  Batch 98/210 - Loss: 3.9895\n",
      "  Batch 99/210 - Loss: 4.1175\n",
      "  Batch 100/210 - Loss: 4.1357\n",
      "  Batch 101/210 - Loss: 3.9715\n",
      "  Batch 102/210 - Loss: 4.1191\n",
      "  Batch 103/210 - Loss: 4.0893\n",
      "  Batch 104/210 - Loss: 4.0774\n",
      "  Batch 105/210 - Loss: 4.0164\n",
      "  Batch 106/210 - Loss: 4.0165\n",
      "  Batch 107/210 - Loss: 3.9624\n",
      "  Batch 108/210 - Loss: 4.0166\n",
      "  Batch 109/210 - Loss: 4.1255\n",
      "  Batch 110/210 - Loss: 4.0220\n",
      "  Batch 111/210 - Loss: 3.9229\n",
      "  Batch 112/210 - Loss: 3.9647\n",
      "  Batch 113/210 - Loss: 4.0348\n",
      "  Batch 114/210 - Loss: 3.9310\n",
      "  Batch 115/210 - Loss: 3.9613\n",
      "  Batch 116/210 - Loss: 3.9527\n",
      "  Batch 117/210 - Loss: 3.9364\n",
      "  Batch 118/210 - Loss: 3.9664\n",
      "  Batch 119/210 - Loss: 3.9314\n",
      "  Batch 120/210 - Loss: 3.9636\n",
      "  Batch 121/210 - Loss: 3.8740\n",
      "  Batch 122/210 - Loss: 3.8838\n",
      "  Batch 123/210 - Loss: 3.9106\n",
      "  Batch 124/210 - Loss: 3.8382\n",
      "  Batch 125/210 - Loss: 3.7843\n",
      "  Batch 126/210 - Loss: 3.8964\n",
      "  Batch 127/210 - Loss: 3.8038\n",
      "  Batch 128/210 - Loss: 3.8730\n",
      "  Batch 129/210 - Loss: 3.9496\n",
      "  Batch 130/210 - Loss: 3.9635\n",
      "  Batch 131/210 - Loss: 3.8875\n",
      "  Batch 132/210 - Loss: 3.9153\n",
      "  Batch 133/210 - Loss: 3.9227\n",
      "  Batch 134/210 - Loss: 3.9767\n",
      "  Batch 135/210 - Loss: 3.8405\n",
      "  Batch 136/210 - Loss: 3.9700\n",
      "  Batch 137/210 - Loss: 3.7999\n",
      "  Batch 138/210 - Loss: 3.9505\n",
      "  Batch 139/210 - Loss: 3.9627\n",
      "  Batch 140/210 - Loss: 3.9356\n",
      "  Batch 141/210 - Loss: 3.8497\n",
      "  Batch 142/210 - Loss: 3.9750\n",
      "  Batch 143/210 - Loss: 3.9292\n",
      "  Batch 144/210 - Loss: 3.8160\n",
      "  Batch 145/210 - Loss: 3.7899\n",
      "  Batch 146/210 - Loss: 3.7609\n",
      "  Batch 147/210 - Loss: 3.8430\n",
      "  Batch 148/210 - Loss: 3.9244\n",
      "  Batch 149/210 - Loss: 3.7963\n",
      "  Batch 150/210 - Loss: 3.8842\n",
      "  Batch 151/210 - Loss: 3.8325\n",
      "  Batch 152/210 - Loss: 3.8801\n",
      "  Batch 153/210 - Loss: 3.7795\n",
      "  Batch 154/210 - Loss: 3.7900\n",
      "  Batch 155/210 - Loss: 3.9329\n",
      "  Batch 156/210 - Loss: 3.7695\n",
      "  Batch 157/210 - Loss: 3.6842\n",
      "  Batch 158/210 - Loss: 3.9393\n",
      "  Batch 159/210 - Loss: 3.7804\n",
      "  Batch 160/210 - Loss: 3.7958\n",
      "  Batch 161/210 - Loss: 3.7314\n",
      "  Batch 162/210 - Loss: 3.7320\n",
      "  Batch 163/210 - Loss: 3.7697\n",
      "  Batch 164/210 - Loss: 3.7181\n",
      "  Batch 165/210 - Loss: 3.7308\n",
      "  Batch 166/210 - Loss: 3.6954\n",
      "  Batch 167/210 - Loss: 3.7785\n",
      "  Batch 168/210 - Loss: 3.8154\n",
      "  Batch 169/210 - Loss: 3.8224\n",
      "  Batch 170/210 - Loss: 3.7086\n",
      "  Batch 171/210 - Loss: 3.7521\n",
      "  Batch 172/210 - Loss: 3.6009\n",
      "  Batch 173/210 - Loss: 3.7559\n",
      "  Batch 174/210 - Loss: 3.7667\n",
      "  Batch 175/210 - Loss: 3.7450\n",
      "  Batch 176/210 - Loss: 3.6961\n",
      "  Batch 177/210 - Loss: 3.6882\n",
      "  Batch 178/210 - Loss: 3.6557\n",
      "  Batch 179/210 - Loss: 3.7703\n",
      "  Batch 180/210 - Loss: 3.7201\n",
      "  Batch 181/210 - Loss: 3.6892\n",
      "  Batch 182/210 - Loss: 3.7619\n",
      "  Batch 183/210 - Loss: 3.6986\n",
      "  Batch 184/210 - Loss: 3.6191\n",
      "  Batch 185/210 - Loss: 3.7871\n",
      "  Batch 186/210 - Loss: 3.6382\n",
      "  Batch 187/210 - Loss: 3.6510\n",
      "  Batch 188/210 - Loss: 3.7310\n",
      "  Batch 189/210 - Loss: 3.6524\n",
      "  Batch 190/210 - Loss: 3.6758\n",
      "  Batch 191/210 - Loss: 3.6065\n",
      "  Batch 192/210 - Loss: 3.6356\n",
      "  Batch 193/210 - Loss: 3.6669\n",
      "  Batch 194/210 - Loss: 3.6409\n",
      "  Batch 195/210 - Loss: 3.6244\n",
      "  Batch 196/210 - Loss: 3.6854\n",
      "  Batch 197/210 - Loss: 3.6190\n",
      "  Batch 198/210 - Loss: 3.6052\n",
      "  Batch 199/210 - Loss: 3.6097\n",
      "  Batch 200/210 - Loss: 3.5366\n",
      "  Batch 201/210 - Loss: 3.5657\n",
      "  Batch 202/210 - Loss: 3.6259\n",
      "  Batch 203/210 - Loss: 3.6083\n",
      "  Batch 204/210 - Loss: 3.5398\n",
      "  Batch 205/210 - Loss: 3.6065\n",
      "  Batch 206/210 - Loss: 3.6247\n",
      "  Batch 207/210 - Loss: 3.6134\n",
      "  Batch 208/210 - Loss: 3.6183\n",
      "  Batch 209/210 - Loss: 3.4055\n",
      "  Batch 210/210 - Loss: 3.5857\n",
      "Epoch 2 Completed. Train Loss: 4.0007, Val Loss: 3.4459\n",
      "\n",
      "Epoch 3/15\n",
      "  Batch 1/210 - Loss: 3.5074\n",
      "  Batch 2/210 - Loss: 3.4576\n",
      "  Batch 3/210 - Loss: 3.5883\n",
      "  Batch 4/210 - Loss: 3.5873\n",
      "  Batch 5/210 - Loss: 3.5209\n",
      "  Batch 6/210 - Loss: 3.6011\n",
      "  Batch 7/210 - Loss: 3.5121\n",
      "  Batch 8/210 - Loss: 3.4899\n",
      "  Batch 9/210 - Loss: 3.4823\n",
      "  Batch 10/210 - Loss: 3.4994\n",
      "  Batch 11/210 - Loss: 3.3434\n",
      "  Batch 12/210 - Loss: 3.4615\n",
      "  Batch 13/210 - Loss: 3.4631\n",
      "  Batch 14/210 - Loss: 3.4666\n",
      "  Batch 15/210 - Loss: 3.4216\n",
      "  Batch 16/210 - Loss: 3.3639\n",
      "  Batch 17/210 - Loss: 3.4954\n",
      "  Batch 18/210 - Loss: 3.3914\n",
      "  Batch 19/210 - Loss: 3.4149\n",
      "  Batch 20/210 - Loss: 3.4545\n",
      "  Batch 21/210 - Loss: 3.3703\n",
      "  Batch 22/210 - Loss: 3.3840\n",
      "  Batch 23/210 - Loss: 3.3635\n",
      "  Batch 24/210 - Loss: 3.4054\n",
      "  Batch 25/210 - Loss: 3.4210\n",
      "  Batch 26/210 - Loss: 3.4341\n",
      "  Batch 27/210 - Loss: 3.3709\n",
      "  Batch 28/210 - Loss: 3.2897\n",
      "  Batch 29/210 - Loss: 3.2701\n",
      "  Batch 30/210 - Loss: 3.3399\n",
      "  Batch 31/210 - Loss: 3.2582\n",
      "  Batch 32/210 - Loss: 3.3739\n",
      "  Batch 33/210 - Loss: 3.4317\n",
      "  Batch 34/210 - Loss: 3.3249\n",
      "  Batch 35/210 - Loss: 3.2922\n",
      "  Batch 36/210 - Loss: 3.2756\n",
      "  Batch 37/210 - Loss: 3.3611\n",
      "  Batch 38/210 - Loss: 3.3199\n",
      "  Batch 39/210 - Loss: 3.3082\n",
      "  Batch 40/210 - Loss: 3.3590\n",
      "  Batch 41/210 - Loss: 3.2932\n",
      "  Batch 42/210 - Loss: 3.2393\n",
      "  Batch 43/210 - Loss: 3.3937\n",
      "  Batch 44/210 - Loss: 3.3407\n",
      "  Batch 45/210 - Loss: 3.2522\n",
      "  Batch 46/210 - Loss: 3.3420\n",
      "  Batch 47/210 - Loss: 3.2875\n",
      "  Batch 48/210 - Loss: 3.2483\n",
      "  Batch 49/210 - Loss: 3.2350\n",
      "  Batch 50/210 - Loss: 3.1908\n",
      "  Batch 51/210 - Loss: 3.2101\n",
      "  Batch 52/210 - Loss: 3.3526\n",
      "  Batch 53/210 - Loss: 3.2238\n",
      "  Batch 54/210 - Loss: 3.3161\n",
      "  Batch 55/210 - Loss: 3.2370\n",
      "  Batch 56/210 - Loss: 3.2603\n",
      "  Batch 57/210 - Loss: 3.2448\n",
      "  Batch 58/210 - Loss: 3.1574\n",
      "  Batch 59/210 - Loss: 3.2271\n",
      "  Batch 60/210 - Loss: 3.2096\n",
      "  Batch 61/210 - Loss: 3.2221\n",
      "  Batch 62/210 - Loss: 3.2023\n",
      "  Batch 63/210 - Loss: 3.1587\n",
      "  Batch 64/210 - Loss: 3.2137\n",
      "  Batch 65/210 - Loss: 3.0916\n",
      "  Batch 66/210 - Loss: 3.1671\n",
      "  Batch 67/210 - Loss: 3.3088\n",
      "  Batch 68/210 - Loss: 3.2287\n",
      "  Batch 69/210 - Loss: 3.2298\n",
      "  Batch 70/210 - Loss: 3.1437\n",
      "  Batch 71/210 - Loss: 3.1875\n",
      "  Batch 72/210 - Loss: 3.2066\n",
      "  Batch 73/210 - Loss: 3.2041\n",
      "  Batch 74/210 - Loss: 3.1561\n",
      "  Batch 75/210 - Loss: 3.1002\n",
      "  Batch 76/210 - Loss: 3.1146\n",
      "  Batch 77/210 - Loss: 3.2507\n",
      "  Batch 78/210 - Loss: 3.1507\n",
      "  Batch 79/210 - Loss: 3.2197\n",
      "  Batch 80/210 - Loss: 3.2000\n",
      "  Batch 81/210 - Loss: 3.0723\n",
      "  Batch 82/210 - Loss: 3.1633\n",
      "  Batch 83/210 - Loss: 3.0731\n",
      "  Batch 84/210 - Loss: 3.1275\n",
      "  Batch 85/210 - Loss: 3.1432\n",
      "  Batch 86/210 - Loss: 3.2314\n",
      "  Batch 87/210 - Loss: 2.9934\n",
      "  Batch 88/210 - Loss: 3.1170\n",
      "  Batch 89/210 - Loss: 3.0775\n",
      "  Batch 90/210 - Loss: 3.0176\n",
      "  Batch 91/210 - Loss: 3.1482\n",
      "  Batch 92/210 - Loss: 3.1904\n",
      "  Batch 93/210 - Loss: 3.0528\n",
      "  Batch 94/210 - Loss: 3.0710\n",
      "  Batch 95/210 - Loss: 3.0861\n",
      "  Batch 96/210 - Loss: 3.1517\n",
      "  Batch 97/210 - Loss: 3.0815\n",
      "  Batch 98/210 - Loss: 3.0799\n",
      "  Batch 99/210 - Loss: 3.0701\n",
      "  Batch 100/210 - Loss: 3.0734\n",
      "  Batch 101/210 - Loss: 3.0898\n",
      "  Batch 102/210 - Loss: 2.9569\n",
      "  Batch 103/210 - Loss: 3.0020\n",
      "  Batch 104/210 - Loss: 2.9962\n",
      "  Batch 105/210 - Loss: 2.8986\n",
      "  Batch 106/210 - Loss: 3.1396\n",
      "  Batch 107/210 - Loss: 2.9042\n",
      "  Batch 108/210 - Loss: 3.0781\n",
      "  Batch 109/210 - Loss: 2.9981\n",
      "  Batch 110/210 - Loss: 3.0322\n",
      "  Batch 111/210 - Loss: 2.9614\n",
      "  Batch 112/210 - Loss: 2.8593\n",
      "  Batch 113/210 - Loss: 3.0069\n",
      "  Batch 114/210 - Loss: 2.9455\n",
      "  Batch 115/210 - Loss: 2.9979\n",
      "  Batch 116/210 - Loss: 2.9823\n",
      "  Batch 117/210 - Loss: 2.8878\n",
      "  Batch 118/210 - Loss: 3.0248\n",
      "  Batch 119/210 - Loss: 2.8966\n",
      "  Batch 120/210 - Loss: 2.9676\n",
      "  Batch 121/210 - Loss: 2.8817\n",
      "  Batch 122/210 - Loss: 2.9028\n",
      "  Batch 123/210 - Loss: 3.0283\n",
      "  Batch 124/210 - Loss: 2.9036\n",
      "  Batch 125/210 - Loss: 2.8215\n",
      "  Batch 126/210 - Loss: 2.8622\n",
      "  Batch 127/210 - Loss: 2.8279\n",
      "  Batch 128/210 - Loss: 3.0078\n",
      "  Batch 129/210 - Loss: 2.8518\n",
      "  Batch 130/210 - Loss: 3.0485\n",
      "  Batch 131/210 - Loss: 2.8824\n",
      "  Batch 132/210 - Loss: 2.9988\n",
      "  Batch 133/210 - Loss: 2.8230\n",
      "  Batch 134/210 - Loss: 2.7843\n",
      "  Batch 135/210 - Loss: 2.7083\n",
      "  Batch 136/210 - Loss: 2.8098\n",
      "  Batch 137/210 - Loss: 2.7770\n",
      "  Batch 138/210 - Loss: 2.8817\n",
      "  Batch 139/210 - Loss: 2.8079\n",
      "  Batch 140/210 - Loss: 2.7801\n",
      "  Batch 141/210 - Loss: 2.7741\n",
      "  Batch 142/210 - Loss: 2.7492\n",
      "  Batch 143/210 - Loss: 2.8132\n",
      "  Batch 144/210 - Loss: 2.7707\n",
      "  Batch 145/210 - Loss: 2.7851\n",
      "  Batch 146/210 - Loss: 2.8528\n",
      "  Batch 147/210 - Loss: 2.6476\n",
      "  Batch 148/210 - Loss: 2.7654\n",
      "  Batch 149/210 - Loss: 2.7642\n",
      "  Batch 150/210 - Loss: 2.8052\n",
      "  Batch 151/210 - Loss: 2.6807\n",
      "  Batch 152/210 - Loss: 2.6786\n",
      "  Batch 153/210 - Loss: 2.7476\n",
      "  Batch 154/210 - Loss: 2.7423\n",
      "  Batch 155/210 - Loss: 2.8097\n",
      "  Batch 156/210 - Loss: 2.6506\n",
      "  Batch 157/210 - Loss: 2.7353\n",
      "  Batch 158/210 - Loss: 2.7524\n",
      "  Batch 159/210 - Loss: 2.6831\n",
      "  Batch 160/210 - Loss: 2.6705\n",
      "  Batch 161/210 - Loss: 2.6700\n",
      "  Batch 162/210 - Loss: 2.7471\n",
      "  Batch 163/210 - Loss: 2.5568\n",
      "  Batch 164/210 - Loss: 2.6628\n",
      "  Batch 165/210 - Loss: 2.7080\n",
      "  Batch 166/210 - Loss: 2.6211\n",
      "  Batch 167/210 - Loss: 2.6410\n",
      "  Batch 168/210 - Loss: 2.5564\n",
      "  Batch 169/210 - Loss: 2.4964\n",
      "  Batch 170/210 - Loss: 2.5315\n",
      "  Batch 171/210 - Loss: 2.6648\n",
      "  Batch 172/210 - Loss: 2.6205\n",
      "  Batch 173/210 - Loss: 2.6258\n",
      "  Batch 174/210 - Loss: 2.5517\n",
      "  Batch 175/210 - Loss: 2.5584\n",
      "  Batch 176/210 - Loss: 2.5532\n",
      "  Batch 177/210 - Loss: 2.6191\n",
      "  Batch 178/210 - Loss: 2.5579\n",
      "  Batch 179/210 - Loss: 2.5353\n",
      "  Batch 180/210 - Loss: 2.5400\n",
      "  Batch 181/210 - Loss: 2.4462\n",
      "  Batch 182/210 - Loss: 2.5080\n",
      "  Batch 183/210 - Loss: 2.3708\n",
      "  Batch 184/210 - Loss: 2.6065\n",
      "  Batch 185/210 - Loss: 2.6924\n",
      "  Batch 186/210 - Loss: 2.3705\n",
      "  Batch 187/210 - Loss: 2.5189\n",
      "  Batch 188/210 - Loss: 2.6160\n",
      "  Batch 189/210 - Loss: 2.4923\n",
      "  Batch 190/210 - Loss: 2.5213\n",
      "  Batch 191/210 - Loss: 2.4245\n",
      "  Batch 192/210 - Loss: 2.4261\n",
      "  Batch 193/210 - Loss: 2.4275\n",
      "  Batch 194/210 - Loss: 2.5473\n",
      "  Batch 195/210 - Loss: 2.3955\n",
      "  Batch 196/210 - Loss: 2.4502\n",
      "  Batch 197/210 - Loss: 2.5625\n",
      "  Batch 198/210 - Loss: 2.3939\n",
      "  Batch 199/210 - Loss: 2.5246\n",
      "  Batch 200/210 - Loss: 2.5353\n",
      "  Batch 201/210 - Loss: 2.3547\n",
      "  Batch 202/210 - Loss: 2.4738\n",
      "  Batch 203/210 - Loss: 2.3204\n",
      "  Batch 204/210 - Loss: 2.4726\n",
      "  Batch 205/210 - Loss: 2.4591\n",
      "  Batch 206/210 - Loss: 2.3348\n",
      "  Batch 207/210 - Loss: 2.3771\n",
      "  Batch 208/210 - Loss: 2.3371\n",
      "  Batch 209/210 - Loss: 2.4171\n",
      "  Batch 210/210 - Loss: 2.1190\n",
      "Epoch 3 Completed. Train Loss: 2.9758, Val Loss: 2.1932\n",
      "\n",
      "Epoch 4/15\n",
      "  Batch 1/210 - Loss: 2.4288\n",
      "  Batch 2/210 - Loss: 2.4005\n",
      "  Batch 3/210 - Loss: 2.3407\n",
      "  Batch 4/210 - Loss: 2.3600\n",
      "  Batch 5/210 - Loss: 2.3183\n",
      "  Batch 6/210 - Loss: 2.3824\n",
      "  Batch 7/210 - Loss: 2.2831\n",
      "  Batch 8/210 - Loss: 2.3125\n",
      "  Batch 9/210 - Loss: 2.3967\n",
      "  Batch 10/210 - Loss: 2.3557\n",
      "  Batch 11/210 - Loss: 2.4268\n",
      "  Batch 12/210 - Loss: 2.2804\n",
      "  Batch 13/210 - Loss: 2.3671\n",
      "  Batch 14/210 - Loss: 2.3369\n",
      "  Batch 15/210 - Loss: 2.2378\n",
      "  Batch 16/210 - Loss: 2.2996\n",
      "  Batch 17/210 - Loss: 2.1637\n",
      "  Batch 18/210 - Loss: 2.1663\n",
      "  Batch 19/210 - Loss: 2.2764\n",
      "  Batch 20/210 - Loss: 2.3345\n",
      "  Batch 21/210 - Loss: 2.3031\n",
      "  Batch 22/210 - Loss: 2.3924\n",
      "  Batch 23/210 - Loss: 2.3046\n",
      "  Batch 24/210 - Loss: 2.0653\n",
      "  Batch 25/210 - Loss: 2.2733\n",
      "  Batch 26/210 - Loss: 2.1824\n",
      "  Batch 27/210 - Loss: 2.2593\n",
      "  Batch 28/210 - Loss: 2.3127\n",
      "  Batch 29/210 - Loss: 2.1668\n",
      "  Batch 30/210 - Loss: 2.2765\n",
      "  Batch 31/210 - Loss: 2.2781\n",
      "  Batch 32/210 - Loss: 2.1310\n",
      "  Batch 33/210 - Loss: 2.2452\n",
      "  Batch 34/210 - Loss: 2.2582\n",
      "  Batch 35/210 - Loss: 2.0322\n",
      "  Batch 36/210 - Loss: 2.3069\n",
      "  Batch 37/210 - Loss: 2.1334\n",
      "  Batch 38/210 - Loss: 2.1998\n",
      "  Batch 39/210 - Loss: 2.1907\n",
      "  Batch 40/210 - Loss: 2.1246\n",
      "  Batch 41/210 - Loss: 2.2959\n",
      "  Batch 42/210 - Loss: 2.0578\n",
      "  Batch 43/210 - Loss: 2.1100\n",
      "  Batch 44/210 - Loss: 2.0567\n",
      "  Batch 45/210 - Loss: 2.1645\n",
      "  Batch 46/210 - Loss: 2.0772\n",
      "  Batch 47/210 - Loss: 2.2408\n",
      "  Batch 48/210 - Loss: 2.2622\n",
      "  Batch 49/210 - Loss: 1.9837\n",
      "  Batch 50/210 - Loss: 2.1604\n",
      "  Batch 51/210 - Loss: 2.0572\n",
      "  Batch 52/210 - Loss: 2.0113\n",
      "  Batch 53/210 - Loss: 2.1725\n",
      "  Batch 54/210 - Loss: 2.0212\n",
      "  Batch 55/210 - Loss: 2.1196\n",
      "  Batch 56/210 - Loss: 2.0658\n",
      "  Batch 57/210 - Loss: 2.1844\n",
      "  Batch 58/210 - Loss: 2.0835\n",
      "  Batch 59/210 - Loss: 1.9714\n",
      "  Batch 60/210 - Loss: 2.0092\n",
      "  Batch 61/210 - Loss: 2.0430\n",
      "  Batch 62/210 - Loss: 2.0266\n",
      "  Batch 63/210 - Loss: 2.0948\n",
      "  Batch 64/210 - Loss: 1.9675\n",
      "  Batch 65/210 - Loss: 1.9700\n",
      "  Batch 66/210 - Loss: 1.9551\n",
      "  Batch 67/210 - Loss: 2.0260\n",
      "  Batch 68/210 - Loss: 1.9291\n",
      "  Batch 69/210 - Loss: 1.9541\n",
      "  Batch 70/210 - Loss: 1.9570\n",
      "  Batch 71/210 - Loss: 2.0707\n",
      "  Batch 72/210 - Loss: 1.8246\n",
      "  Batch 73/210 - Loss: 1.9401\n",
      "  Batch 74/210 - Loss: 2.0020\n",
      "  Batch 75/210 - Loss: 1.9284\n",
      "  Batch 76/210 - Loss: 1.9511\n",
      "  Batch 77/210 - Loss: 1.9486\n",
      "  Batch 78/210 - Loss: 1.9787\n",
      "  Batch 79/210 - Loss: 1.9718\n",
      "  Batch 80/210 - Loss: 1.9228\n",
      "  Batch 81/210 - Loss: 2.0866\n",
      "  Batch 82/210 - Loss: 1.8947\n",
      "  Batch 83/210 - Loss: 1.9866\n",
      "  Batch 84/210 - Loss: 1.9249\n",
      "  Batch 85/210 - Loss: 1.9854\n",
      "  Batch 86/210 - Loss: 1.7691\n",
      "  Batch 87/210 - Loss: 1.8603\n",
      "  Batch 88/210 - Loss: 1.9764\n",
      "  Batch 89/210 - Loss: 1.9155\n",
      "  Batch 90/210 - Loss: 1.9387\n",
      "  Batch 91/210 - Loss: 1.8392\n",
      "  Batch 92/210 - Loss: 1.8351\n",
      "  Batch 93/210 - Loss: 1.8246\n",
      "  Batch 94/210 - Loss: 1.9389\n",
      "  Batch 95/210 - Loss: 1.8490\n",
      "  Batch 96/210 - Loss: 1.7938\n",
      "  Batch 97/210 - Loss: 1.8623\n",
      "  Batch 98/210 - Loss: 1.8314\n",
      "  Batch 99/210 - Loss: 1.8891\n",
      "  Batch 100/210 - Loss: 1.8553\n",
      "  Batch 101/210 - Loss: 1.9430\n",
      "  Batch 102/210 - Loss: 1.8713\n",
      "  Batch 103/210 - Loss: 1.9233\n",
      "  Batch 104/210 - Loss: 1.7142\n",
      "  Batch 105/210 - Loss: 1.7754\n",
      "  Batch 106/210 - Loss: 1.8021\n",
      "  Batch 107/210 - Loss: 1.7482\n",
      "  Batch 108/210 - Loss: 1.7985\n",
      "  Batch 109/210 - Loss: 1.7111\n",
      "  Batch 110/210 - Loss: 1.8201\n",
      "  Batch 111/210 - Loss: 1.7789\n",
      "  Batch 112/210 - Loss: 1.7830\n",
      "  Batch 113/210 - Loss: 1.7611\n",
      "  Batch 114/210 - Loss: 1.7334\n",
      "  Batch 115/210 - Loss: 1.6947\n",
      "  Batch 116/210 - Loss: 1.7357\n",
      "  Batch 117/210 - Loss: 1.8549\n",
      "  Batch 118/210 - Loss: 1.7570\n",
      "  Batch 119/210 - Loss: 1.7288\n",
      "  Batch 120/210 - Loss: 1.7486\n",
      "  Batch 121/210 - Loss: 1.6513\n",
      "  Batch 122/210 - Loss: 1.6957\n",
      "  Batch 123/210 - Loss: 1.7306\n",
      "  Batch 124/210 - Loss: 1.5580\n",
      "  Batch 125/210 - Loss: 1.7542\n",
      "  Batch 126/210 - Loss: 1.7120\n",
      "  Batch 127/210 - Loss: 1.7567\n",
      "  Batch 128/210 - Loss: 1.7560\n",
      "  Batch 129/210 - Loss: 1.6270\n",
      "  Batch 130/210 - Loss: 1.6855\n",
      "  Batch 131/210 - Loss: 1.7340\n",
      "  Batch 132/210 - Loss: 1.7012\n",
      "  Batch 133/210 - Loss: 1.7145\n",
      "  Batch 134/210 - Loss: 1.8360\n",
      "  Batch 135/210 - Loss: 1.7514\n",
      "  Batch 136/210 - Loss: 1.7179\n",
      "  Batch 137/210 - Loss: 1.7384\n",
      "  Batch 138/210 - Loss: 1.6653\n",
      "  Batch 139/210 - Loss: 1.6883\n",
      "  Batch 140/210 - Loss: 1.7030\n",
      "  Batch 141/210 - Loss: 1.6237\n",
      "  Batch 142/210 - Loss: 1.7653\n",
      "  Batch 143/210 - Loss: 1.6400\n",
      "  Batch 144/210 - Loss: 1.6921\n",
      "  Batch 145/210 - Loss: 1.5324\n",
      "  Batch 146/210 - Loss: 1.7573\n",
      "  Batch 147/210 - Loss: 1.6544\n",
      "  Batch 148/210 - Loss: 1.5963\n",
      "  Batch 149/210 - Loss: 1.6766\n",
      "  Batch 150/210 - Loss: 1.6671\n",
      "  Batch 151/210 - Loss: 1.5720\n",
      "  Batch 152/210 - Loss: 1.5829\n",
      "  Batch 153/210 - Loss: 1.5248\n",
      "  Batch 154/210 - Loss: 1.6315\n",
      "  Batch 155/210 - Loss: 1.6723\n",
      "  Batch 156/210 - Loss: 1.5880\n",
      "  Batch 157/210 - Loss: 1.5331\n",
      "  Batch 158/210 - Loss: 1.6764\n",
      "  Batch 159/210 - Loss: 1.5598\n",
      "  Batch 160/210 - Loss: 1.6085\n",
      "  Batch 161/210 - Loss: 1.6632\n",
      "  Batch 162/210 - Loss: 1.5134\n",
      "  Batch 163/210 - Loss: 1.5613\n",
      "  Batch 164/210 - Loss: 1.5635\n",
      "  Batch 165/210 - Loss: 1.5800\n",
      "  Batch 166/210 - Loss: 1.6327\n",
      "  Batch 167/210 - Loss: 1.5332\n",
      "  Batch 168/210 - Loss: 1.5434\n",
      "  Batch 169/210 - Loss: 1.6274\n",
      "  Batch 170/210 - Loss: 1.5889\n",
      "  Batch 171/210 - Loss: 1.6165\n",
      "  Batch 172/210 - Loss: 1.5701\n",
      "  Batch 173/210 - Loss: 1.4506\n",
      "  Batch 174/210 - Loss: 1.5869\n",
      "  Batch 175/210 - Loss: 1.4264\n",
      "  Batch 176/210 - Loss: 1.5010\n",
      "  Batch 177/210 - Loss: 1.5041\n",
      "  Batch 178/210 - Loss: 1.5339\n",
      "  Batch 179/210 - Loss: 1.5217\n",
      "  Batch 180/210 - Loss: 1.6087\n",
      "  Batch 181/210 - Loss: 1.6075\n",
      "  Batch 182/210 - Loss: 1.4568\n",
      "  Batch 183/210 - Loss: 1.5429\n",
      "  Batch 184/210 - Loss: 1.4719\n",
      "  Batch 185/210 - Loss: 1.4523\n",
      "  Batch 186/210 - Loss: 1.5654\n",
      "  Batch 187/210 - Loss: 1.5020\n",
      "  Batch 188/210 - Loss: 1.5148\n",
      "  Batch 189/210 - Loss: 1.6054\n",
      "  Batch 190/210 - Loss: 1.4891\n",
      "  Batch 191/210 - Loss: 1.4226\n",
      "  Batch 192/210 - Loss: 1.4739\n",
      "  Batch 193/210 - Loss: 1.6236\n",
      "  Batch 194/210 - Loss: 1.5233\n",
      "  Batch 195/210 - Loss: 1.4014\n",
      "  Batch 196/210 - Loss: 1.5264\n",
      "  Batch 197/210 - Loss: 1.4015\n",
      "  Batch 198/210 - Loss: 1.4526\n",
      "  Batch 199/210 - Loss: 1.4316\n",
      "  Batch 200/210 - Loss: 1.4774\n",
      "  Batch 201/210 - Loss: 1.3730\n",
      "  Batch 202/210 - Loss: 1.3345\n",
      "  Batch 203/210 - Loss: 1.4720\n",
      "  Batch 204/210 - Loss: 1.5561\n",
      "  Batch 205/210 - Loss: 1.3778\n",
      "  Batch 206/210 - Loss: 1.4763\n",
      "  Batch 207/210 - Loss: 1.3587\n",
      "  Batch 208/210 - Loss: 1.4967\n",
      "  Batch 209/210 - Loss: 1.4220\n",
      "  Batch 210/210 - Loss: 1.3796\n",
      "Epoch 4 Completed. Train Loss: 1.8479, Val Loss: 1.1393\n",
      "\n",
      "Epoch 5/15\n",
      "  Batch 1/210 - Loss: 1.3043\n",
      "  Batch 2/210 - Loss: 1.3188\n",
      "  Batch 3/210 - Loss: 1.4018\n",
      "  Batch 4/210 - Loss: 1.4492\n",
      "  Batch 5/210 - Loss: 1.2976\n",
      "  Batch 6/210 - Loss: 1.2700\n",
      "  Batch 7/210 - Loss: 1.2834\n",
      "  Batch 8/210 - Loss: 1.2090\n",
      "  Batch 9/210 - Loss: 1.3087\n",
      "  Batch 10/210 - Loss: 1.2986\n",
      "  Batch 11/210 - Loss: 1.4182\n",
      "  Batch 12/210 - Loss: 1.2585\n",
      "  Batch 13/210 - Loss: 1.3792\n",
      "  Batch 14/210 - Loss: 1.3921\n",
      "  Batch 15/210 - Loss: 1.3826\n",
      "  Batch 16/210 - Loss: 1.3805\n",
      "  Batch 17/210 - Loss: 1.3626\n",
      "  Batch 18/210 - Loss: 1.3700\n",
      "  Batch 19/210 - Loss: 1.2581\n",
      "  Batch 20/210 - Loss: 1.2438\n",
      "  Batch 21/210 - Loss: 1.2459\n",
      "  Batch 22/210 - Loss: 1.1830\n",
      "  Batch 23/210 - Loss: 1.2361\n",
      "  Batch 24/210 - Loss: 1.2733\n",
      "  Batch 25/210 - Loss: 1.2758\n",
      "  Batch 26/210 - Loss: 1.2612\n",
      "  Batch 27/210 - Loss: 1.3148\n",
      "  Batch 28/210 - Loss: 1.2887\n",
      "  Batch 29/210 - Loss: 1.3649\n",
      "  Batch 30/210 - Loss: 1.2648\n",
      "  Batch 31/210 - Loss: 1.1422\n",
      "  Batch 32/210 - Loss: 1.4203\n",
      "  Batch 33/210 - Loss: 1.3281\n",
      "  Batch 34/210 - Loss: 1.2550\n",
      "  Batch 35/210 - Loss: 1.2446\n",
      "  Batch 36/210 - Loss: 1.1904\n",
      "  Batch 37/210 - Loss: 1.1739\n",
      "  Batch 38/210 - Loss: 1.1646\n",
      "  Batch 39/210 - Loss: 1.2178\n",
      "  Batch 40/210 - Loss: 1.2922\n",
      "  Batch 41/210 - Loss: 1.2282\n",
      "  Batch 42/210 - Loss: 1.2254\n",
      "  Batch 43/210 - Loss: 1.1361\n",
      "  Batch 44/210 - Loss: 1.1084\n",
      "  Batch 45/210 - Loss: 1.3216\n",
      "  Batch 46/210 - Loss: 1.2228\n",
      "  Batch 47/210 - Loss: 1.2208\n",
      "  Batch 48/210 - Loss: 1.1999\n",
      "  Batch 49/210 - Loss: 1.1651\n",
      "  Batch 50/210 - Loss: 1.2271\n",
      "  Batch 51/210 - Loss: 1.2249\n",
      "  Batch 52/210 - Loss: 1.2515\n",
      "  Batch 53/210 - Loss: 1.2236\n",
      "  Batch 54/210 - Loss: 1.2439\n",
      "  Batch 55/210 - Loss: 1.1043\n",
      "  Batch 56/210 - Loss: 1.0111\n",
      "  Batch 57/210 - Loss: 1.2042\n",
      "  Batch 58/210 - Loss: 1.1765\n",
      "  Batch 59/210 - Loss: 1.0485\n",
      "  Batch 60/210 - Loss: 1.0702\n",
      "  Batch 61/210 - Loss: 1.0197\n",
      "  Batch 62/210 - Loss: 1.1074\n",
      "  Batch 63/210 - Loss: 1.2113\n",
      "  Batch 64/210 - Loss: 1.1422\n",
      "  Batch 65/210 - Loss: 1.1651\n",
      "  Batch 66/210 - Loss: 1.2299\n",
      "  Batch 67/210 - Loss: 1.0317\n",
      "  Batch 68/210 - Loss: 1.0631\n",
      "  Batch 69/210 - Loss: 1.1490\n",
      "  Batch 70/210 - Loss: 1.1984\n",
      "  Batch 71/210 - Loss: 1.1615\n",
      "  Batch 72/210 - Loss: 1.1999\n",
      "  Batch 73/210 - Loss: 1.0811\n",
      "  Batch 74/210 - Loss: 1.1071\n",
      "  Batch 75/210 - Loss: 1.1570\n",
      "  Batch 76/210 - Loss: 1.1512\n",
      "  Batch 77/210 - Loss: 1.0185\n",
      "  Batch 78/210 - Loss: 1.2009\n",
      "  Batch 79/210 - Loss: 1.0759\n",
      "  Batch 80/210 - Loss: 0.9746\n",
      "  Batch 81/210 - Loss: 1.1083\n",
      "  Batch 82/210 - Loss: 1.1366\n",
      "  Batch 83/210 - Loss: 1.1813\n",
      "  Batch 84/210 - Loss: 1.1217\n",
      "  Batch 85/210 - Loss: 1.1409\n",
      "  Batch 86/210 - Loss: 1.0970\n",
      "  Batch 87/210 - Loss: 1.0564\n",
      "  Batch 88/210 - Loss: 1.0773\n",
      "  Batch 89/210 - Loss: 1.1164\n",
      "  Batch 90/210 - Loss: 1.1409\n",
      "  Batch 91/210 - Loss: 1.0266\n",
      "  Batch 92/210 - Loss: 1.1038\n",
      "  Batch 93/210 - Loss: 1.0249\n",
      "  Batch 94/210 - Loss: 1.1486\n",
      "  Batch 95/210 - Loss: 1.0784\n",
      "  Batch 96/210 - Loss: 1.0182\n",
      "  Batch 97/210 - Loss: 0.9408\n",
      "  Batch 98/210 - Loss: 1.1308\n",
      "  Batch 99/210 - Loss: 1.1205\n",
      "  Batch 100/210 - Loss: 1.1494\n",
      "  Batch 101/210 - Loss: 1.0212\n",
      "  Batch 102/210 - Loss: 1.1023\n",
      "  Batch 103/210 - Loss: 1.0507\n",
      "  Batch 104/210 - Loss: 0.9701\n",
      "  Batch 105/210 - Loss: 1.0473\n",
      "  Batch 106/210 - Loss: 1.0428\n",
      "  Batch 107/210 - Loss: 1.0378\n",
      "  Batch 108/210 - Loss: 1.0234\n",
      "  Batch 109/210 - Loss: 1.0383\n",
      "  Batch 110/210 - Loss: 0.9409\n",
      "  Batch 111/210 - Loss: 0.9260\n",
      "  Batch 112/210 - Loss: 1.0856\n",
      "  Batch 113/210 - Loss: 0.9387\n",
      "  Batch 114/210 - Loss: 1.0522\n",
      "  Batch 115/210 - Loss: 0.9311\n",
      "  Batch 116/210 - Loss: 0.9843\n",
      "  Batch 117/210 - Loss: 0.9760\n",
      "  Batch 118/210 - Loss: 0.9351\n",
      "  Batch 119/210 - Loss: 1.0328\n",
      "  Batch 120/210 - Loss: 0.9412\n",
      "  Batch 121/210 - Loss: 0.9776\n",
      "  Batch 122/210 - Loss: 0.8848\n",
      "  Batch 123/210 - Loss: 0.9500\n",
      "  Batch 124/210 - Loss: 0.9489\n",
      "  Batch 125/210 - Loss: 0.8834\n",
      "  Batch 126/210 - Loss: 0.8588\n",
      "  Batch 127/210 - Loss: 1.0547\n",
      "  Batch 128/210 - Loss: 0.9533\n",
      "  Batch 129/210 - Loss: 0.9777\n",
      "  Batch 130/210 - Loss: 1.0465\n",
      "  Batch 131/210 - Loss: 0.9780\n",
      "  Batch 132/210 - Loss: 0.9739\n",
      "  Batch 133/210 - Loss: 0.9336\n",
      "  Batch 134/210 - Loss: 1.0425\n",
      "  Batch 135/210 - Loss: 0.8928\n",
      "  Batch 136/210 - Loss: 0.8995\n",
      "  Batch 137/210 - Loss: 0.9781\n",
      "  Batch 138/210 - Loss: 0.9250\n",
      "  Batch 139/210 - Loss: 0.9535\n",
      "  Batch 140/210 - Loss: 0.9329\n",
      "  Batch 141/210 - Loss: 0.8995\n",
      "  Batch 142/210 - Loss: 0.9716\n",
      "  Batch 143/210 - Loss: 1.0076\n",
      "  Batch 144/210 - Loss: 0.9430\n",
      "  Batch 145/210 - Loss: 0.8758\n",
      "  Batch 146/210 - Loss: 0.9274\n",
      "  Batch 147/210 - Loss: 0.9753\n",
      "  Batch 148/210 - Loss: 0.9983\n",
      "  Batch 149/210 - Loss: 0.9227\n",
      "  Batch 150/210 - Loss: 0.9293\n",
      "  Batch 151/210 - Loss: 0.9547\n",
      "  Batch 152/210 - Loss: 0.8292\n",
      "  Batch 153/210 - Loss: 0.9593\n",
      "  Batch 154/210 - Loss: 0.9386\n",
      "  Batch 155/210 - Loss: 0.9314\n",
      "  Batch 156/210 - Loss: 0.8297\n",
      "  Batch 157/210 - Loss: 0.9626\n",
      "  Batch 158/210 - Loss: 0.9003\n",
      "  Batch 159/210 - Loss: 0.8966\n",
      "  Batch 160/210 - Loss: 0.8653\n",
      "  Batch 161/210 - Loss: 0.8986\n",
      "  Batch 162/210 - Loss: 0.8512\n",
      "  Batch 163/210 - Loss: 0.8840\n",
      "  Batch 164/210 - Loss: 0.9882\n",
      "  Batch 165/210 - Loss: 0.8442\n",
      "  Batch 166/210 - Loss: 0.7516\n",
      "  Batch 167/210 - Loss: 0.9567\n",
      "  Batch 168/210 - Loss: 0.8626\n",
      "  Batch 169/210 - Loss: 0.8347\n",
      "  Batch 170/210 - Loss: 0.8206\n",
      "  Batch 171/210 - Loss: 0.8972\n",
      "  Batch 172/210 - Loss: 0.8553\n",
      "  Batch 173/210 - Loss: 0.8767\n",
      "  Batch 174/210 - Loss: 0.8790\n",
      "  Batch 175/210 - Loss: 0.8146\n",
      "  Batch 176/210 - Loss: 0.8244\n",
      "  Batch 177/210 - Loss: 0.8149\n",
      "  Batch 178/210 - Loss: 0.8289\n",
      "  Batch 179/210 - Loss: 0.7882\n",
      "  Batch 180/210 - Loss: 0.9425\n",
      "  Batch 181/210 - Loss: 0.7951\n",
      "  Batch 182/210 - Loss: 0.7341\n",
      "  Batch 183/210 - Loss: 0.7735\n",
      "  Batch 184/210 - Loss: 0.9109\n",
      "  Batch 185/210 - Loss: 0.8503\n",
      "  Batch 186/210 - Loss: 0.8142\n",
      "  Batch 187/210 - Loss: 0.7757\n",
      "  Batch 188/210 - Loss: 0.8089\n",
      "  Batch 189/210 - Loss: 0.7948\n",
      "  Batch 190/210 - Loss: 0.7918\n",
      "  Batch 191/210 - Loss: 0.7126\n",
      "  Batch 192/210 - Loss: 0.7884\n",
      "  Batch 193/210 - Loss: 0.7778\n",
      "  Batch 194/210 - Loss: 0.7901\n",
      "  Batch 195/210 - Loss: 0.7756\n",
      "  Batch 196/210 - Loss: 0.7932\n",
      "  Batch 197/210 - Loss: 0.8301\n",
      "  Batch 198/210 - Loss: 0.8264\n",
      "  Batch 199/210 - Loss: 0.7138\n",
      "  Batch 200/210 - Loss: 0.8024\n",
      "  Batch 201/210 - Loss: 0.7076\n",
      "  Batch 202/210 - Loss: 0.7947\n",
      "  Batch 203/210 - Loss: 0.7888\n",
      "  Batch 204/210 - Loss: 0.7174\n",
      "  Batch 205/210 - Loss: 0.7915\n",
      "  Batch 206/210 - Loss: 0.7483\n",
      "  Batch 207/210 - Loss: 0.7787\n",
      "  Batch 208/210 - Loss: 0.6954\n",
      "  Batch 209/210 - Loss: 0.7783\n",
      "  Batch 210/210 - Loss: 0.7626\n",
      "Epoch 5 Completed. Train Loss: 1.0380, Val Loss: 0.5459\n",
      "\n",
      "Epoch 6/15\n",
      "  Batch 1/210 - Loss: 0.7600\n",
      "  Batch 2/210 - Loss: 0.7626\n",
      "  Batch 3/210 - Loss: 0.7569\n",
      "  Batch 4/210 - Loss: 0.7401\n",
      "  Batch 5/210 - Loss: 0.7487\n",
      "  Batch 6/210 - Loss: 0.7661\n",
      "  Batch 7/210 - Loss: 0.6848\n",
      "  Batch 8/210 - Loss: 0.5926\n",
      "  Batch 9/210 - Loss: 0.7548\n",
      "  Batch 10/210 - Loss: 0.7420\n",
      "  Batch 11/210 - Loss: 0.7025\n",
      "  Batch 12/210 - Loss: 0.7659\n",
      "  Batch 13/210 - Loss: 0.7662\n",
      "  Batch 14/210 - Loss: 0.7579\n",
      "  Batch 15/210 - Loss: 0.6627\n",
      "  Batch 16/210 - Loss: 0.6874\n",
      "  Batch 17/210 - Loss: 0.6133\n",
      "  Batch 18/210 - Loss: 0.6826\n",
      "  Batch 19/210 - Loss: 0.7636\n",
      "  Batch 20/210 - Loss: 0.7310\n",
      "  Batch 21/210 - Loss: 0.6783\n",
      "  Batch 22/210 - Loss: 0.6799\n",
      "  Batch 23/210 - Loss: 0.6799\n",
      "  Batch 24/210 - Loss: 0.6454\n",
      "  Batch 25/210 - Loss: 0.7118\n",
      "  Batch 26/210 - Loss: 0.7564\n",
      "  Batch 27/210 - Loss: 0.7218\n",
      "  Batch 28/210 - Loss: 0.6651\n",
      "  Batch 29/210 - Loss: 0.7234\n",
      "  Batch 30/210 - Loss: 0.6135\n",
      "  Batch 31/210 - Loss: 0.7352\n",
      "  Batch 32/210 - Loss: 0.6824\n",
      "  Batch 33/210 - Loss: 0.6147\n",
      "  Batch 34/210 - Loss: 0.6323\n",
      "  Batch 35/210 - Loss: 0.6726\n",
      "  Batch 36/210 - Loss: 0.5767\n",
      "  Batch 37/210 - Loss: 0.6454\n",
      "  Batch 38/210 - Loss: 0.6927\n",
      "  Batch 39/210 - Loss: 0.6281\n",
      "  Batch 40/210 - Loss: 0.6548\n",
      "  Batch 41/210 - Loss: 0.6001\n",
      "  Batch 42/210 - Loss: 0.6954\n",
      "  Batch 43/210 - Loss: 0.6457\n",
      "  Batch 44/210 - Loss: 0.6773\n",
      "  Batch 45/210 - Loss: 0.6888\n",
      "  Batch 46/210 - Loss: 0.6722\n",
      "  Batch 47/210 - Loss: 0.6797\n",
      "  Batch 48/210 - Loss: 0.7296\n",
      "  Batch 49/210 - Loss: 0.6202\n",
      "  Batch 50/210 - Loss: 0.5947\n",
      "  Batch 51/210 - Loss: 0.6542\n",
      "  Batch 52/210 - Loss: 0.6845\n",
      "  Batch 53/210 - Loss: 0.7146\n",
      "  Batch 54/210 - Loss: 0.6352\n",
      "  Batch 55/210 - Loss: 0.5968\n",
      "  Batch 56/210 - Loss: 0.5708\n",
      "  Batch 57/210 - Loss: 0.6660\n",
      "  Batch 58/210 - Loss: 0.6570\n",
      "  Batch 59/210 - Loss: 0.5705\n",
      "  Batch 60/210 - Loss: 0.6655\n",
      "  Batch 61/210 - Loss: 0.5931\n",
      "  Batch 62/210 - Loss: 0.6304\n",
      "  Batch 63/210 - Loss: 0.6139\n",
      "  Batch 64/210 - Loss: 0.5394\n",
      "  Batch 65/210 - Loss: 0.6419\n",
      "  Batch 66/210 - Loss: 0.5784\n",
      "  Batch 67/210 - Loss: 0.6175\n",
      "  Batch 68/210 - Loss: 0.6469\n",
      "  Batch 69/210 - Loss: 0.5967\n",
      "  Batch 70/210 - Loss: 0.6173\n",
      "  Batch 71/210 - Loss: 0.6434\n",
      "  Batch 72/210 - Loss: 0.6156\n",
      "  Batch 73/210 - Loss: 0.6525\n",
      "  Batch 74/210 - Loss: 0.6098\n",
      "  Batch 75/210 - Loss: 0.6455\n",
      "  Batch 76/210 - Loss: 0.6002\n",
      "  Batch 77/210 - Loss: 0.6532\n",
      "  Batch 78/210 - Loss: 0.5703\n",
      "  Batch 79/210 - Loss: 0.6560\n",
      "  Batch 80/210 - Loss: 0.6009\n",
      "  Batch 81/210 - Loss: 0.5991\n",
      "  Batch 82/210 - Loss: 0.5912\n",
      "  Batch 83/210 - Loss: 0.6092\n",
      "  Batch 84/210 - Loss: 0.5812\n",
      "  Batch 85/210 - Loss: 0.5488\n",
      "  Batch 86/210 - Loss: 0.5800\n",
      "  Batch 87/210 - Loss: 0.5936\n",
      "  Batch 88/210 - Loss: 0.6028\n",
      "  Batch 89/210 - Loss: 0.5899\n",
      "  Batch 90/210 - Loss: 0.6003\n",
      "  Batch 91/210 - Loss: 0.6172\n",
      "  Batch 92/210 - Loss: 0.6007\n",
      "  Batch 93/210 - Loss: 0.5762\n",
      "  Batch 94/210 - Loss: 0.5762\n",
      "  Batch 95/210 - Loss: 0.6410\n",
      "  Batch 96/210 - Loss: 0.5283\n",
      "  Batch 97/210 - Loss: 0.5637\n",
      "  Batch 98/210 - Loss: 0.6202\n",
      "  Batch 99/210 - Loss: 0.6198\n",
      "  Batch 100/210 - Loss: 0.6566\n",
      "  Batch 101/210 - Loss: 0.6344\n",
      "  Batch 102/210 - Loss: 0.5965\n",
      "  Batch 103/210 - Loss: 0.5350\n",
      "  Batch 104/210 - Loss: 0.5085\n",
      "  Batch 105/210 - Loss: 0.5766\n",
      "  Batch 106/210 - Loss: 0.5905\n",
      "  Batch 107/210 - Loss: 0.5490\n",
      "  Batch 108/210 - Loss: 0.5035\n",
      "  Batch 109/210 - Loss: 0.4954\n",
      "  Batch 110/210 - Loss: 0.5680\n",
      "  Batch 111/210 - Loss: 0.5703\n",
      "  Batch 112/210 - Loss: 0.5006\n",
      "  Batch 113/210 - Loss: 0.6062\n",
      "  Batch 114/210 - Loss: 0.5454\n",
      "  Batch 115/210 - Loss: 0.5507\n",
      "  Batch 116/210 - Loss: 0.6101\n",
      "  Batch 117/210 - Loss: 0.5371\n",
      "  Batch 118/210 - Loss: 0.5440\n",
      "  Batch 119/210 - Loss: 0.5027\n",
      "  Batch 120/210 - Loss: 0.5872\n",
      "  Batch 121/210 - Loss: 0.5203\n",
      "  Batch 122/210 - Loss: 0.5257\n",
      "  Batch 123/210 - Loss: 0.5345\n",
      "  Batch 124/210 - Loss: 0.5134\n",
      "  Batch 125/210 - Loss: 0.5508\n",
      "  Batch 126/210 - Loss: 0.5781\n",
      "  Batch 127/210 - Loss: 0.5270\n",
      "  Batch 128/210 - Loss: 0.5175\n",
      "  Batch 129/210 - Loss: 0.5064\n",
      "  Batch 130/210 - Loss: 0.5758\n",
      "  Batch 131/210 - Loss: 0.4861\n",
      "  Batch 132/210 - Loss: 0.5119\n",
      "  Batch 133/210 - Loss: 0.3991\n",
      "  Batch 134/210 - Loss: 0.5309\n",
      "  Batch 135/210 - Loss: 0.4828\n",
      "  Batch 136/210 - Loss: 0.4932\n",
      "  Batch 137/210 - Loss: 0.5600\n",
      "  Batch 138/210 - Loss: 0.5323\n",
      "  Batch 139/210 - Loss: 0.5066\n",
      "  Batch 140/210 - Loss: 0.5287\n",
      "  Batch 141/210 - Loss: 0.4329\n",
      "  Batch 142/210 - Loss: 0.5283\n",
      "  Batch 143/210 - Loss: 0.4743\n",
      "  Batch 144/210 - Loss: 0.5018\n",
      "  Batch 145/210 - Loss: 0.4745\n",
      "  Batch 146/210 - Loss: 0.4849\n",
      "  Batch 147/210 - Loss: 0.4290\n",
      "  Batch 148/210 - Loss: 0.4766\n",
      "  Batch 149/210 - Loss: 0.4909\n",
      "  Batch 150/210 - Loss: 0.5198\n",
      "  Batch 151/210 - Loss: 0.4589\n",
      "  Batch 152/210 - Loss: 0.5156\n",
      "  Batch 153/210 - Loss: 0.5332\n",
      "  Batch 154/210 - Loss: 0.4643\n",
      "  Batch 155/210 - Loss: 0.5199\n",
      "  Batch 156/210 - Loss: 0.5224\n",
      "  Batch 157/210 - Loss: 0.4543\n",
      "  Batch 158/210 - Loss: 0.4802\n",
      "  Batch 159/210 - Loss: 0.5949\n",
      "  Batch 160/210 - Loss: 0.5475\n",
      "  Batch 161/210 - Loss: 0.4964\n",
      "  Batch 162/210 - Loss: 0.5118\n",
      "  Batch 163/210 - Loss: 0.5310\n",
      "  Batch 164/210 - Loss: 0.4955\n",
      "  Batch 165/210 - Loss: 0.4880\n",
      "  Batch 166/210 - Loss: 0.4493\n",
      "  Batch 167/210 - Loss: 0.5172\n",
      "  Batch 168/210 - Loss: 0.5192\n",
      "  Batch 169/210 - Loss: 0.4987\n",
      "  Batch 170/210 - Loss: 0.4659\n",
      "  Batch 171/210 - Loss: 0.6227\n",
      "  Batch 172/210 - Loss: 0.5038\n",
      "  Batch 173/210 - Loss: 0.5481\n",
      "  Batch 174/210 - Loss: 0.4579\n",
      "  Batch 175/210 - Loss: 0.4301\n",
      "  Batch 176/210 - Loss: 0.4748\n",
      "  Batch 177/210 - Loss: 0.4317\n",
      "  Batch 178/210 - Loss: 0.5012\n",
      "  Batch 179/210 - Loss: 0.4423\n",
      "  Batch 180/210 - Loss: 0.4703\n",
      "  Batch 181/210 - Loss: 0.4506\n",
      "  Batch 182/210 - Loss: 0.5179\n",
      "  Batch 183/210 - Loss: 0.3829\n",
      "  Batch 184/210 - Loss: 0.4385\n",
      "  Batch 185/210 - Loss: 0.4066\n",
      "  Batch 186/210 - Loss: 0.4789\n",
      "  Batch 187/210 - Loss: 0.4903\n",
      "  Batch 188/210 - Loss: 0.4381\n",
      "  Batch 189/210 - Loss: 0.4807\n",
      "  Batch 190/210 - Loss: 0.4088\n",
      "  Batch 191/210 - Loss: 0.4357\n",
      "  Batch 192/210 - Loss: 0.4375\n",
      "  Batch 193/210 - Loss: 0.4516\n",
      "  Batch 194/210 - Loss: 0.4431\n",
      "  Batch 195/210 - Loss: 0.4385\n",
      "  Batch 196/210 - Loss: 0.4777\n",
      "  Batch 197/210 - Loss: 0.4749\n",
      "  Batch 198/210 - Loss: 0.4344\n",
      "  Batch 199/210 - Loss: 0.4297\n",
      "  Batch 200/210 - Loss: 0.3770\n",
      "  Batch 201/210 - Loss: 0.4862\n",
      "  Batch 202/210 - Loss: 0.4965\n",
      "  Batch 203/210 - Loss: 0.4689\n",
      "  Batch 204/210 - Loss: 0.4604\n",
      "  Batch 205/210 - Loss: 0.3752\n",
      "  Batch 206/210 - Loss: 0.4095\n",
      "  Batch 207/210 - Loss: 0.3980\n",
      "  Batch 208/210 - Loss: 0.4607\n",
      "  Batch 209/210 - Loss: 0.4343\n",
      "  Batch 210/210 - Loss: 0.3798\n",
      "Epoch 6 Completed. Train Loss: 0.5695, Val Loss: 0.2771\n",
      "\n",
      "Epoch 7/15\n",
      "  Batch 1/210 - Loss: 0.4107\n",
      "  Batch 2/210 - Loss: 0.3805\n",
      "  Batch 3/210 - Loss: 0.3598\n",
      "  Batch 4/210 - Loss: 0.4009\n",
      "  Batch 5/210 - Loss: 0.4079\n",
      "  Batch 6/210 - Loss: 0.3576\n",
      "  Batch 7/210 - Loss: 0.4154\n",
      "  Batch 8/210 - Loss: 0.4550\n",
      "  Batch 9/210 - Loss: 0.4078\n",
      "  Batch 10/210 - Loss: 0.4210\n",
      "  Batch 11/210 - Loss: 0.4091\n",
      "  Batch 12/210 - Loss: 0.3769\n",
      "  Batch 13/210 - Loss: 0.3739\n",
      "  Batch 14/210 - Loss: 0.4312\n",
      "  Batch 15/210 - Loss: 0.4079\n",
      "  Batch 16/210 - Loss: 0.4010\n",
      "  Batch 17/210 - Loss: 0.3998\n",
      "  Batch 18/210 - Loss: 0.4109\n",
      "  Batch 19/210 - Loss: 0.4174\n",
      "  Batch 20/210 - Loss: 0.4198\n",
      "  Batch 21/210 - Loss: 0.3735\n",
      "  Batch 22/210 - Loss: 0.3708\n",
      "  Batch 23/210 - Loss: 0.3680\n",
      "  Batch 24/210 - Loss: 0.3989\n",
      "  Batch 25/210 - Loss: 0.4340\n",
      "  Batch 26/210 - Loss: 0.3556\n",
      "  Batch 27/210 - Loss: 0.3633\n",
      "  Batch 28/210 - Loss: 0.3698\n",
      "  Batch 29/210 - Loss: 0.4176\n",
      "  Batch 30/210 - Loss: 0.3493\n",
      "  Batch 31/210 - Loss: 0.3575\n",
      "  Batch 32/210 - Loss: 0.3228\n",
      "  Batch 33/210 - Loss: 0.3711\n",
      "  Batch 34/210 - Loss: 0.3540\n",
      "  Batch 35/210 - Loss: 0.4018\n",
      "  Batch 36/210 - Loss: 0.4113\n",
      "  Batch 37/210 - Loss: 0.3365\n",
      "  Batch 38/210 - Loss: 0.3710\n",
      "  Batch 39/210 - Loss: 0.3161\n",
      "  Batch 40/210 - Loss: 0.3841\n",
      "  Batch 41/210 - Loss: 0.3374\n",
      "  Batch 42/210 - Loss: 0.3698\n",
      "  Batch 43/210 - Loss: 0.3777\n",
      "  Batch 44/210 - Loss: 0.4405\n",
      "  Batch 45/210 - Loss: 0.3704\n",
      "  Batch 46/210 - Loss: 0.3292\n",
      "  Batch 47/210 - Loss: 0.3962\n",
      "  Batch 48/210 - Loss: 0.3528\n",
      "  Batch 49/210 - Loss: 0.3525\n",
      "  Batch 50/210 - Loss: 0.3837\n",
      "  Batch 51/210 - Loss: 0.3220\n",
      "  Batch 52/210 - Loss: 0.3764\n",
      "  Batch 53/210 - Loss: 0.3586\n",
      "  Batch 54/210 - Loss: 0.3595\n",
      "  Batch 55/210 - Loss: 0.3593\n",
      "  Batch 56/210 - Loss: 0.3643\n",
      "  Batch 57/210 - Loss: 0.3505\n",
      "  Batch 58/210 - Loss: 0.3310\n",
      "  Batch 59/210 - Loss: 0.3873\n",
      "  Batch 60/210 - Loss: 0.3538\n",
      "  Batch 61/210 - Loss: 0.3690\n",
      "  Batch 62/210 - Loss: 0.4024\n",
      "  Batch 63/210 - Loss: 0.3487\n",
      "  Batch 64/210 - Loss: 0.3298\n",
      "  Batch 65/210 - Loss: 0.3989\n",
      "  Batch 66/210 - Loss: 0.3469\n",
      "  Batch 67/210 - Loss: 0.3314\n",
      "  Batch 68/210 - Loss: 0.3604\n",
      "  Batch 69/210 - Loss: 0.3560\n",
      "  Batch 70/210 - Loss: 0.3043\n",
      "  Batch 71/210 - Loss: 0.3216\n",
      "  Batch 72/210 - Loss: 0.3282\n",
      "  Batch 73/210 - Loss: 0.3675\n",
      "  Batch 74/210 - Loss: 0.3713\n",
      "  Batch 75/210 - Loss: 0.3404\n",
      "  Batch 76/210 - Loss: 0.3006\n",
      "  Batch 77/210 - Loss: 0.3758\n",
      "  Batch 78/210 - Loss: 0.3825\n",
      "  Batch 79/210 - Loss: 0.3069\n",
      "  Batch 80/210 - Loss: 0.3404\n",
      "  Batch 81/210 - Loss: 0.3490\n",
      "  Batch 82/210 - Loss: 0.2963\n",
      "  Batch 83/210 - Loss: 0.3450\n",
      "  Batch 84/210 - Loss: 0.3533\n",
      "  Batch 85/210 - Loss: 0.3675\n",
      "  Batch 86/210 - Loss: 0.3031\n",
      "  Batch 87/210 - Loss: 0.3263\n",
      "  Batch 88/210 - Loss: 0.2855\n",
      "  Batch 89/210 - Loss: 0.3208\n",
      "  Batch 90/210 - Loss: 0.3137\n",
      "  Batch 91/210 - Loss: 0.3318\n",
      "  Batch 92/210 - Loss: 0.3256\n",
      "  Batch 93/210 - Loss: 0.3228\n",
      "  Batch 94/210 - Loss: 0.3491\n",
      "  Batch 95/210 - Loss: 0.2938\n",
      "  Batch 96/210 - Loss: 0.3465\n",
      "  Batch 97/210 - Loss: 0.3442\n",
      "  Batch 98/210 - Loss: 0.2858\n",
      "  Batch 99/210 - Loss: 0.3294\n",
      "  Batch 100/210 - Loss: 0.2912\n",
      "  Batch 101/210 - Loss: 0.3218\n",
      "  Batch 102/210 - Loss: 0.3220\n",
      "  Batch 103/210 - Loss: 0.3013\n",
      "  Batch 104/210 - Loss: 0.3226\n",
      "  Batch 105/210 - Loss: 0.3630\n",
      "  Batch 106/210 - Loss: 0.3046\n",
      "  Batch 107/210 - Loss: 0.3571\n",
      "  Batch 108/210 - Loss: 0.3413\n",
      "  Batch 109/210 - Loss: 0.3058\n",
      "  Batch 110/210 - Loss: 0.3045\n",
      "  Batch 111/210 - Loss: 0.3044\n",
      "  Batch 112/210 - Loss: 0.3336\n",
      "  Batch 113/210 - Loss: 0.2978\n",
      "  Batch 114/210 - Loss: 0.3026\n",
      "  Batch 115/210 - Loss: 0.2611\n",
      "  Batch 116/210 - Loss: 0.3061\n",
      "  Batch 117/210 - Loss: 0.3387\n",
      "  Batch 118/210 - Loss: 0.2743\n",
      "  Batch 119/210 - Loss: 0.3331\n",
      "  Batch 120/210 - Loss: 0.3222\n",
      "  Batch 121/210 - Loss: 0.2984\n",
      "  Batch 122/210 - Loss: 0.2977\n",
      "  Batch 123/210 - Loss: 0.2978\n",
      "  Batch 124/210 - Loss: 0.2781\n",
      "  Batch 125/210 - Loss: 0.3874\n",
      "  Batch 126/210 - Loss: 0.2899\n",
      "  Batch 127/210 - Loss: 0.2600\n",
      "  Batch 128/210 - Loss: 0.2717\n",
      "  Batch 129/210 - Loss: 0.3033\n",
      "  Batch 130/210 - Loss: 0.2635\n",
      "  Batch 131/210 - Loss: 0.2709\n",
      "  Batch 132/210 - Loss: 0.3665\n",
      "  Batch 133/210 - Loss: 0.3017\n",
      "  Batch 134/210 - Loss: 0.3333\n",
      "  Batch 135/210 - Loss: 0.2480\n",
      "  Batch 136/210 - Loss: 0.3175\n",
      "  Batch 137/210 - Loss: 0.2916\n",
      "  Batch 138/210 - Loss: 0.2966\n",
      "  Batch 139/210 - Loss: 0.2759\n",
      "  Batch 140/210 - Loss: 0.2562\n",
      "  Batch 141/210 - Loss: 0.2800\n",
      "  Batch 142/210 - Loss: 0.2595\n",
      "  Batch 143/210 - Loss: 0.2760\n",
      "  Batch 144/210 - Loss: 0.2871\n",
      "  Batch 145/210 - Loss: 0.2599\n",
      "  Batch 146/210 - Loss: 0.2678\n",
      "  Batch 147/210 - Loss: 0.2504\n",
      "  Batch 148/210 - Loss: 0.2745\n",
      "  Batch 149/210 - Loss: 0.3147\n",
      "  Batch 150/210 - Loss: 0.2960\n",
      "  Batch 151/210 - Loss: 0.2678\n",
      "  Batch 152/210 - Loss: 0.2949\n",
      "  Batch 153/210 - Loss: 0.3174\n",
      "  Batch 154/210 - Loss: 0.3483\n",
      "  Batch 155/210 - Loss: 0.2605\n",
      "  Batch 156/210 - Loss: 0.2915\n",
      "  Batch 157/210 - Loss: 0.3000\n",
      "  Batch 158/210 - Loss: 0.2955\n",
      "  Batch 159/210 - Loss: 0.2767\n",
      "  Batch 160/210 - Loss: 0.2645\n",
      "  Batch 161/210 - Loss: 0.2864\n",
      "  Batch 162/210 - Loss: 0.2790\n",
      "  Batch 163/210 - Loss: 0.2797\n",
      "  Batch 164/210 - Loss: 0.3010\n",
      "  Batch 165/210 - Loss: 0.2766\n",
      "  Batch 166/210 - Loss: 0.2195\n",
      "  Batch 167/210 - Loss: 0.2712\n",
      "  Batch 168/210 - Loss: 0.2759\n",
      "  Batch 169/210 - Loss: 0.3068\n",
      "  Batch 170/210 - Loss: 0.2683\n",
      "  Batch 171/210 - Loss: 0.2755\n",
      "  Batch 172/210 - Loss: 0.2684\n",
      "  Batch 173/210 - Loss: 0.3186\n",
      "  Batch 174/210 - Loss: 0.2952\n",
      "  Batch 175/210 - Loss: 0.2852\n",
      "  Batch 176/210 - Loss: 0.2427\n",
      "  Batch 177/210 - Loss: 0.2951\n",
      "  Batch 178/210 - Loss: 0.2613\n",
      "  Batch 179/210 - Loss: 0.2705\n",
      "  Batch 180/210 - Loss: 0.2371\n",
      "  Batch 181/210 - Loss: 0.2482\n",
      "  Batch 182/210 - Loss: 0.2931\n",
      "  Batch 183/210 - Loss: 0.2561\n",
      "  Batch 184/210 - Loss: 0.2114\n",
      "  Batch 185/210 - Loss: 0.2644\n",
      "  Batch 186/210 - Loss: 0.2341\n",
      "  Batch 187/210 - Loss: 0.2240\n",
      "  Batch 188/210 - Loss: 0.2262\n",
      "  Batch 189/210 - Loss: 0.2431\n",
      "  Batch 190/210 - Loss: 0.2273\n",
      "  Batch 191/210 - Loss: 0.2616\n",
      "  Batch 192/210 - Loss: 0.2685\n",
      "  Batch 193/210 - Loss: 0.2087\n",
      "  Batch 194/210 - Loss: 0.2201\n",
      "  Batch 195/210 - Loss: 0.2455\n",
      "  Batch 196/210 - Loss: 0.2176\n",
      "  Batch 197/210 - Loss: 0.2257\n",
      "  Batch 198/210 - Loss: 0.2370\n",
      "  Batch 199/210 - Loss: 0.2856\n",
      "  Batch 200/210 - Loss: 0.2120\n",
      "  Batch 201/210 - Loss: 0.2374\n",
      "  Batch 202/210 - Loss: 0.2407\n",
      "  Batch 203/210 - Loss: 0.2318\n",
      "  Batch 204/210 - Loss: 0.2095\n",
      "  Batch 205/210 - Loss: 0.2980\n",
      "  Batch 206/210 - Loss: 0.2570\n",
      "  Batch 207/210 - Loss: 0.2594\n",
      "  Batch 208/210 - Loss: 0.2598\n",
      "  Batch 209/210 - Loss: 0.2456\n",
      "  Batch 210/210 - Loss: 0.2537\n",
      "Epoch 7 Completed. Train Loss: 0.3188, Val Loss: 0.1401\n",
      "\n",
      "Epoch 8/15\n",
      "  Batch 1/210 - Loss: 0.2096\n",
      "  Batch 2/210 - Loss: 0.2175\n",
      "  Batch 3/210 - Loss: 0.2323\n",
      "  Batch 4/210 - Loss: 0.2149\n",
      "  Batch 5/210 - Loss: 0.2403\n",
      "  Batch 6/210 - Loss: 0.2458\n",
      "  Batch 7/210 - Loss: 0.2070\n",
      "  Batch 8/210 - Loss: 0.2486\n",
      "  Batch 9/210 - Loss: 0.2590\n",
      "  Batch 10/210 - Loss: 0.2122\n",
      "  Batch 11/210 - Loss: 0.2042\n",
      "  Batch 12/210 - Loss: 0.2242\n",
      "  Batch 13/210 - Loss: 0.2158\n",
      "  Batch 14/210 - Loss: 0.2537\n",
      "  Batch 15/210 - Loss: 0.2371\n",
      "  Batch 16/210 - Loss: 0.1959\n",
      "  Batch 17/210 - Loss: 0.2562\n",
      "  Batch 18/210 - Loss: 0.2378\n",
      "  Batch 19/210 - Loss: 0.2806\n",
      "  Batch 20/210 - Loss: 0.2173\n",
      "  Batch 21/210 - Loss: 0.2040\n",
      "  Batch 22/210 - Loss: 0.2403\n",
      "  Batch 23/210 - Loss: 0.2481\n",
      "  Batch 24/210 - Loss: 0.2217\n",
      "  Batch 25/210 - Loss: 0.2004\n",
      "  Batch 26/210 - Loss: 0.2113\n",
      "  Batch 27/210 - Loss: 0.2483\n",
      "  Batch 28/210 - Loss: 0.2378\n",
      "  Batch 29/210 - Loss: 0.2214\n",
      "  Batch 30/210 - Loss: 0.2001\n",
      "  Batch 31/210 - Loss: 0.2029\n",
      "  Batch 32/210 - Loss: 0.2356\n",
      "  Batch 33/210 - Loss: 0.1719\n",
      "  Batch 34/210 - Loss: 0.1849\n",
      "  Batch 35/210 - Loss: 0.1885\n",
      "  Batch 36/210 - Loss: 0.1904\n",
      "  Batch 37/210 - Loss: 0.2216\n",
      "  Batch 38/210 - Loss: 0.2106\n",
      "  Batch 39/210 - Loss: 0.2284\n",
      "  Batch 40/210 - Loss: 0.2032\n",
      "  Batch 41/210 - Loss: 0.1853\n",
      "  Batch 42/210 - Loss: 0.2207\n",
      "  Batch 43/210 - Loss: 0.2178\n",
      "  Batch 44/210 - Loss: 0.2025\n",
      "  Batch 45/210 - Loss: 0.1949\n",
      "  Batch 46/210 - Loss: 0.2072\n",
      "  Batch 47/210 - Loss: 0.1802\n",
      "  Batch 48/210 - Loss: 0.1812\n",
      "  Batch 49/210 - Loss: 0.2109\n",
      "  Batch 50/210 - Loss: 0.2119\n",
      "  Batch 51/210 - Loss: 0.2221\n",
      "  Batch 52/210 - Loss: 0.1622\n",
      "  Batch 53/210 - Loss: 0.2283\n",
      "  Batch 54/210 - Loss: 0.1944\n",
      "  Batch 55/210 - Loss: 0.2074\n",
      "  Batch 56/210 - Loss: 0.1960\n",
      "  Batch 57/210 - Loss: 0.1990\n",
      "  Batch 58/210 - Loss: 0.2138\n",
      "  Batch 59/210 - Loss: 0.2121\n",
      "  Batch 60/210 - Loss: 0.1865\n",
      "  Batch 61/210 - Loss: 0.1973\n",
      "  Batch 62/210 - Loss: 0.1879\n",
      "  Batch 63/210 - Loss: 0.2155\n",
      "  Batch 64/210 - Loss: 0.1879\n",
      "  Batch 65/210 - Loss: 0.1755\n",
      "  Batch 66/210 - Loss: 0.2272\n",
      "  Batch 67/210 - Loss: 0.1712\n",
      "  Batch 68/210 - Loss: 0.1924\n",
      "  Batch 69/210 - Loss: 0.2195\n",
      "  Batch 70/210 - Loss: 0.2083\n",
      "  Batch 71/210 - Loss: 0.2058\n",
      "  Batch 72/210 - Loss: 0.2312\n",
      "  Batch 73/210 - Loss: 0.2193\n",
      "  Batch 74/210 - Loss: 0.1981\n",
      "  Batch 75/210 - Loss: 0.1893\n",
      "  Batch 76/210 - Loss: 0.1900\n",
      "  Batch 77/210 - Loss: 0.1974\n",
      "  Batch 78/210 - Loss: 0.1873\n",
      "  Batch 79/210 - Loss: 0.1940\n",
      "  Batch 80/210 - Loss: 0.1827\n",
      "  Batch 81/210 - Loss: 0.1851\n",
      "  Batch 82/210 - Loss: 0.2425\n",
      "  Batch 83/210 - Loss: 0.1940\n",
      "  Batch 84/210 - Loss: 0.1923\n",
      "  Batch 85/210 - Loss: 0.1989\n",
      "  Batch 86/210 - Loss: 0.1791\n",
      "  Batch 87/210 - Loss: 0.2222\n",
      "  Batch 88/210 - Loss: 0.2099\n",
      "  Batch 89/210 - Loss: 0.1460\n",
      "  Batch 90/210 - Loss: 0.1711\n",
      "  Batch 91/210 - Loss: 0.1679\n",
      "  Batch 92/210 - Loss: 0.1829\n",
      "  Batch 93/210 - Loss: 0.1660\n",
      "  Batch 94/210 - Loss: 0.2026\n",
      "  Batch 95/210 - Loss: 0.2066\n",
      "  Batch 96/210 - Loss: 0.1948\n",
      "  Batch 97/210 - Loss: 0.1621\n",
      "  Batch 98/210 - Loss: 0.1813\n",
      "  Batch 99/210 - Loss: 0.1738\n",
      "  Batch 100/210 - Loss: 0.1796\n",
      "  Batch 101/210 - Loss: 0.1892\n",
      "  Batch 102/210 - Loss: 0.1622\n",
      "  Batch 103/210 - Loss: 0.1422\n",
      "  Batch 104/210 - Loss: 0.1859\n",
      "  Batch 105/210 - Loss: 0.1736\n",
      "  Batch 106/210 - Loss: 0.2037\n",
      "  Batch 107/210 - Loss: 0.1772\n",
      "  Batch 108/210 - Loss: 0.1749\n",
      "  Batch 109/210 - Loss: 0.1808\n",
      "  Batch 110/210 - Loss: 0.1714\n",
      "  Batch 111/210 - Loss: 0.1819\n",
      "  Batch 112/210 - Loss: 0.1601\n",
      "  Batch 113/210 - Loss: 0.1398\n",
      "  Batch 114/210 - Loss: 0.1779\n",
      "  Batch 115/210 - Loss: 0.1641\n",
      "  Batch 116/210 - Loss: 0.1766\n",
      "  Batch 117/210 - Loss: 0.2010\n",
      "  Batch 118/210 - Loss: 0.1907\n",
      "  Batch 119/210 - Loss: 0.1891\n",
      "  Batch 120/210 - Loss: 0.1714\n",
      "  Batch 121/210 - Loss: 0.2422\n",
      "  Batch 122/210 - Loss: 0.1893\n",
      "  Batch 123/210 - Loss: 0.1811\n",
      "  Batch 124/210 - Loss: 0.1616\n",
      "  Batch 125/210 - Loss: 0.1667\n",
      "  Batch 126/210 - Loss: 0.1454\n",
      "  Batch 127/210 - Loss: 0.1745\n",
      "  Batch 128/210 - Loss: 0.1631\n",
      "  Batch 129/210 - Loss: 0.1603\n",
      "  Batch 130/210 - Loss: 0.1837\n",
      "  Batch 131/210 - Loss: 0.1935\n",
      "  Batch 132/210 - Loss: 0.1602\n",
      "  Batch 133/210 - Loss: 0.1915\n",
      "  Batch 134/210 - Loss: 0.1869\n",
      "  Batch 135/210 - Loss: 0.1685\n",
      "  Batch 136/210 - Loss: 0.1577\n",
      "  Batch 137/210 - Loss: 0.1593\n",
      "  Batch 138/210 - Loss: 0.1800\n",
      "  Batch 139/210 - Loss: 0.1962\n",
      "  Batch 140/210 - Loss: 0.1636\n",
      "  Batch 141/210 - Loss: 0.1441\n",
      "  Batch 142/210 - Loss: 0.1461\n",
      "  Batch 143/210 - Loss: 0.1482\n",
      "  Batch 144/210 - Loss: 0.1791\n",
      "  Batch 145/210 - Loss: 0.1806\n",
      "  Batch 146/210 - Loss: 0.1682\n",
      "  Batch 147/210 - Loss: 0.1685\n",
      "  Batch 148/210 - Loss: 0.1698\n",
      "  Batch 149/210 - Loss: 0.1570\n",
      "  Batch 150/210 - Loss: 0.1714\n",
      "  Batch 151/210 - Loss: 0.1650\n",
      "  Batch 152/210 - Loss: 0.1399\n",
      "  Batch 153/210 - Loss: 0.1503\n",
      "  Batch 154/210 - Loss: 0.1839\n",
      "  Batch 155/210 - Loss: 0.1741\n",
      "  Batch 156/210 - Loss: 0.1793\n",
      "  Batch 157/210 - Loss: 0.1842\n",
      "  Batch 158/210 - Loss: 0.1807\n",
      "  Batch 159/210 - Loss: 0.1532\n",
      "  Batch 160/210 - Loss: 0.2179\n",
      "  Batch 161/210 - Loss: 0.1711\n",
      "  Batch 162/210 - Loss: 0.1609\n",
      "  Batch 163/210 - Loss: 0.1488\n",
      "  Batch 164/210 - Loss: 0.1649\n",
      "  Batch 165/210 - Loss: 0.1300\n",
      "  Batch 166/210 - Loss: 0.1863\n",
      "  Batch 167/210 - Loss: 0.1275\n",
      "  Batch 168/210 - Loss: 0.1454\n",
      "  Batch 169/210 - Loss: 0.1988\n",
      "  Batch 170/210 - Loss: 0.1655\n",
      "  Batch 171/210 - Loss: 0.1624\n",
      "  Batch 172/210 - Loss: 0.1505\n",
      "  Batch 173/210 - Loss: 0.1686\n",
      "  Batch 174/210 - Loss: 0.1552\n",
      "  Batch 175/210 - Loss: 0.1247\n",
      "  Batch 176/210 - Loss: 0.1602\n",
      "  Batch 177/210 - Loss: 0.1752\n",
      "  Batch 178/210 - Loss: 0.1678\n",
      "  Batch 179/210 - Loss: 0.1446\n",
      "  Batch 180/210 - Loss: 0.1527\n",
      "  Batch 181/210 - Loss: 0.1773\n",
      "  Batch 182/210 - Loss: 0.1681\n",
      "  Batch 183/210 - Loss: 0.1719\n",
      "  Batch 184/210 - Loss: 0.1821\n",
      "  Batch 185/210 - Loss: 0.1544\n",
      "  Batch 186/210 - Loss: 0.1355\n",
      "  Batch 187/210 - Loss: 0.1781\n",
      "  Batch 188/210 - Loss: 0.1680\n",
      "  Batch 189/210 - Loss: 0.1239\n",
      "  Batch 190/210 - Loss: 0.1753\n",
      "  Batch 191/210 - Loss: 0.1510\n",
      "  Batch 192/210 - Loss: 0.1223\n",
      "  Batch 193/210 - Loss: 0.1660\n",
      "  Batch 194/210 - Loss: 0.1351\n",
      "  Batch 195/210 - Loss: 0.1652\n",
      "  Batch 196/210 - Loss: 0.1536\n",
      "  Batch 197/210 - Loss: 0.1370\n",
      "  Batch 198/210 - Loss: 0.1487\n",
      "  Batch 199/210 - Loss: 0.1796\n",
      "  Batch 200/210 - Loss: 0.1444\n",
      "  Batch 201/210 - Loss: 0.1157\n",
      "  Batch 202/210 - Loss: 0.1587\n",
      "  Batch 203/210 - Loss: 0.1696\n",
      "  Batch 204/210 - Loss: 0.1287\n",
      "  Batch 205/210 - Loss: 0.1407\n",
      "  Batch 206/210 - Loss: 0.1561\n",
      "  Batch 207/210 - Loss: 0.1601\n",
      "  Batch 208/210 - Loss: 0.1410\n",
      "  Batch 209/210 - Loss: 0.1575\n",
      "  Batch 210/210 - Loss: 0.1475\n",
      "Epoch 8 Completed. Train Loss: 0.1849, Val Loss: 0.0776\n",
      "\n",
      "Epoch 9/15\n",
      "  Batch 1/210 - Loss: 0.1310\n",
      "  Batch 2/210 - Loss: 0.1703\n",
      "  Batch 3/210 - Loss: 0.1342\n",
      "  Batch 4/210 - Loss: 0.1161\n",
      "  Batch 5/210 - Loss: 0.1085\n",
      "  Batch 6/210 - Loss: 0.1166\n",
      "  Batch 7/210 - Loss: 0.1323\n",
      "  Batch 8/210 - Loss: 0.1027\n",
      "  Batch 9/210 - Loss: 0.1473\n",
      "  Batch 10/210 - Loss: 0.1476\n",
      "  Batch 11/210 - Loss: 0.1477\n",
      "  Batch 12/210 - Loss: 0.1335\n",
      "  Batch 13/210 - Loss: 0.1391\n",
      "  Batch 14/210 - Loss: 0.1615\n",
      "  Batch 15/210 - Loss: 0.1318\n",
      "  Batch 16/210 - Loss: 0.1347\n",
      "  Batch 17/210 - Loss: 0.1077\n",
      "  Batch 18/210 - Loss: 0.1676\n",
      "  Batch 19/210 - Loss: 0.1010\n",
      "  Batch 20/210 - Loss: 0.1534\n",
      "  Batch 21/210 - Loss: 0.1242\n",
      "  Batch 22/210 - Loss: 0.1417\n",
      "  Batch 23/210 - Loss: 0.1304\n",
      "  Batch 24/210 - Loss: 0.1180\n",
      "  Batch 25/210 - Loss: 0.1382\n",
      "  Batch 26/210 - Loss: 0.1705\n",
      "  Batch 27/210 - Loss: 0.1147\n",
      "  Batch 28/210 - Loss: 0.1353\n",
      "  Batch 29/210 - Loss: 0.1205\n",
      "  Batch 30/210 - Loss: 0.1188\n",
      "  Batch 31/210 - Loss: 0.1370\n",
      "  Batch 32/210 - Loss: 0.1238\n",
      "  Batch 33/210 - Loss: 0.1271\n",
      "  Batch 34/210 - Loss: 0.1103\n",
      "  Batch 35/210 - Loss: 0.1436\n",
      "  Batch 36/210 - Loss: 0.1439\n",
      "  Batch 37/210 - Loss: 0.1270\n",
      "  Batch 38/210 - Loss: 0.1151\n",
      "  Batch 39/210 - Loss: 0.1275\n",
      "  Batch 40/210 - Loss: 0.1189\n",
      "  Batch 41/210 - Loss: 0.1242\n",
      "  Batch 42/210 - Loss: 0.1291\n",
      "  Batch 43/210 - Loss: 0.1097\n",
      "  Batch 44/210 - Loss: 0.1186\n",
      "  Batch 45/210 - Loss: 0.1157\n",
      "  Batch 46/210 - Loss: 0.1068\n",
      "  Batch 47/210 - Loss: 0.1284\n",
      "  Batch 48/210 - Loss: 0.1185\n",
      "  Batch 49/210 - Loss: 0.1416\n",
      "  Batch 50/210 - Loss: 0.1144\n",
      "  Batch 51/210 - Loss: 0.1592\n",
      "  Batch 52/210 - Loss: 0.1188\n",
      "  Batch 53/210 - Loss: 0.1215\n",
      "  Batch 54/210 - Loss: 0.1289\n",
      "  Batch 55/210 - Loss: 0.1226\n",
      "  Batch 56/210 - Loss: 0.1158\n",
      "  Batch 57/210 - Loss: 0.0883\n",
      "  Batch 58/210 - Loss: 0.1033\n",
      "  Batch 59/210 - Loss: 0.1420\n",
      "  Batch 60/210 - Loss: 0.1374\n",
      "  Batch 61/210 - Loss: 0.1305\n",
      "  Batch 62/210 - Loss: 0.1257\n",
      "  Batch 63/210 - Loss: 0.1402\n",
      "  Batch 64/210 - Loss: 0.1175\n",
      "  Batch 65/210 - Loss: 0.1272\n",
      "  Batch 66/210 - Loss: 0.1289\n",
      "  Batch 67/210 - Loss: 0.1279\n",
      "  Batch 68/210 - Loss: 0.1316\n",
      "  Batch 69/210 - Loss: 0.0948\n",
      "  Batch 70/210 - Loss: 0.1446\n",
      "  Batch 71/210 - Loss: 0.1040\n",
      "  Batch 72/210 - Loss: 0.1287\n",
      "  Batch 73/210 - Loss: 0.1355\n",
      "  Batch 74/210 - Loss: 0.1497\n",
      "  Batch 75/210 - Loss: 0.1468\n",
      "  Batch 76/210 - Loss: 0.1355\n",
      "  Batch 77/210 - Loss: 0.1264\n",
      "  Batch 78/210 - Loss: 0.1435\n",
      "  Batch 79/210 - Loss: 0.1153\n",
      "  Batch 80/210 - Loss: 0.1094\n",
      "  Batch 81/210 - Loss: 0.1028\n",
      "  Batch 82/210 - Loss: 0.1004\n",
      "  Batch 83/210 - Loss: 0.1297\n",
      "  Batch 84/210 - Loss: 0.1228\n",
      "  Batch 85/210 - Loss: 0.1077\n",
      "  Batch 86/210 - Loss: 0.1253\n",
      "  Batch 87/210 - Loss: 0.1130\n",
      "  Batch 88/210 - Loss: 0.1130\n",
      "  Batch 89/210 - Loss: 0.1202\n",
      "  Batch 90/210 - Loss: 0.1102\n",
      "  Batch 91/210 - Loss: 0.1320\n",
      "  Batch 92/210 - Loss: 0.0969\n",
      "  Batch 93/210 - Loss: 0.1144\n",
      "  Batch 94/210 - Loss: 0.1078\n",
      "  Batch 95/210 - Loss: 0.1180\n",
      "  Batch 96/210 - Loss: 0.1207\n",
      "  Batch 97/210 - Loss: 0.1059\n",
      "  Batch 98/210 - Loss: 0.1231\n",
      "  Batch 99/210 - Loss: 0.1264\n",
      "  Batch 100/210 - Loss: 0.1205\n",
      "  Batch 101/210 - Loss: 0.1256\n",
      "  Batch 102/210 - Loss: 0.1254\n",
      "  Batch 103/210 - Loss: 0.1242\n",
      "  Batch 104/210 - Loss: 0.1316\n",
      "  Batch 105/210 - Loss: 0.1001\n",
      "  Batch 106/210 - Loss: 0.0860\n",
      "  Batch 107/210 - Loss: 0.1281\n",
      "  Batch 108/210 - Loss: 0.1258\n",
      "  Batch 109/210 - Loss: 0.1250\n",
      "  Batch 110/210 - Loss: 0.1262\n",
      "  Batch 111/210 - Loss: 0.0904\n",
      "  Batch 112/210 - Loss: 0.1115\n",
      "  Batch 113/210 - Loss: 0.1004\n",
      "  Batch 114/210 - Loss: 0.1013\n",
      "  Batch 115/210 - Loss: 0.1362\n",
      "  Batch 116/210 - Loss: 0.1075\n",
      "  Batch 117/210 - Loss: 0.1006\n",
      "  Batch 118/210 - Loss: 0.1163\n",
      "  Batch 119/210 - Loss: 0.0949\n",
      "  Batch 120/210 - Loss: 0.0862\n",
      "  Batch 121/210 - Loss: 0.1191\n",
      "  Batch 122/210 - Loss: 0.1034\n",
      "  Batch 123/210 - Loss: 0.1305\n",
      "  Batch 124/210 - Loss: 0.0943\n",
      "  Batch 125/210 - Loss: 0.1217\n",
      "  Batch 126/210 - Loss: 0.1344\n",
      "  Batch 127/210 - Loss: 0.1075\n",
      "  Batch 128/210 - Loss: 0.1093\n",
      "  Batch 129/210 - Loss: 0.1012\n",
      "  Batch 130/210 - Loss: 0.1028\n",
      "  Batch 131/210 - Loss: 0.0813\n",
      "  Batch 132/210 - Loss: 0.1025\n",
      "  Batch 133/210 - Loss: 0.0961\n",
      "  Batch 134/210 - Loss: 0.1201\n",
      "  Batch 135/210 - Loss: 0.1029\n",
      "  Batch 136/210 - Loss: 0.1305\n",
      "  Batch 137/210 - Loss: 0.1258\n",
      "  Batch 138/210 - Loss: 0.1136\n",
      "  Batch 139/210 - Loss: 0.1057\n",
      "  Batch 140/210 - Loss: 0.1012\n",
      "  Batch 141/210 - Loss: 0.0991\n",
      "  Batch 142/210 - Loss: 0.0995\n",
      "  Batch 143/210 - Loss: 0.1023\n",
      "  Batch 144/210 - Loss: 0.1319\n",
      "  Batch 145/210 - Loss: 0.0797\n",
      "  Batch 146/210 - Loss: 0.0967\n",
      "  Batch 147/210 - Loss: 0.1151\n",
      "  Batch 148/210 - Loss: 0.0916\n",
      "  Batch 149/210 - Loss: 0.1061\n",
      "  Batch 150/210 - Loss: 0.0949\n",
      "  Batch 151/210 - Loss: 0.0865\n",
      "  Batch 152/210 - Loss: 0.1189\n",
      "  Batch 153/210 - Loss: 0.1013\n",
      "  Batch 154/210 - Loss: 0.1322\n",
      "  Batch 155/210 - Loss: 0.0949\n",
      "  Batch 156/210 - Loss: 0.0879\n",
      "  Batch 157/210 - Loss: 0.0934\n",
      "  Batch 158/210 - Loss: 0.0987\n",
      "  Batch 159/210 - Loss: 0.0975\n",
      "  Batch 160/210 - Loss: 0.0922\n",
      "  Batch 161/210 - Loss: 0.0795\n",
      "  Batch 162/210 - Loss: 0.1011\n",
      "  Batch 163/210 - Loss: 0.1039\n",
      "  Batch 164/210 - Loss: 0.1414\n",
      "  Batch 165/210 - Loss: 0.1113\n",
      "  Batch 166/210 - Loss: 0.0811\n",
      "  Batch 167/210 - Loss: 0.0891\n",
      "  Batch 168/210 - Loss: 0.0827\n",
      "  Batch 169/210 - Loss: 0.0908\n",
      "  Batch 170/210 - Loss: 0.0945\n",
      "  Batch 171/210 - Loss: 0.1100\n",
      "  Batch 172/210 - Loss: 0.0933\n",
      "  Batch 173/210 - Loss: 0.0998\n",
      "  Batch 174/210 - Loss: 0.1071\n",
      "  Batch 175/210 - Loss: 0.1003\n",
      "  Batch 176/210 - Loss: 0.1006\n",
      "  Batch 177/210 - Loss: 0.0971\n",
      "  Batch 178/210 - Loss: 0.0760\n",
      "  Batch 179/210 - Loss: 0.0947\n",
      "  Batch 180/210 - Loss: 0.0855\n",
      "  Batch 181/210 - Loss: 0.1145\n",
      "  Batch 182/210 - Loss: 0.1120\n",
      "  Batch 183/210 - Loss: 0.0965\n",
      "  Batch 184/210 - Loss: 0.0892\n",
      "  Batch 185/210 - Loss: 0.0951\n",
      "  Batch 186/210 - Loss: 0.1149\n",
      "  Batch 187/210 - Loss: 0.0991\n",
      "  Batch 188/210 - Loss: 0.0770\n",
      "  Batch 189/210 - Loss: 0.0972\n",
      "  Batch 190/210 - Loss: 0.1040\n",
      "  Batch 191/210 - Loss: 0.0984\n",
      "  Batch 192/210 - Loss: 0.0816\n",
      "  Batch 193/210 - Loss: 0.0931\n",
      "  Batch 194/210 - Loss: 0.1032\n",
      "  Batch 195/210 - Loss: 0.1198\n",
      "  Batch 196/210 - Loss: 0.0853\n",
      "  Batch 197/210 - Loss: 0.1063\n",
      "  Batch 198/210 - Loss: 0.0975\n",
      "  Batch 199/210 - Loss: 0.0856\n",
      "  Batch 200/210 - Loss: 0.1052\n",
      "  Batch 201/210 - Loss: 0.0900\n",
      "  Batch 202/210 - Loss: 0.0977\n",
      "  Batch 203/210 - Loss: 0.1059\n",
      "  Batch 204/210 - Loss: 0.0752\n",
      "  Batch 205/210 - Loss: 0.1016\n",
      "  Batch 206/210 - Loss: 0.1062\n",
      "  Batch 207/210 - Loss: 0.0892\n",
      "  Batch 208/210 - Loss: 0.0931\n",
      "  Batch 209/210 - Loss: 0.0903\n",
      "  Batch 210/210 - Loss: 0.1020\n",
      "Epoch 9 Completed. Train Loss: 0.1139, Val Loss: 0.0436\n",
      "\n",
      "Epoch 10/15\n",
      "  Batch 1/210 - Loss: 0.0679\n",
      "  Batch 2/210 - Loss: 0.0947\n",
      "  Batch 3/210 - Loss: 0.0737\n",
      "  Batch 4/210 - Loss: 0.1033\n",
      "  Batch 5/210 - Loss: 0.0889\n",
      "  Batch 6/210 - Loss: 0.0914\n",
      "  Batch 7/210 - Loss: 0.0733\n",
      "  Batch 8/210 - Loss: 0.0681\n",
      "  Batch 9/210 - Loss: 0.1012\n",
      "  Batch 10/210 - Loss: 0.0869\n",
      "  Batch 11/210 - Loss: 0.0818\n",
      "  Batch 12/210 - Loss: 0.0775\n",
      "  Batch 13/210 - Loss: 0.0897\n",
      "  Batch 14/210 - Loss: 0.0812\n",
      "  Batch 15/210 - Loss: 0.0775\n",
      "  Batch 16/210 - Loss: 0.0719\n",
      "  Batch 17/210 - Loss: 0.0800\n",
      "  Batch 18/210 - Loss: 0.0621\n",
      "  Batch 19/210 - Loss: 0.1007\n",
      "  Batch 20/210 - Loss: 0.0749\n",
      "  Batch 21/210 - Loss: 0.0757\n",
      "  Batch 22/210 - Loss: 0.0738\n",
      "  Batch 23/210 - Loss: 0.0696\n",
      "  Batch 24/210 - Loss: 0.0688\n",
      "  Batch 25/210 - Loss: 0.0984\n",
      "  Batch 26/210 - Loss: 0.1015\n",
      "  Batch 27/210 - Loss: 0.1088\n",
      "  Batch 28/210 - Loss: 0.0733\n",
      "  Batch 29/210 - Loss: 0.0802\n",
      "  Batch 30/210 - Loss: 0.0734\n",
      "  Batch 31/210 - Loss: 0.0718\n",
      "  Batch 32/210 - Loss: 0.0797\n",
      "  Batch 33/210 - Loss: 0.0782\n",
      "  Batch 34/210 - Loss: 0.0714\n",
      "  Batch 35/210 - Loss: 0.0709\n",
      "  Batch 36/210 - Loss: 0.0950\n",
      "  Batch 37/210 - Loss: 0.0804\n",
      "  Batch 38/210 - Loss: 0.0822\n",
      "  Batch 39/210 - Loss: 0.0601\n",
      "  Batch 40/210 - Loss: 0.0845\n",
      "  Batch 41/210 - Loss: 0.0755\n",
      "  Batch 42/210 - Loss: 0.0873\n",
      "  Batch 43/210 - Loss: 0.0905\n",
      "  Batch 44/210 - Loss: 0.0592\n",
      "  Batch 45/210 - Loss: 0.0781\n",
      "  Batch 46/210 - Loss: 0.0892\n",
      "  Batch 47/210 - Loss: 0.0871\n",
      "  Batch 48/210 - Loss: 0.0801\n",
      "  Batch 49/210 - Loss: 0.0927\n",
      "  Batch 50/210 - Loss: 0.0966\n",
      "  Batch 51/210 - Loss: 0.0621\n",
      "  Batch 52/210 - Loss: 0.0682\n",
      "  Batch 53/210 - Loss: 0.0653\n",
      "  Batch 54/210 - Loss: 0.0954\n",
      "  Batch 55/210 - Loss: 0.0764\n",
      "  Batch 56/210 - Loss: 0.0696\n",
      "  Batch 57/210 - Loss: 0.0829\n",
      "  Batch 58/210 - Loss: 0.0795\n",
      "  Batch 59/210 - Loss: 0.0689\n",
      "  Batch 60/210 - Loss: 0.0800\n",
      "  Batch 61/210 - Loss: 0.0877\n",
      "  Batch 62/210 - Loss: 0.0822\n",
      "  Batch 63/210 - Loss: 0.0841\n",
      "  Batch 64/210 - Loss: 0.0736\n",
      "  Batch 65/210 - Loss: 0.0612\n",
      "  Batch 66/210 - Loss: 0.0781\n",
      "  Batch 67/210 - Loss: 0.0806\n",
      "  Batch 68/210 - Loss: 0.0833\n",
      "  Batch 69/210 - Loss: 0.0810\n",
      "  Batch 70/210 - Loss: 0.0861\n",
      "  Batch 71/210 - Loss: 0.0769\n",
      "  Batch 72/210 - Loss: 0.0832\n",
      "  Batch 73/210 - Loss: 0.0829\n",
      "  Batch 74/210 - Loss: 0.0666\n",
      "  Batch 75/210 - Loss: 0.0812\n",
      "  Batch 76/210 - Loss: 0.0754\n",
      "  Batch 77/210 - Loss: 0.0836\n",
      "  Batch 78/210 - Loss: 0.0679\n",
      "  Batch 79/210 - Loss: 0.0591\n",
      "  Batch 80/210 - Loss: 0.0720\n",
      "  Batch 81/210 - Loss: 0.0870\n",
      "  Batch 82/210 - Loss: 0.0901\n",
      "  Batch 83/210 - Loss: 0.0724\n",
      "  Batch 84/210 - Loss: 0.0870\n",
      "  Batch 85/210 - Loss: 0.0810\n",
      "  Batch 86/210 - Loss: 0.0912\n",
      "  Batch 87/210 - Loss: 0.0631\n",
      "  Batch 88/210 - Loss: 0.0859\n",
      "  Batch 89/210 - Loss: 0.0829\n",
      "  Batch 90/210 - Loss: 0.0575\n",
      "  Batch 91/210 - Loss: 0.0880\n",
      "  Batch 92/210 - Loss: 0.0717\n",
      "  Batch 93/210 - Loss: 0.0744\n",
      "  Batch 94/210 - Loss: 0.1101\n",
      "  Batch 95/210 - Loss: 0.0919\n",
      "  Batch 96/210 - Loss: 0.0744\n",
      "  Batch 97/210 - Loss: 0.0814\n",
      "  Batch 98/210 - Loss: 0.0671\n",
      "  Batch 99/210 - Loss: 0.0765\n",
      "  Batch 100/210 - Loss: 0.0857\n",
      "  Batch 101/210 - Loss: 0.0930\n",
      "  Batch 102/210 - Loss: 0.0751\n",
      "  Batch 103/210 - Loss: 0.0859\n",
      "  Batch 104/210 - Loss: 0.0689\n",
      "  Batch 105/210 - Loss: 0.0657\n",
      "  Batch 106/210 - Loss: 0.0757\n",
      "  Batch 107/210 - Loss: 0.0926\n",
      "  Batch 108/210 - Loss: 0.0989\n",
      "  Batch 109/210 - Loss: 0.0884\n",
      "  Batch 110/210 - Loss: 0.0682\n",
      "  Batch 111/210 - Loss: 0.0678\n",
      "  Batch 112/210 - Loss: 0.0742\n",
      "  Batch 113/210 - Loss: 0.0756\n",
      "  Batch 114/210 - Loss: 0.0909\n",
      "  Batch 115/210 - Loss: 0.0850\n",
      "  Batch 116/210 - Loss: 0.0532\n",
      "  Batch 117/210 - Loss: 0.0705\n",
      "  Batch 118/210 - Loss: 0.0655\n",
      "  Batch 119/210 - Loss: 0.0869\n",
      "  Batch 120/210 - Loss: 0.0646\n",
      "  Batch 121/210 - Loss: 0.0729\n",
      "  Batch 122/210 - Loss: 0.0599\n",
      "  Batch 123/210 - Loss: 0.0656\n",
      "  Batch 124/210 - Loss: 0.0582\n",
      "  Batch 125/210 - Loss: 0.0792\n",
      "  Batch 126/210 - Loss: 0.0760\n",
      "  Batch 127/210 - Loss: 0.0508\n",
      "  Batch 128/210 - Loss: 0.0573\n",
      "  Batch 129/210 - Loss: 0.0936\n",
      "  Batch 130/210 - Loss: 0.0885\n",
      "  Batch 131/210 - Loss: 0.0696\n",
      "  Batch 132/210 - Loss: 0.0808\n",
      "  Batch 133/210 - Loss: 0.0598\n",
      "  Batch 134/210 - Loss: 0.0862\n",
      "  Batch 135/210 - Loss: 0.0640\n",
      "  Batch 136/210 - Loss: 0.0607\n",
      "  Batch 137/210 - Loss: 0.0651\n",
      "  Batch 138/210 - Loss: 0.0838\n",
      "  Batch 139/210 - Loss: 0.0847\n",
      "  Batch 140/210 - Loss: 0.0631\n",
      "  Batch 141/210 - Loss: 0.0819\n",
      "  Batch 142/210 - Loss: 0.0674\n",
      "  Batch 143/210 - Loss: 0.0932\n",
      "  Batch 144/210 - Loss: 0.0604\n",
      "  Batch 145/210 - Loss: 0.0784\n",
      "  Batch 146/210 - Loss: 0.0590\n",
      "  Batch 147/210 - Loss: 0.0510\n",
      "  Batch 148/210 - Loss: 0.0554\n",
      "  Batch 149/210 - Loss: 0.0677\n",
      "  Batch 150/210 - Loss: 0.0661\n",
      "  Batch 151/210 - Loss: 0.0806\n",
      "  Batch 152/210 - Loss: 0.0838\n",
      "  Batch 153/210 - Loss: 0.0791\n",
      "  Batch 154/210 - Loss: 0.0573\n",
      "  Batch 155/210 - Loss: 0.0570\n",
      "  Batch 156/210 - Loss: 0.0647\n",
      "  Batch 157/210 - Loss: 0.0810\n",
      "  Batch 158/210 - Loss: 0.0632\n",
      "  Batch 159/210 - Loss: 0.0872\n",
      "  Batch 160/210 - Loss: 0.0710\n",
      "  Batch 161/210 - Loss: 0.0642\n",
      "  Batch 162/210 - Loss: 0.0859\n",
      "  Batch 163/210 - Loss: 0.0505\n",
      "  Batch 164/210 - Loss: 0.0549\n",
      "  Batch 165/210 - Loss: 0.0552\n",
      "  Batch 166/210 - Loss: 0.0567\n",
      "  Batch 167/210 - Loss: 0.0581\n",
      "  Batch 168/210 - Loss: 0.0566\n",
      "  Batch 169/210 - Loss: 0.0703\n",
      "  Batch 170/210 - Loss: 0.0599\n",
      "  Batch 171/210 - Loss: 0.0702\n",
      "  Batch 172/210 - Loss: 0.0724\n",
      "  Batch 173/210 - Loss: 0.0684\n",
      "  Batch 174/210 - Loss: 0.0682\n",
      "  Batch 175/210 - Loss: 0.0656\n",
      "  Batch 176/210 - Loss: 0.0523\n",
      "  Batch 177/210 - Loss: 0.0762\n",
      "  Batch 178/210 - Loss: 0.0560\n",
      "  Batch 179/210 - Loss: 0.0753\n",
      "  Batch 180/210 - Loss: 0.0659\n",
      "  Batch 181/210 - Loss: 0.0397\n",
      "  Batch 182/210 - Loss: 0.0778\n",
      "  Batch 183/210 - Loss: 0.0934\n",
      "  Batch 184/210 - Loss: 0.0768\n",
      "  Batch 185/210 - Loss: 0.0565\n",
      "  Batch 186/210 - Loss: 0.0754\n",
      "  Batch 187/210 - Loss: 0.0674\n",
      "  Batch 188/210 - Loss: 0.0640\n",
      "  Batch 189/210 - Loss: 0.0719\n",
      "  Batch 190/210 - Loss: 0.0523\n",
      "  Batch 191/210 - Loss: 0.0687\n",
      "  Batch 192/210 - Loss: 0.0535\n",
      "  Batch 193/210 - Loss: 0.0723\n",
      "  Batch 194/210 - Loss: 0.0699\n",
      "  Batch 195/210 - Loss: 0.0662\n",
      "  Batch 196/210 - Loss: 0.0663\n",
      "  Batch 197/210 - Loss: 0.0595\n",
      "  Batch 198/210 - Loss: 0.0648\n",
      "  Batch 199/210 - Loss: 0.0557\n",
      "  Batch 200/210 - Loss: 0.0656\n",
      "  Batch 201/210 - Loss: 0.0621\n",
      "  Batch 202/210 - Loss: 0.0577\n",
      "  Batch 203/210 - Loss: 0.0673\n",
      "  Batch 204/210 - Loss: 0.0735\n",
      "  Batch 205/210 - Loss: 0.0524\n",
      "  Batch 206/210 - Loss: 0.0718\n",
      "  Batch 207/210 - Loss: 0.0576\n",
      "  Batch 208/210 - Loss: 0.0553\n",
      "  Batch 209/210 - Loss: 0.0687\n",
      "  Batch 210/210 - Loss: 0.0608\n",
      "Epoch 10 Completed. Train Loss: 0.0743, Val Loss: 0.0292\n",
      "\n",
      "Epoch 11/15\n",
      "  Batch 1/210 - Loss: 0.0712\n",
      "  Batch 2/210 - Loss: 0.0651\n",
      "  Batch 3/210 - Loss: 0.0593\n",
      "  Batch 4/210 - Loss: 0.0547\n",
      "  Batch 5/210 - Loss: 0.0529\n",
      "  Batch 6/210 - Loss: 0.0518\n",
      "  Batch 7/210 - Loss: 0.0546\n",
      "  Batch 8/210 - Loss: 0.0649\n",
      "  Batch 9/210 - Loss: 0.0602\n",
      "  Batch 10/210 - Loss: 0.0477\n",
      "  Batch 11/210 - Loss: 0.0802\n",
      "  Batch 12/210 - Loss: 0.0594\n",
      "  Batch 13/210 - Loss: 0.0402\n",
      "  Batch 14/210 - Loss: 0.0562\n",
      "  Batch 15/210 - Loss: 0.0568\n",
      "  Batch 16/210 - Loss: 0.0492\n",
      "  Batch 17/210 - Loss: 0.0499\n",
      "  Batch 18/210 - Loss: 0.0623\n",
      "  Batch 19/210 - Loss: 0.0509\n",
      "  Batch 20/210 - Loss: 0.0655\n",
      "  Batch 21/210 - Loss: 0.0471\n",
      "  Batch 22/210 - Loss: 0.0477\n",
      "  Batch 23/210 - Loss: 0.0529\n",
      "  Batch 24/210 - Loss: 0.0513\n",
      "  Batch 25/210 - Loss: 0.0512\n",
      "  Batch 26/210 - Loss: 0.0495\n",
      "  Batch 27/210 - Loss: 0.0601\n",
      "  Batch 28/210 - Loss: 0.0572\n",
      "  Batch 29/210 - Loss: 0.0617\n",
      "  Batch 30/210 - Loss: 0.0803\n",
      "  Batch 31/210 - Loss: 0.0420\n",
      "  Batch 32/210 - Loss: 0.0551\n",
      "  Batch 33/210 - Loss: 0.0757\n",
      "  Batch 34/210 - Loss: 0.0634\n",
      "  Batch 35/210 - Loss: 0.0498\n",
      "  Batch 36/210 - Loss: 0.0602\n",
      "  Batch 37/210 - Loss: 0.0630\n",
      "  Batch 38/210 - Loss: 0.0428\n",
      "  Batch 39/210 - Loss: 0.0462\n",
      "  Batch 40/210 - Loss: 0.0505\n",
      "  Batch 41/210 - Loss: 0.0461\n",
      "  Batch 42/210 - Loss: 0.0556\n",
      "  Batch 43/210 - Loss: 0.0482\n",
      "  Batch 44/210 - Loss: 0.0591\n",
      "  Batch 45/210 - Loss: 0.0458\n",
      "  Batch 46/210 - Loss: 0.0512\n",
      "  Batch 47/210 - Loss: 0.0633\n",
      "  Batch 48/210 - Loss: 0.0699\n",
      "  Batch 49/210 - Loss: 0.0564\n",
      "  Batch 50/210 - Loss: 0.0650\n",
      "  Batch 51/210 - Loss: 0.0514\n",
      "  Batch 52/210 - Loss: 0.0523\n",
      "  Batch 53/210 - Loss: 0.0559\n",
      "  Batch 54/210 - Loss: 0.0428\n",
      "  Batch 55/210 - Loss: 0.0501\n",
      "  Batch 56/210 - Loss: 0.0702\n",
      "  Batch 57/210 - Loss: 0.0702\n",
      "  Batch 58/210 - Loss: 0.0441\n",
      "  Batch 59/210 - Loss: 0.0415\n",
      "  Batch 60/210 - Loss: 0.0635\n",
      "  Batch 61/210 - Loss: 0.0519\n",
      "  Batch 62/210 - Loss: 0.0587\n",
      "  Batch 63/210 - Loss: 0.0571\n",
      "  Batch 64/210 - Loss: 0.0485\n",
      "  Batch 65/210 - Loss: 0.0428\n",
      "  Batch 66/210 - Loss: 0.0566\n",
      "  Batch 67/210 - Loss: 0.0549\n",
      "  Batch 68/210 - Loss: 0.0434\n",
      "  Batch 69/210 - Loss: 0.0655\n",
      "  Batch 70/210 - Loss: 0.0601\n",
      "  Batch 71/210 - Loss: 0.0577\n",
      "  Batch 72/210 - Loss: 0.0627\n",
      "  Batch 73/210 - Loss: 0.0698\n",
      "  Batch 74/210 - Loss: 0.0689\n",
      "  Batch 75/210 - Loss: 0.0454\n",
      "  Batch 76/210 - Loss: 0.0529\n",
      "  Batch 77/210 - Loss: 0.0481\n",
      "  Batch 78/210 - Loss: 0.0472\n",
      "  Batch 79/210 - Loss: 0.0587\n",
      "  Batch 80/210 - Loss: 0.0430\n",
      "  Batch 81/210 - Loss: 0.0584\n",
      "  Batch 82/210 - Loss: 0.0558\n",
      "  Batch 83/210 - Loss: 0.0629\n",
      "  Batch 84/210 - Loss: 0.0608\n",
      "  Batch 85/210 - Loss: 0.0581\n",
      "  Batch 86/210 - Loss: 0.0620\n",
      "  Batch 87/210 - Loss: 0.0465\n",
      "  Batch 88/210 - Loss: 0.0628\n",
      "  Batch 89/210 - Loss: 0.0647\n",
      "  Batch 90/210 - Loss: 0.0423\n",
      "  Batch 91/210 - Loss: 0.0453\n",
      "  Batch 92/210 - Loss: 0.0391\n",
      "  Batch 93/210 - Loss: 0.0769\n",
      "  Batch 94/210 - Loss: 0.0515\n",
      "  Batch 95/210 - Loss: 0.0616\n",
      "  Batch 96/210 - Loss: 0.0520\n",
      "  Batch 97/210 - Loss: 0.0443\n",
      "  Batch 98/210 - Loss: 0.0547\n",
      "  Batch 99/210 - Loss: 0.0684\n",
      "  Batch 100/210 - Loss: 0.0534\n",
      "  Batch 101/210 - Loss: 0.0436\n",
      "  Batch 102/210 - Loss: 0.0411\n",
      "  Batch 103/210 - Loss: 0.0391\n",
      "  Batch 104/210 - Loss: 0.0508\n",
      "  Batch 105/210 - Loss: 0.0604\n",
      "  Batch 106/210 - Loss: 0.0590\n",
      "  Batch 107/210 - Loss: 0.0451\n",
      "  Batch 108/210 - Loss: 0.0622\n",
      "  Batch 109/210 - Loss: 0.0524\n",
      "  Batch 110/210 - Loss: 0.0396\n",
      "  Batch 111/210 - Loss: 0.0551\n",
      "  Batch 112/210 - Loss: 0.0560\n",
      "  Batch 113/210 - Loss: 0.0402\n",
      "  Batch 114/210 - Loss: 0.0528\n",
      "  Batch 115/210 - Loss: 0.0578\n",
      "  Batch 116/210 - Loss: 0.0552\n",
      "  Batch 117/210 - Loss: 0.0507\n",
      "  Batch 118/210 - Loss: 0.0570\n",
      "  Batch 119/210 - Loss: 0.0484\n",
      "  Batch 120/210 - Loss: 0.0413\n",
      "  Batch 121/210 - Loss: 0.0533\n",
      "  Batch 122/210 - Loss: 0.0424\n",
      "  Batch 123/210 - Loss: 0.0514\n",
      "  Batch 124/210 - Loss: 0.0502\n",
      "  Batch 125/210 - Loss: 0.0480\n",
      "  Batch 126/210 - Loss: 0.0584\n",
      "  Batch 127/210 - Loss: 0.0632\n",
      "  Batch 128/210 - Loss: 0.0642\n",
      "  Batch 129/210 - Loss: 0.0492\n",
      "  Batch 130/210 - Loss: 0.0529\n",
      "  Batch 131/210 - Loss: 0.0570\n",
      "  Batch 132/210 - Loss: 0.0413\n",
      "  Batch 133/210 - Loss: 0.0618\n",
      "  Batch 134/210 - Loss: 0.0408\n",
      "  Batch 135/210 - Loss: 0.0474\n",
      "  Batch 136/210 - Loss: 0.0438\n",
      "  Batch 137/210 - Loss: 0.0441\n",
      "  Batch 138/210 - Loss: 0.0621\n",
      "  Batch 139/210 - Loss: 0.0510\n",
      "  Batch 140/210 - Loss: 0.0370\n",
      "  Batch 141/210 - Loss: 0.0662\n",
      "  Batch 142/210 - Loss: 0.0589\n",
      "  Batch 143/210 - Loss: 0.0517\n",
      "  Batch 144/210 - Loss: 0.0448\n",
      "  Batch 145/210 - Loss: 0.0532\n",
      "  Batch 146/210 - Loss: 0.0498\n",
      "  Batch 147/210 - Loss: 0.0435\n",
      "  Batch 148/210 - Loss: 0.0568\n",
      "  Batch 149/210 - Loss: 0.0532\n",
      "  Batch 150/210 - Loss: 0.0529\n",
      "  Batch 151/210 - Loss: 0.0546\n",
      "  Batch 152/210 - Loss: 0.0360\n",
      "  Batch 153/210 - Loss: 0.0446\n",
      "  Batch 154/210 - Loss: 0.0574\n",
      "  Batch 155/210 - Loss: 0.0412\n",
      "  Batch 156/210 - Loss: 0.0555\n",
      "  Batch 157/210 - Loss: 0.0420\n",
      "  Batch 158/210 - Loss: 0.0500\n",
      "  Batch 159/210 - Loss: 0.0464\n",
      "  Batch 160/210 - Loss: 0.0399\n",
      "  Batch 161/210 - Loss: 0.0472\n",
      "  Batch 162/210 - Loss: 0.0640\n",
      "  Batch 163/210 - Loss: 0.0550\n",
      "  Batch 164/210 - Loss: 0.0551\n",
      "  Batch 165/210 - Loss: 0.0452\n",
      "  Batch 166/210 - Loss: 0.0484\n",
      "  Batch 167/210 - Loss: 0.0420\n",
      "  Batch 168/210 - Loss: 0.0495\n",
      "  Batch 169/210 - Loss: 0.0574\n",
      "  Batch 170/210 - Loss: 0.0438\n",
      "  Batch 171/210 - Loss: 0.0597\n",
      "  Batch 172/210 - Loss: 0.0497\n",
      "  Batch 173/210 - Loss: 0.0493\n",
      "  Batch 174/210 - Loss: 0.0498\n",
      "  Batch 175/210 - Loss: 0.0475\n",
      "  Batch 176/210 - Loss: 0.0656\n",
      "  Batch 177/210 - Loss: 0.0479\n",
      "  Batch 178/210 - Loss: 0.0543\n",
      "  Batch 179/210 - Loss: 0.0528\n",
      "  Batch 180/210 - Loss: 0.0402\n",
      "  Batch 181/210 - Loss: 0.0669\n",
      "  Batch 182/210 - Loss: 0.0552\n",
      "  Batch 183/210 - Loss: 0.0394\n",
      "  Batch 184/210 - Loss: 0.0412\n",
      "  Batch 185/210 - Loss: 0.0572\n",
      "  Batch 186/210 - Loss: 0.0465\n",
      "  Batch 187/210 - Loss: 0.0472\n",
      "  Batch 188/210 - Loss: 0.0527\n",
      "  Batch 189/210 - Loss: 0.0433\n",
      "  Batch 190/210 - Loss: 0.0397\n",
      "  Batch 191/210 - Loss: 0.0431\n",
      "  Batch 192/210 - Loss: 0.0336\n",
      "  Batch 193/210 - Loss: 0.0366\n",
      "  Batch 194/210 - Loss: 0.0508\n",
      "  Batch 195/210 - Loss: 0.0352\n",
      "  Batch 196/210 - Loss: 0.0456\n",
      "  Batch 197/210 - Loss: 0.0438\n",
      "  Batch 198/210 - Loss: 0.0590\n",
      "  Batch 199/210 - Loss: 0.0333\n",
      "  Batch 200/210 - Loss: 0.0603\n",
      "  Batch 201/210 - Loss: 0.0447\n",
      "  Batch 202/210 - Loss: 0.0403\n",
      "  Batch 203/210 - Loss: 0.0572\n",
      "  Batch 204/210 - Loss: 0.0622\n",
      "  Batch 205/210 - Loss: 0.0298\n",
      "  Batch 206/210 - Loss: 0.0535\n",
      "  Batch 207/210 - Loss: 0.0426\n",
      "  Batch 208/210 - Loss: 0.0342\n",
      "  Batch 209/210 - Loss: 0.0397\n",
      "  Batch 210/210 - Loss: 0.0380\n",
      "Epoch 11 Completed. Train Loss: 0.0524, Val Loss: 0.0195\n",
      "\n",
      "Epoch 12/15\n",
      "  Batch 1/210 - Loss: 0.0350\n",
      "  Batch 2/210 - Loss: 0.0488\n",
      "  Batch 3/210 - Loss: 0.0458\n",
      "  Batch 4/210 - Loss: 0.0424\n",
      "  Batch 5/210 - Loss: 0.0629\n",
      "  Batch 6/210 - Loss: 0.0463\n",
      "  Batch 7/210 - Loss: 0.0374\n",
      "  Batch 8/210 - Loss: 0.0479\n",
      "  Batch 9/210 - Loss: 0.0426\n",
      "  Batch 10/210 - Loss: 0.0503\n",
      "  Batch 11/210 - Loss: 0.0280\n",
      "  Batch 12/210 - Loss: 0.0468\n",
      "  Batch 13/210 - Loss: 0.0432\n",
      "  Batch 14/210 - Loss: 0.0447\n",
      "  Batch 15/210 - Loss: 0.0418\n",
      "  Batch 16/210 - Loss: 0.0447\n",
      "  Batch 17/210 - Loss: 0.0403\n",
      "  Batch 18/210 - Loss: 0.0409\n",
      "  Batch 19/210 - Loss: 0.0378\n",
      "  Batch 20/210 - Loss: 0.0401\n",
      "  Batch 21/210 - Loss: 0.0498\n",
      "  Batch 22/210 - Loss: 0.0416\n",
      "  Batch 23/210 - Loss: 0.0502\n",
      "  Batch 24/210 - Loss: 0.0423\n",
      "  Batch 25/210 - Loss: 0.0369\n",
      "  Batch 26/210 - Loss: 0.0435\n",
      "  Batch 27/210 - Loss: 0.0475\n",
      "  Batch 28/210 - Loss: 0.0330\n",
      "  Batch 29/210 - Loss: 0.0377\n",
      "  Batch 30/210 - Loss: 0.0356\n",
      "  Batch 31/210 - Loss: 0.0393\n",
      "  Batch 32/210 - Loss: 0.0446\n",
      "  Batch 33/210 - Loss: 0.0408\n",
      "  Batch 34/210 - Loss: 0.0469\n",
      "  Batch 35/210 - Loss: 0.0521\n",
      "  Batch 36/210 - Loss: 0.0374\n",
      "  Batch 37/210 - Loss: 0.0444\n",
      "  Batch 38/210 - Loss: 0.0562\n",
      "  Batch 39/210 - Loss: 0.0431\n",
      "  Batch 40/210 - Loss: 0.0398\n",
      "  Batch 41/210 - Loss: 0.0517\n",
      "  Batch 42/210 - Loss: 0.0522\n",
      "  Batch 43/210 - Loss: 0.0462\n",
      "  Batch 44/210 - Loss: 0.0411\n",
      "  Batch 45/210 - Loss: 0.0394\n",
      "  Batch 46/210 - Loss: 0.0446\n",
      "  Batch 47/210 - Loss: 0.0488\n",
      "  Batch 48/210 - Loss: 0.0404\n",
      "  Batch 49/210 - Loss: 0.0417\n",
      "  Batch 50/210 - Loss: 0.0485\n",
      "  Batch 51/210 - Loss: 0.0438\n",
      "  Batch 52/210 - Loss: 0.0456\n",
      "  Batch 53/210 - Loss: 0.0300\n",
      "  Batch 54/210 - Loss: 0.0333\n",
      "  Batch 55/210 - Loss: 0.0400\n",
      "  Batch 56/210 - Loss: 0.0327\n",
      "  Batch 57/210 - Loss: 0.0370\n",
      "  Batch 58/210 - Loss: 0.0407\n",
      "  Batch 59/210 - Loss: 0.0423\n",
      "  Batch 60/210 - Loss: 0.0391\n",
      "  Batch 61/210 - Loss: 0.0452\n",
      "  Batch 62/210 - Loss: 0.0481\n",
      "  Batch 63/210 - Loss: 0.0400\n",
      "  Batch 64/210 - Loss: 0.0454\n",
      "  Batch 65/210 - Loss: 0.0390\n",
      "  Batch 66/210 - Loss: 0.0322\n",
      "  Batch 67/210 - Loss: 0.0316\n",
      "  Batch 68/210 - Loss: 0.0422\n",
      "  Batch 69/210 - Loss: 0.0369\n",
      "  Batch 70/210 - Loss: 0.0419\n",
      "  Batch 71/210 - Loss: 0.0384\n",
      "  Batch 72/210 - Loss: 0.0414\n",
      "  Batch 73/210 - Loss: 0.0448\n",
      "  Batch 74/210 - Loss: 0.0361\n",
      "  Batch 75/210 - Loss: 0.0349\n",
      "  Batch 76/210 - Loss: 0.0433\n",
      "  Batch 77/210 - Loss: 0.0424\n",
      "  Batch 78/210 - Loss: 0.0418\n",
      "  Batch 79/210 - Loss: 0.0270\n",
      "  Batch 80/210 - Loss: 0.0409\n",
      "  Batch 81/210 - Loss: 0.0380\n",
      "  Batch 82/210 - Loss: 0.0502\n",
      "  Batch 83/210 - Loss: 0.0339\n",
      "  Batch 84/210 - Loss: 0.0350\n",
      "  Batch 85/210 - Loss: 0.0368\n",
      "  Batch 86/210 - Loss: 0.0474\n",
      "  Batch 87/210 - Loss: 0.0484\n",
      "  Batch 88/210 - Loss: 0.0395\n",
      "  Batch 89/210 - Loss: 0.0321\n",
      "  Batch 90/210 - Loss: 0.0383\n",
      "  Batch 91/210 - Loss: 0.0297\n",
      "  Batch 92/210 - Loss: 0.0336\n",
      "  Batch 93/210 - Loss: 0.0226\n",
      "  Batch 94/210 - Loss: 0.0370\n",
      "  Batch 95/210 - Loss: 0.0333\n",
      "  Batch 96/210 - Loss: 0.0370\n",
      "  Batch 97/210 - Loss: 0.0372\n",
      "  Batch 98/210 - Loss: 0.0447\n",
      "  Batch 99/210 - Loss: 0.0386\n",
      "  Batch 100/210 - Loss: 0.0383\n",
      "  Batch 101/210 - Loss: 0.0417\n",
      "  Batch 102/210 - Loss: 0.0302\n",
      "  Batch 103/210 - Loss: 0.0297\n",
      "  Batch 104/210 - Loss: 0.0404\n",
      "  Batch 105/210 - Loss: 0.0516\n",
      "  Batch 106/210 - Loss: 0.0476\n",
      "  Batch 107/210 - Loss: 0.0440\n",
      "  Batch 108/210 - Loss: 0.0377\n",
      "  Batch 109/210 - Loss: 0.0307\n",
      "  Batch 110/210 - Loss: 0.0330\n",
      "  Batch 111/210 - Loss: 0.0417\n",
      "  Batch 112/210 - Loss: 0.0352\n",
      "  Batch 113/210 - Loss: 0.0377\n",
      "  Batch 114/210 - Loss: 0.0448\n",
      "  Batch 115/210 - Loss: 0.0327\n",
      "  Batch 116/210 - Loss: 0.0327\n",
      "  Batch 117/210 - Loss: 0.0391\n",
      "  Batch 118/210 - Loss: 0.0278\n",
      "  Batch 119/210 - Loss: 0.0382\n",
      "  Batch 120/210 - Loss: 0.0248\n",
      "  Batch 121/210 - Loss: 0.0461\n",
      "  Batch 122/210 - Loss: 0.0415\n",
      "  Batch 123/210 - Loss: 0.0284\n",
      "  Batch 124/210 - Loss: 0.0334\n",
      "  Batch 125/210 - Loss: 0.0358\n",
      "  Batch 126/210 - Loss: 0.0478\n",
      "  Batch 127/210 - Loss: 0.0401\n",
      "  Batch 128/210 - Loss: 0.0403\n",
      "  Batch 129/210 - Loss: 0.0402\n",
      "  Batch 130/210 - Loss: 0.0353\n",
      "  Batch 131/210 - Loss: 0.0412\n",
      "  Batch 132/210 - Loss: 0.0425\n",
      "  Batch 133/210 - Loss: 0.0319\n",
      "  Batch 134/210 - Loss: 0.0322\n",
      "  Batch 135/210 - Loss: 0.0268\n",
      "  Batch 136/210 - Loss: 0.0327\n",
      "  Batch 137/210 - Loss: 0.0310\n",
      "  Batch 138/210 - Loss: 0.0426\n",
      "  Batch 139/210 - Loss: 0.0287\n",
      "  Batch 140/210 - Loss: 0.0339\n",
      "  Batch 141/210 - Loss: 0.0383\n",
      "  Batch 142/210 - Loss: 0.0396\n",
      "  Batch 143/210 - Loss: 0.0501\n",
      "  Batch 144/210 - Loss: 0.0326\n",
      "  Batch 145/210 - Loss: 0.0328\n",
      "  Batch 146/210 - Loss: 0.0304\n",
      "  Batch 147/210 - Loss: 0.0451\n",
      "  Batch 148/210 - Loss: 0.0258\n",
      "  Batch 149/210 - Loss: 0.0269\n",
      "  Batch 150/210 - Loss: 0.0280\n",
      "  Batch 151/210 - Loss: 0.0450\n",
      "  Batch 152/210 - Loss: 0.0499\n",
      "  Batch 153/210 - Loss: 0.0333\n",
      "  Batch 154/210 - Loss: 0.0286\n",
      "  Batch 155/210 - Loss: 0.0307\n",
      "  Batch 156/210 - Loss: 0.0403\n",
      "  Batch 157/210 - Loss: 0.0355\n",
      "  Batch 158/210 - Loss: 0.0365\n",
      "  Batch 159/210 - Loss: 0.0649\n",
      "  Batch 160/210 - Loss: 0.0314\n",
      "  Batch 161/210 - Loss: 0.0499\n",
      "  Batch 162/210 - Loss: 0.0279\n",
      "  Batch 163/210 - Loss: 0.0411\n",
      "  Batch 164/210 - Loss: 0.0281\n",
      "  Batch 165/210 - Loss: 0.0519\n",
      "  Batch 166/210 - Loss: 0.0383\n",
      "  Batch 167/210 - Loss: 0.0465\n",
      "  Batch 168/210 - Loss: 0.0419\n",
      "  Batch 169/210 - Loss: 0.0447\n",
      "  Batch 170/210 - Loss: 0.0361\n",
      "  Batch 171/210 - Loss: 0.0396\n",
      "  Batch 172/210 - Loss: 0.0355\n",
      "  Batch 173/210 - Loss: 0.0268\n",
      "  Batch 174/210 - Loss: 0.0440\n",
      "  Batch 175/210 - Loss: 0.0330\n",
      "  Batch 176/210 - Loss: 0.0349\n",
      "  Batch 177/210 - Loss: 0.0408\n",
      "  Batch 178/210 - Loss: 0.0302\n",
      "  Batch 179/210 - Loss: 0.0271\n",
      "  Batch 180/210 - Loss: 0.0347\n",
      "  Batch 181/210 - Loss: 0.0282\n",
      "  Batch 182/210 - Loss: 0.0363\n",
      "  Batch 183/210 - Loss: 0.0325\n",
      "  Batch 184/210 - Loss: 0.0378\n",
      "  Batch 185/210 - Loss: 0.0379\n",
      "  Batch 186/210 - Loss: 0.0253\n",
      "  Batch 187/210 - Loss: 0.0280\n",
      "  Batch 188/210 - Loss: 0.0427\n",
      "  Batch 189/210 - Loss: 0.0450\n",
      "  Batch 190/210 - Loss: 0.0384\n",
      "  Batch 191/210 - Loss: 0.0404\n",
      "  Batch 192/210 - Loss: 0.0530\n",
      "  Batch 193/210 - Loss: 0.0297\n",
      "  Batch 194/210 - Loss: 0.0338\n",
      "  Batch 195/210 - Loss: 0.0416\n",
      "  Batch 196/210 - Loss: 0.0389\n",
      "  Batch 197/210 - Loss: 0.0327\n",
      "  Batch 198/210 - Loss: 0.0294\n",
      "  Batch 199/210 - Loss: 0.0323\n",
      "  Batch 200/210 - Loss: 0.0398\n",
      "  Batch 201/210 - Loss: 0.0247\n",
      "  Batch 202/210 - Loss: 0.0486\n",
      "  Batch 203/210 - Loss: 0.0321\n",
      "  Batch 204/210 - Loss: 0.0269\n",
      "  Batch 205/210 - Loss: 0.0244\n",
      "  Batch 206/210 - Loss: 0.0382\n",
      "  Batch 207/210 - Loss: 0.0389\n",
      "  Batch 208/210 - Loss: 0.0345\n",
      "  Batch 209/210 - Loss: 0.0356\n",
      "  Batch 210/210 - Loss: 0.0373\n",
      "Epoch 12 Completed. Train Loss: 0.0388, Val Loss: 0.0176\n",
      "\n",
      "Epoch 13/15\n",
      "  Batch 1/210 - Loss: 0.0389\n",
      "  Batch 2/210 - Loss: 0.0361\n",
      "  Batch 3/210 - Loss: 0.0267\n",
      "  Batch 4/210 - Loss: 0.0352\n",
      "  Batch 5/210 - Loss: 0.0285\n",
      "  Batch 6/210 - Loss: 0.0252\n",
      "  Batch 7/210 - Loss: 0.0308\n",
      "  Batch 8/210 - Loss: 0.0331\n",
      "  Batch 9/210 - Loss: 0.0359\n",
      "  Batch 10/210 - Loss: 0.0262\n",
      "  Batch 11/210 - Loss: 0.0410\n",
      "  Batch 12/210 - Loss: 0.0349\n",
      "  Batch 13/210 - Loss: 0.0272\n",
      "  Batch 14/210 - Loss: 0.0339\n",
      "  Batch 15/210 - Loss: 0.0319\n",
      "  Batch 16/210 - Loss: 0.0242\n",
      "  Batch 17/210 - Loss: 0.0288\n",
      "  Batch 18/210 - Loss: 0.0256\n",
      "  Batch 19/210 - Loss: 0.0296\n",
      "  Batch 20/210 - Loss: 0.0273\n",
      "  Batch 21/210 - Loss: 0.0280\n",
      "  Batch 22/210 - Loss: 0.0316\n",
      "  Batch 23/210 - Loss: 0.0242\n",
      "  Batch 24/210 - Loss: 0.0307\n",
      "  Batch 25/210 - Loss: 0.0417\n",
      "  Batch 26/210 - Loss: 0.0333\n",
      "  Batch 27/210 - Loss: 0.0307\n",
      "  Batch 28/210 - Loss: 0.0240\n",
      "  Batch 29/210 - Loss: 0.0332\n",
      "  Batch 30/210 - Loss: 0.0390\n",
      "  Batch 31/210 - Loss: 0.0353\n",
      "  Batch 32/210 - Loss: 0.0362\n",
      "  Batch 33/210 - Loss: 0.0229\n",
      "  Batch 34/210 - Loss: 0.0277\n",
      "  Batch 35/210 - Loss: 0.0325\n",
      "  Batch 36/210 - Loss: 0.0417\n",
      "  Batch 37/210 - Loss: 0.0269\n",
      "  Batch 38/210 - Loss: 0.0264\n",
      "  Batch 39/210 - Loss: 0.0416\n",
      "  Batch 40/210 - Loss: 0.0313\n",
      "  Batch 41/210 - Loss: 0.0359\n",
      "  Batch 42/210 - Loss: 0.0256\n",
      "  Batch 43/210 - Loss: 0.0280\n",
      "  Batch 44/210 - Loss: 0.0366\n",
      "  Batch 45/210 - Loss: 0.0358\n",
      "  Batch 46/210 - Loss: 0.0283\n",
      "  Batch 47/210 - Loss: 0.0321\n",
      "  Batch 48/210 - Loss: 0.0275\n",
      "  Batch 49/210 - Loss: 0.0328\n",
      "  Batch 50/210 - Loss: 0.0371\n",
      "  Batch 51/210 - Loss: 0.0316\n",
      "  Batch 52/210 - Loss: 0.0422\n",
      "  Batch 53/210 - Loss: 0.0434\n",
      "  Batch 54/210 - Loss: 0.0340\n",
      "  Batch 55/210 - Loss: 0.0303\n",
      "  Batch 56/210 - Loss: 0.0339\n",
      "  Batch 57/210 - Loss: 0.0359\n",
      "  Batch 58/210 - Loss: 0.0422\n",
      "  Batch 59/210 - Loss: 0.0218\n",
      "  Batch 60/210 - Loss: 0.0367\n",
      "  Batch 61/210 - Loss: 0.0304\n",
      "  Batch 62/210 - Loss: 0.0404\n",
      "  Batch 63/210 - Loss: 0.0280\n",
      "  Batch 64/210 - Loss: 0.0329\n",
      "  Batch 65/210 - Loss: 0.0470\n",
      "  Batch 66/210 - Loss: 0.0350\n",
      "  Batch 67/210 - Loss: 0.0344\n",
      "  Batch 68/210 - Loss: 0.0219\n",
      "  Batch 69/210 - Loss: 0.0315\n",
      "  Batch 70/210 - Loss: 0.0289\n",
      "  Batch 71/210 - Loss: 0.0415\n",
      "  Batch 72/210 - Loss: 0.0328\n",
      "  Batch 73/210 - Loss: 0.0307\n",
      "  Batch 74/210 - Loss: 0.0261\n",
      "  Batch 75/210 - Loss: 0.0353\n",
      "  Batch 76/210 - Loss: 0.0347\n",
      "  Batch 77/210 - Loss: 0.0405\n",
      "  Batch 78/210 - Loss: 0.0256\n",
      "  Batch 79/210 - Loss: 0.0277\n",
      "  Batch 80/210 - Loss: 0.0404\n",
      "  Batch 81/210 - Loss: 0.0291\n",
      "  Batch 82/210 - Loss: 0.0399\n",
      "  Batch 83/210 - Loss: 0.0298\n",
      "  Batch 84/210 - Loss: 0.0279\n",
      "  Batch 85/210 - Loss: 0.0246\n",
      "  Batch 86/210 - Loss: 0.0261\n",
      "  Batch 87/210 - Loss: 0.0332\n",
      "  Batch 88/210 - Loss: 0.0285\n",
      "  Batch 89/210 - Loss: 0.0303\n",
      "  Batch 90/210 - Loss: 0.0312\n",
      "  Batch 91/210 - Loss: 0.0369\n",
      "  Batch 92/210 - Loss: 0.0324\n",
      "  Batch 93/210 - Loss: 0.0445\n",
      "  Batch 94/210 - Loss: 0.0273\n",
      "  Batch 95/210 - Loss: 0.0248\n",
      "  Batch 96/210 - Loss: 0.0223\n",
      "  Batch 97/210 - Loss: 0.0411\n",
      "  Batch 98/210 - Loss: 0.0225\n",
      "  Batch 99/210 - Loss: 0.0257\n",
      "  Batch 100/210 - Loss: 0.0279\n",
      "  Batch 101/210 - Loss: 0.0313\n",
      "  Batch 102/210 - Loss: 0.0313\n",
      "  Batch 103/210 - Loss: 0.0235\n",
      "  Batch 104/210 - Loss: 0.0301\n",
      "  Batch 105/210 - Loss: 0.0245\n",
      "  Batch 106/210 - Loss: 0.0305\n",
      "  Batch 107/210 - Loss: 0.0351\n",
      "  Batch 108/210 - Loss: 0.0356\n",
      "  Batch 109/210 - Loss: 0.0194\n",
      "  Batch 110/210 - Loss: 0.0333\n",
      "  Batch 111/210 - Loss: 0.0238\n",
      "  Batch 112/210 - Loss: 0.0342\n",
      "  Batch 113/210 - Loss: 0.0237\n",
      "  Batch 114/210 - Loss: 0.0232\n",
      "  Batch 115/210 - Loss: 0.0323\n",
      "  Batch 116/210 - Loss: 0.0256\n",
      "  Batch 117/210 - Loss: 0.0309\n",
      "  Batch 118/210 - Loss: 0.0286\n",
      "  Batch 119/210 - Loss: 0.0353\n",
      "  Batch 120/210 - Loss: 0.0260\n",
      "  Batch 121/210 - Loss: 0.0302\n",
      "  Batch 122/210 - Loss: 0.0295\n",
      "  Batch 123/210 - Loss: 0.0311\n",
      "  Batch 124/210 - Loss: 0.0330\n",
      "  Batch 125/210 - Loss: 0.0244\n",
      "  Batch 126/210 - Loss: 0.0370\n",
      "  Batch 127/210 - Loss: 0.0316\n",
      "  Batch 128/210 - Loss: 0.0416\n",
      "  Batch 129/210 - Loss: 0.0339\n",
      "  Batch 130/210 - Loss: 0.0289\n",
      "  Batch 131/210 - Loss: 0.0385\n",
      "  Batch 132/210 - Loss: 0.0204\n",
      "  Batch 133/210 - Loss: 0.0343\n",
      "  Batch 134/210 - Loss: 0.0222\n",
      "  Batch 135/210 - Loss: 0.0300\n",
      "  Batch 136/210 - Loss: 0.0239\n",
      "  Batch 137/210 - Loss: 0.0404\n",
      "  Batch 138/210 - Loss: 0.0225\n",
      "  Batch 139/210 - Loss: 0.0191\n",
      "  Batch 140/210 - Loss: 0.0310\n",
      "  Batch 141/210 - Loss: 0.0441\n",
      "  Batch 142/210 - Loss: 0.0382\n",
      "  Batch 143/210 - Loss: 0.0197\n",
      "  Batch 144/210 - Loss: 0.0294\n",
      "  Batch 145/210 - Loss: 0.0386\n",
      "  Batch 146/210 - Loss: 0.0212\n",
      "  Batch 147/210 - Loss: 0.0329\n",
      "  Batch 148/210 - Loss: 0.0327\n",
      "  Batch 149/210 - Loss: 0.0197\n",
      "  Batch 150/210 - Loss: 0.0254\n",
      "  Batch 151/210 - Loss: 0.0316\n",
      "  Batch 152/210 - Loss: 0.0260\n",
      "  Batch 153/210 - Loss: 0.0404\n",
      "  Batch 154/210 - Loss: 0.0310\n",
      "  Batch 155/210 - Loss: 0.0223\n",
      "  Batch 156/210 - Loss: 0.0327\n",
      "  Batch 157/210 - Loss: 0.0409\n",
      "  Batch 158/210 - Loss: 0.0233\n",
      "  Batch 159/210 - Loss: 0.0343\n",
      "  Batch 160/210 - Loss: 0.0352\n",
      "  Batch 161/210 - Loss: 0.0347\n",
      "  Batch 162/210 - Loss: 0.0228\n",
      "  Batch 163/210 - Loss: 0.0262\n",
      "  Batch 164/210 - Loss: 0.0187\n",
      "  Batch 165/210 - Loss: 0.0322\n",
      "  Batch 166/210 - Loss: 0.0237\n",
      "  Batch 167/210 - Loss: 0.0253\n",
      "  Batch 168/210 - Loss: 0.0307\n",
      "  Batch 169/210 - Loss: 0.0250\n",
      "  Batch 170/210 - Loss: 0.0253\n",
      "  Batch 171/210 - Loss: 0.0408\n",
      "  Batch 172/210 - Loss: 0.0365\n",
      "  Batch 173/210 - Loss: 0.0302\n",
      "  Batch 174/210 - Loss: 0.0383\n",
      "  Batch 175/210 - Loss: 0.0410\n",
      "  Batch 176/210 - Loss: 0.0247\n",
      "  Batch 177/210 - Loss: 0.0326\n",
      "  Batch 178/210 - Loss: 0.0369\n",
      "  Batch 179/210 - Loss: 0.0256\n",
      "  Batch 180/210 - Loss: 0.0226\n",
      "  Batch 181/210 - Loss: 0.0331\n",
      "  Batch 182/210 - Loss: 0.0265\n",
      "  Batch 183/210 - Loss: 0.0271\n",
      "  Batch 184/210 - Loss: 0.0241\n",
      "  Batch 185/210 - Loss: 0.0300\n",
      "  Batch 186/210 - Loss: 0.0296\n",
      "  Batch 187/210 - Loss: 0.0212\n",
      "  Batch 188/210 - Loss: 0.0380\n",
      "  Batch 189/210 - Loss: 0.0295\n",
      "  Batch 190/210 - Loss: 0.0268\n",
      "  Batch 191/210 - Loss: 0.0330\n",
      "  Batch 192/210 - Loss: 0.0281\n",
      "  Batch 193/210 - Loss: 0.0281\n",
      "  Batch 194/210 - Loss: 0.0284\n",
      "  Batch 195/210 - Loss: 0.0321\n",
      "  Batch 196/210 - Loss: 0.0310\n",
      "  Batch 197/210 - Loss: 0.0276\n",
      "  Batch 198/210 - Loss: 0.0294\n",
      "  Batch 199/210 - Loss: 0.0301\n",
      "  Batch 200/210 - Loss: 0.0284\n",
      "  Batch 201/210 - Loss: 0.0249\n",
      "  Batch 202/210 - Loss: 0.0312\n",
      "  Batch 203/210 - Loss: 0.0243\n",
      "  Batch 204/210 - Loss: 0.0215\n",
      "  Batch 205/210 - Loss: 0.0292\n",
      "  Batch 206/210 - Loss: 0.0264\n",
      "  Batch 207/210 - Loss: 0.0279\n",
      "  Batch 208/210 - Loss: 0.0308\n",
      "  Batch 209/210 - Loss: 0.0208\n",
      "  Batch 210/210 - Loss: 0.0256\n",
      "Epoch 13 Completed. Train Loss: 0.0307, Val Loss: 0.0147\n",
      "\n",
      "Epoch 14/15\n",
      "  Batch 1/210 - Loss: 0.0232\n",
      "  Batch 2/210 - Loss: 0.0290\n",
      "  Batch 3/210 - Loss: 0.0325\n",
      "  Batch 4/210 - Loss: 0.0292\n",
      "  Batch 5/210 - Loss: 0.0313\n",
      "  Batch 6/210 - Loss: 0.0255\n",
      "  Batch 7/210 - Loss: 0.0281\n",
      "  Batch 8/210 - Loss: 0.0350\n",
      "  Batch 9/210 - Loss: 0.0261\n",
      "  Batch 10/210 - Loss: 0.0242\n",
      "  Batch 11/210 - Loss: 0.0264\n",
      "  Batch 12/210 - Loss: 0.0204\n",
      "  Batch 13/210 - Loss: 0.0322\n",
      "  Batch 14/210 - Loss: 0.0173\n",
      "  Batch 15/210 - Loss: 0.0246\n",
      "  Batch 16/210 - Loss: 0.0216\n",
      "  Batch 17/210 - Loss: 0.0299\n",
      "  Batch 18/210 - Loss: 0.0218\n",
      "  Batch 19/210 - Loss: 0.0267\n",
      "  Batch 20/210 - Loss: 0.0237\n",
      "  Batch 21/210 - Loss: 0.0276\n",
      "  Batch 22/210 - Loss: 0.0201\n",
      "  Batch 23/210 - Loss: 0.0317\n",
      "  Batch 24/210 - Loss: 0.0298\n",
      "  Batch 25/210 - Loss: 0.0243\n",
      "  Batch 26/210 - Loss: 0.0307\n",
      "  Batch 27/210 - Loss: 0.0269\n",
      "  Batch 28/210 - Loss: 0.0304\n",
      "  Batch 29/210 - Loss: 0.0274\n",
      "  Batch 30/210 - Loss: 0.0257\n",
      "  Batch 31/210 - Loss: 0.0408\n",
      "  Batch 32/210 - Loss: 0.0218\n",
      "  Batch 33/210 - Loss: 0.0312\n",
      "  Batch 34/210 - Loss: 0.0321\n",
      "  Batch 35/210 - Loss: 0.0285\n",
      "  Batch 36/210 - Loss: 0.0265\n",
      "  Batch 37/210 - Loss: 0.0236\n",
      "  Batch 38/210 - Loss: 0.0371\n",
      "  Batch 39/210 - Loss: 0.0205\n",
      "  Batch 40/210 - Loss: 0.0190\n",
      "  Batch 41/210 - Loss: 0.0161\n",
      "  Batch 42/210 - Loss: 0.0295\n",
      "  Batch 43/210 - Loss: 0.0355\n",
      "  Batch 44/210 - Loss: 0.0232\n",
      "  Batch 45/210 - Loss: 0.0217\n",
      "  Batch 46/210 - Loss: 0.0268\n",
      "  Batch 47/210 - Loss: 0.0283\n",
      "  Batch 48/210 - Loss: 0.0363\n",
      "  Batch 49/210 - Loss: 0.0288\n",
      "  Batch 50/210 - Loss: 0.0244\n",
      "  Batch 51/210 - Loss: 0.0270\n",
      "  Batch 52/210 - Loss: 0.0308\n",
      "  Batch 53/210 - Loss: 0.0212\n",
      "  Batch 54/210 - Loss: 0.0240\n",
      "  Batch 55/210 - Loss: 0.0200\n",
      "  Batch 56/210 - Loss: 0.0305\n",
      "  Batch 57/210 - Loss: 0.0213\n",
      "  Batch 58/210 - Loss: 0.0322\n",
      "  Batch 59/210 - Loss: 0.0275\n",
      "  Batch 60/210 - Loss: 0.0178\n",
      "  Batch 61/210 - Loss: 0.0322\n",
      "  Batch 62/210 - Loss: 0.0210\n",
      "  Batch 63/210 - Loss: 0.0256\n",
      "  Batch 64/210 - Loss: 0.0243\n",
      "  Batch 65/210 - Loss: 0.0325\n",
      "  Batch 66/210 - Loss: 0.0268\n",
      "  Batch 67/210 - Loss: 0.0278\n",
      "  Batch 68/210 - Loss: 0.0229\n",
      "  Batch 69/210 - Loss: 0.0207\n",
      "  Batch 70/210 - Loss: 0.0320\n",
      "  Batch 71/210 - Loss: 0.0239\n",
      "  Batch 72/210 - Loss: 0.0239\n",
      "  Batch 73/210 - Loss: 0.0371\n",
      "  Batch 74/210 - Loss: 0.0262\n",
      "  Batch 75/210 - Loss: 0.0274\n",
      "  Batch 76/210 - Loss: 0.0257\n",
      "  Batch 77/210 - Loss: 0.0278\n",
      "  Batch 78/210 - Loss: 0.0355\n",
      "  Batch 79/210 - Loss: 0.0180\n",
      "  Batch 80/210 - Loss: 0.0226\n",
      "  Batch 81/210 - Loss: 0.0283\n",
      "  Batch 82/210 - Loss: 0.0202\n",
      "  Batch 83/210 - Loss: 0.0285\n",
      "  Batch 84/210 - Loss: 0.0292\n",
      "  Batch 85/210 - Loss: 0.0233\n",
      "  Batch 86/210 - Loss: 0.0234\n",
      "  Batch 87/210 - Loss: 0.0242\n",
      "  Batch 88/210 - Loss: 0.0261\n",
      "  Batch 89/210 - Loss: 0.0272\n",
      "  Batch 90/210 - Loss: 0.0235\n",
      "  Batch 91/210 - Loss: 0.0243\n",
      "  Batch 92/210 - Loss: 0.0181\n",
      "  Batch 93/210 - Loss: 0.0270\n",
      "  Batch 94/210 - Loss: 0.0240\n",
      "  Batch 95/210 - Loss: 0.0219\n",
      "  Batch 96/210 - Loss: 0.0310\n",
      "  Batch 97/210 - Loss: 0.0298\n",
      "  Batch 98/210 - Loss: 0.0200\n",
      "  Batch 99/210 - Loss: 0.0175\n",
      "  Batch 100/210 - Loss: 0.0374\n",
      "  Batch 101/210 - Loss: 0.0274\n",
      "  Batch 102/210 - Loss: 0.0237\n",
      "  Batch 103/210 - Loss: 0.0189\n",
      "  Batch 104/210 - Loss: 0.0206\n",
      "  Batch 105/210 - Loss: 0.0293\n",
      "  Batch 106/210 - Loss: 0.0217\n",
      "  Batch 107/210 - Loss: 0.0288\n",
      "  Batch 108/210 - Loss: 0.0204\n",
      "  Batch 109/210 - Loss: 0.0203\n",
      "  Batch 110/210 - Loss: 0.0271\n",
      "  Batch 111/210 - Loss: 0.0240\n",
      "  Batch 112/210 - Loss: 0.0205\n",
      "  Batch 113/210 - Loss: 0.0182\n",
      "  Batch 114/210 - Loss: 0.0229\n",
      "  Batch 115/210 - Loss: 0.0144\n",
      "  Batch 116/210 - Loss: 0.0274\n",
      "  Batch 117/210 - Loss: 0.0179\n",
      "  Batch 118/210 - Loss: 0.0323\n",
      "  Batch 119/210 - Loss: 0.0180\n",
      "  Batch 120/210 - Loss: 0.0156\n",
      "  Batch 121/210 - Loss: 0.0218\n",
      "  Batch 122/210 - Loss: 0.0299\n",
      "  Batch 123/210 - Loss: 0.0276\n",
      "  Batch 124/210 - Loss: 0.0235\n",
      "  Batch 125/210 - Loss: 0.0257\n",
      "  Batch 126/210 - Loss: 0.0288\n",
      "  Batch 127/210 - Loss: 0.0338\n",
      "  Batch 128/210 - Loss: 0.0345\n",
      "  Batch 129/210 - Loss: 0.0255\n",
      "  Batch 130/210 - Loss: 0.0238\n",
      "  Batch 131/210 - Loss: 0.0350\n",
      "  Batch 132/210 - Loss: 0.0202\n",
      "  Batch 133/210 - Loss: 0.0259\n",
      "  Batch 134/210 - Loss: 0.0237\n",
      "  Batch 135/210 - Loss: 0.0282\n",
      "  Batch 136/210 - Loss: 0.0240\n",
      "  Batch 137/210 - Loss: 0.0249\n",
      "  Batch 138/210 - Loss: 0.0224\n",
      "  Batch 139/210 - Loss: 0.0225\n",
      "  Batch 140/210 - Loss: 0.0159\n",
      "  Batch 141/210 - Loss: 0.0229\n",
      "  Batch 142/210 - Loss: 0.0271\n",
      "  Batch 143/210 - Loss: 0.0317\n",
      "  Batch 144/210 - Loss: 0.0285\n",
      "  Batch 145/210 - Loss: 0.0233\n",
      "  Batch 146/210 - Loss: 0.0296\n",
      "  Batch 147/210 - Loss: 0.0238\n",
      "  Batch 148/210 - Loss: 0.0224\n",
      "  Batch 149/210 - Loss: 0.0177\n",
      "  Batch 150/210 - Loss: 0.0182\n",
      "  Batch 151/210 - Loss: 0.0182\n",
      "  Batch 152/210 - Loss: 0.0200\n",
      "  Batch 153/210 - Loss: 0.0258\n",
      "  Batch 154/210 - Loss: 0.0289\n",
      "  Batch 155/210 - Loss: 0.0279\n",
      "  Batch 156/210 - Loss: 0.0192\n",
      "  Batch 157/210 - Loss: 0.0186\n",
      "  Batch 158/210 - Loss: 0.0283\n",
      "  Batch 159/210 - Loss: 0.0202\n",
      "  Batch 160/210 - Loss: 0.0188\n",
      "  Batch 161/210 - Loss: 0.0186\n",
      "  Batch 162/210 - Loss: 0.0172\n",
      "  Batch 163/210 - Loss: 0.0212\n",
      "  Batch 164/210 - Loss: 0.0190\n",
      "  Batch 165/210 - Loss: 0.0307\n",
      "  Batch 166/210 - Loss: 0.0334\n",
      "  Batch 167/210 - Loss: 0.0182\n",
      "  Batch 168/210 - Loss: 0.0333\n",
      "  Batch 169/210 - Loss: 0.0175\n",
      "  Batch 170/210 - Loss: 0.0198\n",
      "  Batch 171/210 - Loss: 0.0197\n",
      "  Batch 172/210 - Loss: 0.0169\n",
      "  Batch 173/210 - Loss: 0.0246\n",
      "  Batch 174/210 - Loss: 0.0272\n",
      "  Batch 175/210 - Loss: 0.0281\n",
      "  Batch 176/210 - Loss: 0.0170\n",
      "  Batch 177/210 - Loss: 0.0365\n",
      "  Batch 178/210 - Loss: 0.0193\n",
      "  Batch 179/210 - Loss: 0.0208\n",
      "  Batch 180/210 - Loss: 0.0206\n",
      "  Batch 181/210 - Loss: 0.0306\n",
      "  Batch 182/210 - Loss: 0.0264\n",
      "  Batch 183/210 - Loss: 0.0200\n",
      "  Batch 184/210 - Loss: 0.0321\n",
      "  Batch 185/210 - Loss: 0.0185\n",
      "  Batch 186/210 - Loss: 0.0221\n",
      "  Batch 187/210 - Loss: 0.0210\n",
      "  Batch 188/210 - Loss: 0.0151\n",
      "  Batch 189/210 - Loss: 0.0202\n",
      "  Batch 190/210 - Loss: 0.0198\n",
      "  Batch 191/210 - Loss: 0.0159\n",
      "  Batch 192/210 - Loss: 0.0247\n",
      "  Batch 193/210 - Loss: 0.0227\n",
      "  Batch 194/210 - Loss: 0.0262\n",
      "  Batch 195/210 - Loss: 0.0310\n",
      "  Batch 196/210 - Loss: 0.0253\n",
      "  Batch 197/210 - Loss: 0.0309\n",
      "  Batch 198/210 - Loss: 0.0243\n",
      "  Batch 199/210 - Loss: 0.0152\n",
      "  Batch 200/210 - Loss: 0.0274\n",
      "  Batch 201/210 - Loss: 0.0264\n",
      "  Batch 202/210 - Loss: 0.0281\n",
      "  Batch 203/210 - Loss: 0.0232\n",
      "  Batch 204/210 - Loss: 0.0210\n",
      "  Batch 205/210 - Loss: 0.0118\n",
      "  Batch 206/210 - Loss: 0.0147\n",
      "  Batch 207/210 - Loss: 0.0199\n",
      "  Batch 208/210 - Loss: 0.0244\n",
      "  Batch 209/210 - Loss: 0.0184\n",
      "  Batch 210/210 - Loss: 0.0242\n",
      "Epoch 14 Completed. Train Loss: 0.0249, Val Loss: 0.0143\n",
      "\n",
      "Epoch 15/15\n",
      "  Batch 1/210 - Loss: 0.0168\n",
      "  Batch 2/210 - Loss: 0.0209\n",
      "  Batch 3/210 - Loss: 0.0224\n",
      "  Batch 4/210 - Loss: 0.0246\n",
      "  Batch 5/210 - Loss: 0.0171\n",
      "  Batch 6/210 - Loss: 0.0224\n",
      "  Batch 7/210 - Loss: 0.0295\n",
      "  Batch 8/210 - Loss: 0.0212\n",
      "  Batch 9/210 - Loss: 0.0197\n",
      "  Batch 10/210 - Loss: 0.0183\n",
      "  Batch 11/210 - Loss: 0.0165\n",
      "  Batch 12/210 - Loss: 0.0199\n",
      "  Batch 13/210 - Loss: 0.0271\n",
      "  Batch 14/210 - Loss: 0.0304\n",
      "  Batch 15/210 - Loss: 0.0185\n",
      "  Batch 16/210 - Loss: 0.0178\n",
      "  Batch 17/210 - Loss: 0.0197\n",
      "  Batch 18/210 - Loss: 0.0322\n",
      "  Batch 19/210 - Loss: 0.0239\n",
      "  Batch 20/210 - Loss: 0.0239\n",
      "  Batch 21/210 - Loss: 0.0250\n",
      "  Batch 22/210 - Loss: 0.0221\n",
      "  Batch 23/210 - Loss: 0.0217\n",
      "  Batch 24/210 - Loss: 0.0148\n",
      "  Batch 25/210 - Loss: 0.0200\n",
      "  Batch 26/210 - Loss: 0.0202\n",
      "  Batch 27/210 - Loss: 0.0142\n",
      "  Batch 28/210 - Loss: 0.0259\n",
      "  Batch 29/210 - Loss: 0.0214\n",
      "  Batch 30/210 - Loss: 0.0207\n",
      "  Batch 31/210 - Loss: 0.0228\n",
      "  Batch 32/210 - Loss: 0.0195\n",
      "  Batch 33/210 - Loss: 0.0166\n",
      "  Batch 34/210 - Loss: 0.0253\n",
      "  Batch 35/210 - Loss: 0.0159\n",
      "  Batch 36/210 - Loss: 0.0250\n",
      "  Batch 37/210 - Loss: 0.0208\n",
      "  Batch 38/210 - Loss: 0.0162\n",
      "  Batch 39/210 - Loss: 0.0206\n",
      "  Batch 40/210 - Loss: 0.0255\n",
      "  Batch 41/210 - Loss: 0.0139\n",
      "  Batch 42/210 - Loss: 0.0192\n",
      "  Batch 43/210 - Loss: 0.0214\n",
      "  Batch 44/210 - Loss: 0.0229\n",
      "  Batch 45/210 - Loss: 0.0174\n",
      "  Batch 46/210 - Loss: 0.0234\n",
      "  Batch 47/210 - Loss: 0.0214\n",
      "  Batch 48/210 - Loss: 0.0247\n",
      "  Batch 49/210 - Loss: 0.0142\n",
      "  Batch 50/210 - Loss: 0.0254\n",
      "  Batch 51/210 - Loss: 0.0117\n",
      "  Batch 52/210 - Loss: 0.0234\n",
      "  Batch 53/210 - Loss: 0.0202\n",
      "  Batch 54/210 - Loss: 0.0195\n",
      "  Batch 55/210 - Loss: 0.0223\n",
      "  Batch 56/210 - Loss: 0.0240\n",
      "  Batch 57/210 - Loss: 0.0133\n",
      "  Batch 58/210 - Loss: 0.0222\n",
      "  Batch 59/210 - Loss: 0.0183\n",
      "  Batch 60/210 - Loss: 0.0312\n",
      "  Batch 61/210 - Loss: 0.0296\n",
      "  Batch 62/210 - Loss: 0.0173\n",
      "  Batch 63/210 - Loss: 0.0224\n",
      "  Batch 64/210 - Loss: 0.0244\n",
      "  Batch 65/210 - Loss: 0.0238\n",
      "  Batch 66/210 - Loss: 0.0187\n",
      "  Batch 67/210 - Loss: 0.0198\n",
      "  Batch 68/210 - Loss: 0.0195\n",
      "  Batch 69/210 - Loss: 0.0231\n",
      "  Batch 70/210 - Loss: 0.0238\n",
      "  Batch 71/210 - Loss: 0.0170\n",
      "  Batch 72/210 - Loss: 0.0214\n",
      "  Batch 73/210 - Loss: 0.0185\n",
      "  Batch 74/210 - Loss: 0.0153\n",
      "  Batch 75/210 - Loss: 0.0317\n",
      "  Batch 76/210 - Loss: 0.0251\n",
      "  Batch 77/210 - Loss: 0.0129\n",
      "  Batch 78/210 - Loss: 0.0218\n",
      "  Batch 79/210 - Loss: 0.0264\n",
      "  Batch 80/210 - Loss: 0.0211\n",
      "  Batch 81/210 - Loss: 0.0158\n",
      "  Batch 82/210 - Loss: 0.0188\n",
      "  Batch 83/210 - Loss: 0.0282\n",
      "  Batch 84/210 - Loss: 0.0182\n",
      "  Batch 85/210 - Loss: 0.0211\n",
      "  Batch 86/210 - Loss: 0.0200\n",
      "  Batch 87/210 - Loss: 0.0180\n",
      "  Batch 88/210 - Loss: 0.0239\n",
      "  Batch 89/210 - Loss: 0.0286\n",
      "  Batch 90/210 - Loss: 0.0298\n",
      "  Batch 91/210 - Loss: 0.0200\n",
      "  Batch 92/210 - Loss: 0.0207\n",
      "  Batch 93/210 - Loss: 0.0134\n",
      "  Batch 94/210 - Loss: 0.0224\n",
      "  Batch 95/210 - Loss: 0.0244\n",
      "  Batch 96/210 - Loss: 0.0189\n",
      "  Batch 97/210 - Loss: 0.0202\n",
      "  Batch 98/210 - Loss: 0.0249\n",
      "  Batch 99/210 - Loss: 0.0265\n",
      "  Batch 100/210 - Loss: 0.0178\n",
      "  Batch 101/210 - Loss: 0.0231\n",
      "  Batch 102/210 - Loss: 0.0248\n",
      "  Batch 103/210 - Loss: 0.0167\n",
      "  Batch 104/210 - Loss: 0.0209\n",
      "  Batch 105/210 - Loss: 0.0211\n",
      "  Batch 106/210 - Loss: 0.0160\n",
      "  Batch 107/210 - Loss: 0.0250\n",
      "  Batch 108/210 - Loss: 0.0280\n",
      "  Batch 109/210 - Loss: 0.0122\n",
      "  Batch 110/210 - Loss: 0.0142\n",
      "  Batch 111/210 - Loss: 0.0178\n",
      "  Batch 112/210 - Loss: 0.0223\n",
      "  Batch 113/210 - Loss: 0.0203\n",
      "  Batch 114/210 - Loss: 0.0138\n",
      "  Batch 115/210 - Loss: 0.0222\n",
      "  Batch 116/210 - Loss: 0.0148\n",
      "  Batch 117/210 - Loss: 0.0172\n",
      "  Batch 118/210 - Loss: 0.0262\n",
      "  Batch 119/210 - Loss: 0.0166\n",
      "  Batch 120/210 - Loss: 0.0183\n",
      "  Batch 121/210 - Loss: 0.0250\n",
      "  Batch 122/210 - Loss: 0.0181\n",
      "  Batch 123/210 - Loss: 0.0217\n",
      "  Batch 124/210 - Loss: 0.0296\n",
      "  Batch 125/210 - Loss: 0.0180\n",
      "  Batch 126/210 - Loss: 0.0261\n",
      "  Batch 127/210 - Loss: 0.0173\n",
      "  Batch 128/210 - Loss: 0.0166\n",
      "  Batch 129/210 - Loss: 0.0212\n",
      "  Batch 130/210 - Loss: 0.0201\n",
      "  Batch 131/210 - Loss: 0.0156\n",
      "  Batch 132/210 - Loss: 0.0239\n",
      "  Batch 133/210 - Loss: 0.0176\n",
      "  Batch 134/210 - Loss: 0.0181\n",
      "  Batch 135/210 - Loss: 0.0150\n",
      "  Batch 136/210 - Loss: 0.0206\n",
      "  Batch 137/210 - Loss: 0.0213\n",
      "  Batch 138/210 - Loss: 0.0275\n",
      "  Batch 139/210 - Loss: 0.0196\n",
      "  Batch 140/210 - Loss: 0.0173\n",
      "  Batch 141/210 - Loss: 0.0218\n",
      "  Batch 142/210 - Loss: 0.0248\n",
      "  Batch 143/210 - Loss: 0.0232\n",
      "  Batch 144/210 - Loss: 0.0173\n",
      "  Batch 145/210 - Loss: 0.0153\n",
      "  Batch 146/210 - Loss: 0.0285\n",
      "  Batch 147/210 - Loss: 0.0179\n",
      "  Batch 148/210 - Loss: 0.0232\n",
      "  Batch 149/210 - Loss: 0.0225\n",
      "  Batch 150/210 - Loss: 0.0199\n",
      "  Batch 151/210 - Loss: 0.0193\n",
      "  Batch 152/210 - Loss: 0.0216\n",
      "  Batch 153/210 - Loss: 0.0143\n",
      "  Batch 154/210 - Loss: 0.0286\n",
      "  Batch 155/210 - Loss: 0.0166\n",
      "  Batch 156/210 - Loss: 0.0271\n",
      "  Batch 157/210 - Loss: 0.0215\n",
      "  Batch 158/210 - Loss: 0.0291\n",
      "  Batch 159/210 - Loss: 0.0195\n",
      "  Batch 160/210 - Loss: 0.0165\n",
      "  Batch 161/210 - Loss: 0.0243\n",
      "  Batch 162/210 - Loss: 0.0138\n",
      "  Batch 163/210 - Loss: 0.0183\n",
      "  Batch 164/210 - Loss: 0.0158\n",
      "  Batch 165/210 - Loss: 0.0207\n",
      "  Batch 166/210 - Loss: 0.0212\n",
      "  Batch 167/210 - Loss: 0.0184\n",
      "  Batch 168/210 - Loss: 0.0223\n",
      "  Batch 169/210 - Loss: 0.0232\n",
      "  Batch 170/210 - Loss: 0.0208\n",
      "  Batch 171/210 - Loss: 0.0139\n",
      "  Batch 172/210 - Loss: 0.0146\n",
      "  Batch 173/210 - Loss: 0.0190\n",
      "  Batch 174/210 - Loss: 0.0165\n",
      "  Batch 175/210 - Loss: 0.0296\n",
      "  Batch 176/210 - Loss: 0.0222\n",
      "  Batch 177/210 - Loss: 0.0309\n",
      "  Batch 178/210 - Loss: 0.0169\n",
      "  Batch 179/210 - Loss: 0.0213\n",
      "  Batch 180/210 - Loss: 0.0277\n",
      "  Batch 181/210 - Loss: 0.0221\n",
      "  Batch 182/210 - Loss: 0.0224\n",
      "  Batch 183/210 - Loss: 0.0143\n",
      "  Batch 184/210 - Loss: 0.0250\n",
      "  Batch 185/210 - Loss: 0.0268\n",
      "  Batch 186/210 - Loss: 0.0243\n",
      "  Batch 187/210 - Loss: 0.0182\n",
      "  Batch 188/210 - Loss: 0.0136\n",
      "  Batch 189/210 - Loss: 0.0225\n",
      "  Batch 190/210 - Loss: 0.0224\n",
      "  Batch 191/210 - Loss: 0.0251\n",
      "  Batch 192/210 - Loss: 0.0197\n",
      "  Batch 193/210 - Loss: 0.0151\n",
      "  Batch 194/210 - Loss: 0.0191\n",
      "  Batch 195/210 - Loss: 0.0146\n",
      "  Batch 196/210 - Loss: 0.0190\n",
      "  Batch 197/210 - Loss: 0.0129\n",
      "  Batch 198/210 - Loss: 0.0267\n",
      "  Batch 199/210 - Loss: 0.0213\n",
      "  Batch 200/210 - Loss: 0.0173\n",
      "  Batch 201/210 - Loss: 0.0250\n",
      "  Batch 202/210 - Loss: 0.0251\n",
      "  Batch 203/210 - Loss: 0.0163\n",
      "  Batch 204/210 - Loss: 0.0141\n",
      "  Batch 205/210 - Loss: 0.0240\n",
      "  Batch 206/210 - Loss: 0.0321\n",
      "  Batch 207/210 - Loss: 0.0246\n",
      "  Batch 208/210 - Loss: 0.0260\n",
      "  Batch 209/210 - Loss: 0.0267\n",
      "  Batch 210/210 - Loss: 0.0175\n",
      "Epoch 15 Completed. Train Loss: 0.0210, Val Loss: 0.0145\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aamir\\AppData\\Local\\Temp\\ipykernel_18448\\1437661571.py:218: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_transformer.pth'))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fraction.__new__() got an unexpected keyword argument '_normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 243\u001b[0m\n\u001b[0;32m    240\u001b[0m         references\u001b[38;5;241m.\u001b[39mappend(tgt_tokens)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Compute and display BLEU & ROUGE\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m bleu \u001b[38;5;241m=\u001b[39m compute_bleu(predictions, references)\n\u001b[0;32m    244\u001b[0m rouge \u001b[38;5;241m=\u001b[39m compute_rouge(predictions, references)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBLEU Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbleu\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 91\u001b[0m, in \u001b[0;36mcompute_bleu\u001b[1;34m(predictions, references)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_bleu\u001b[39m(predictions, references):\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m corpus_bleu([[ref] \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m references], predictions)\n",
      "File \u001b[1;32mc:\\Users\\aamir\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:210\u001b[0m, in \u001b[0;36mcorpus_bleu\u001b[1;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m references, hypothesis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(list_of_references, hypotheses):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# For each order of ngram, calculate the numerator and\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# denominator for the corpus-level modified precision.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_weight_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 210\u001b[0m         p_i \u001b[38;5;241m=\u001b[39m modified_precision(references, hypothesis, i)\n\u001b[0;32m    211\u001b[0m         p_numerators[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_i\u001b[38;5;241m.\u001b[39mnumerator\n\u001b[0;32m    212\u001b[0m         p_denominators[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_i\u001b[38;5;241m.\u001b[39mdenominator\n",
      "File \u001b[1;32mc:\\Users\\aamir\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:368\u001b[0m, in \u001b[0;36mmodified_precision\u001b[1;34m(references, hypothesis, n)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# Usually this happens when the ngram order is > len(reference).\u001b[39;00m\n\u001b[0;32m    366\u001b[0m denominator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28msum\u001b[39m(counts\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m--> 368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Fraction(numerator, denominator, _normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Fraction.__new__() got an unexpected keyword argument '_normalize'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# =================== Dataset ===================\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_lines, target_lines, src_tokenizer, tgt_tokenizer, max_len=100):\n",
    "        self.source_lines = source_lines\n",
    "        self.target_lines = target_lines\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.source_lines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src = self.source_lines[idx][:self.max_len]\n",
    "        tgt = self.target_lines[idx][:self.max_len]\n",
    "        \n",
    "        src_ids = [self.src_tokenizer.piece_to_id(p) for p in src]\n",
    "        tgt_ids = [self.tgt_tokenizer.piece_to_id(p) for p in tgt]\n",
    "        \n",
    "        return torch.tensor(src_ids), torch.tensor(tgt_ids)\n",
    "\n",
    "# =================== Positional Encoding ===================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# =================== Transformer Model ===================\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers,\n",
    "                                          dim_feedforward, dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        output = self.transformer(src_emb, tgt_emb, src_key_padding_mask=src_mask, tgt_key_padding_mask=tgt_mask)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# =================== Utilities ===================\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz))) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "    tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "def compute_bleu(predictions, references):\n",
    "    return corpus_bleu([[ref] for ref in references], predictions)\n",
    "\n",
    "def compute_rouge(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge1, rouge2, rougeL = [], [], []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_sentence = \" \".join(pred)\n",
    "        ref_sentence = \" \".join(ref)\n",
    "        scores = scorer.score(ref_sentence, pred_sentence)\n",
    "        rouge1.append(scores['rouge1'].fmeasure)\n",
    "        rouge2.append(scores['rouge2'].fmeasure)\n",
    "        rougeL.append(scores['rougeL'].fmeasure)\n",
    "    return {\n",
    "        \"rouge-1\": np.mean(rouge1),\n",
    "        \"rouge-2\": np.mean(rouge2),\n",
    "        \"rouge-L\": np.mean(rougeL)\n",
    "    }\n",
    "\n",
    "def plot_training(train_losses, val_losses):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# =================== Training Function ===================\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, num_epochs=30, patience=5):\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt_input)\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            loss = criterion(output, tgt_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            print(f\"  Batch {batch_idx+1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        avg_val_loss = evaluate_model(model, val_loader, criterion)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Completed. Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        scheduler.step(avg_val_loss)  # No 'verbose' here\n",
    "        \n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_transformer.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print('Early stopping triggered!')\n",
    "                break\n",
    "    \n",
    "    plot_training(train_losses, val_losses)\n",
    "\n",
    "def evaluate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            output = model(src, tgt_input)\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            loss = criterion(output, tgt_output)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# =================== Main Execution ===================\n",
    "\n",
    "train_src = tokenized_data['quran']['train.en'] + tokenized_data['bible']['train.en']\n",
    "train_tgt = tokenized_data['quran']['train.ur'] + tokenized_data['bible']['train.ur']\n",
    "\n",
    "val_src = tokenized_data['quran']['dev.en'] + tokenized_data['bible']['dev.en']\n",
    "val_tgt = tokenized_data['quran']['dev.ur'] + tokenized_data['bible']['dev.ur']\n",
    "\n",
    "test_src = tokenized_data['quran']['test.en'] + tokenized_data['bible']['test.en']\n",
    "test_tgt = tokenized_data['quran']['test.ur'] + tokenized_data['bible']['test.ur']\n",
    "\n",
    "train_dataset = TranslationDataset(train_src, train_tgt, tokenizer_en, tokenizer_ur)\n",
    "val_dataset = TranslationDataset(val_src, val_tgt, tokenizer_en, tokenizer_ur)\n",
    "test_dataset = TranslationDataset(test_src, test_tgt, tokenizer_en, tokenizer_ur)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(tokenizer_en)\n",
    "TGT_VOCAB_SIZE = len(tokenizer_ur)\n",
    "\n",
    "model = Transformer(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, d_model=256, nhead=4, num_encoder_layers=3, num_decoder_layers=3).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Train\n",
    "train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, num_epochs=15, patience=5)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_transformer.pth'))\n",
    "model.eval()\n",
    "\n",
    "predictions, references = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for src, tgt in test_loader:\n",
    "        src = src.to(device)\n",
    "        tgt_input = torch.zeros_like(src[:, :1]).fill_(2)  # Assuming 2 = <BOS> token id\n",
    "        outputs = []\n",
    "        \n",
    "        for _ in range(50):\n",
    "            output = model(src, tgt_input)\n",
    "            next_token = output[:, -1, :].argmax(-1, keepdim=True)\n",
    "            tgt_input = torch.cat([tgt_input, next_token], dim=1)\n",
    "            outputs.append(next_token)\n",
    "        \n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        pred_tokens = [tokenizer_ur.id_to_piece(int(tok)) for tok in outputs[0]]\n",
    "        tgt_tokens = [tokenizer_ur.id_to_piece(int(tok)) for tok in tgt[0]]\n",
    "        \n",
    "        predictions.append(pred_tokens)\n",
    "        references.append(tgt_tokens)\n",
    "\n",
    "# Compute and display BLEU & ROUGE\n",
    "bleu = compute_bleu(predictions, references)\n",
    "rouge = compute_rouge(predictions, references)\n",
    "\n",
    "print(f'\\nBLEU Score: {bleu:.4f}')\n",
    "print('ROUGE Scores:', rouge)\n",
    "\n",
    "# Visualization\n",
    "labels = list(rouge.keys())\n",
    "scores = list(rouge.values())\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.bar(labels, scores, color='skyblue')\n",
    "plt.title('ROUGE Scores')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f5b658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293c6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "209f9ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/13\n",
      "  Batch 1/210 - Loss: 7.1009\n",
      "  Batch 2/210 - Loss: 6.8162\n",
      "  Batch 3/210 - Loss: 6.6317\n",
      "  Batch 4/210 - Loss: 6.5375\n",
      "  Batch 5/210 - Loss: 6.4502\n",
      "  Batch 6/210 - Loss: 6.3997\n",
      "  Batch 7/210 - Loss: 6.3508\n",
      "  Batch 8/210 - Loss: 6.3275\n",
      "  Batch 9/210 - Loss: 6.2932\n",
      "  Batch 10/210 - Loss: 6.2760\n",
      "  Batch 11/210 - Loss: 6.2782\n",
      "  Batch 12/210 - Loss: 6.2384\n",
      "  Batch 13/210 - Loss: 6.2619\n",
      "  Batch 14/210 - Loss: 6.2240\n",
      "  Batch 15/210 - Loss: 6.1458\n",
      "  Batch 16/210 - Loss: 6.1652\n",
      "  Batch 17/210 - Loss: 6.1257\n",
      "  Batch 18/210 - Loss: 6.1259\n",
      "  Batch 19/210 - Loss: 6.1302\n",
      "  Batch 20/210 - Loss: 6.0971\n",
      "  Batch 21/210 - Loss: 6.0919\n",
      "  Batch 22/210 - Loss: 6.0332\n",
      "  Batch 23/210 - Loss: 6.0520\n",
      "  Batch 24/210 - Loss: 6.0212\n",
      "  Batch 25/210 - Loss: 5.9839\n",
      "  Batch 26/210 - Loss: 6.0013\n",
      "  Batch 27/210 - Loss: 5.9597\n",
      "  Batch 28/210 - Loss: 5.9726\n",
      "  Batch 29/210 - Loss: 5.8964\n",
      "  Batch 30/210 - Loss: 5.9275\n",
      "  Batch 31/210 - Loss: 5.9127\n",
      "  Batch 32/210 - Loss: 5.8933\n",
      "  Batch 33/210 - Loss: 5.8966\n",
      "  Batch 34/210 - Loss: 5.9010\n",
      "  Batch 35/210 - Loss: 5.8347\n",
      "  Batch 36/210 - Loss: 5.8470\n",
      "  Batch 37/210 - Loss: 5.8012\n",
      "  Batch 38/210 - Loss: 5.8555\n",
      "  Batch 39/210 - Loss: 5.8197\n",
      "  Batch 40/210 - Loss: 5.7632\n",
      "  Batch 41/210 - Loss: 5.7742\n",
      "  Batch 42/210 - Loss: 5.7440\n",
      "  Batch 43/210 - Loss: 5.7776\n",
      "  Batch 44/210 - Loss: 5.7525\n",
      "  Batch 45/210 - Loss: 5.7057\n",
      "  Batch 46/210 - Loss: 5.7092\n",
      "  Batch 47/210 - Loss: 5.6324\n",
      "  Batch 48/210 - Loss: 5.6424\n",
      "  Batch 49/210 - Loss: 5.6484\n",
      "  Batch 50/210 - Loss: 5.6114\n",
      "  Batch 51/210 - Loss: 5.5716\n",
      "  Batch 52/210 - Loss: 5.6273\n",
      "  Batch 53/210 - Loss: 5.6139\n",
      "  Batch 54/210 - Loss: 5.6666\n",
      "  Batch 55/210 - Loss: 5.6108\n",
      "  Batch 56/210 - Loss: 5.5517\n",
      "  Batch 57/210 - Loss: 5.5865\n",
      "  Batch 58/210 - Loss: 5.5072\n",
      "  Batch 59/210 - Loss: 5.5485\n",
      "  Batch 60/210 - Loss: 5.5337\n",
      "  Batch 61/210 - Loss: 5.4260\n",
      "  Batch 62/210 - Loss: 5.4724\n",
      "  Batch 63/210 - Loss: 5.4862\n",
      "  Batch 64/210 - Loss: 5.4269\n",
      "  Batch 65/210 - Loss: 5.4384\n",
      "  Batch 66/210 - Loss: 5.3634\n",
      "  Batch 67/210 - Loss: 5.4296\n",
      "  Batch 68/210 - Loss: 5.4391\n",
      "  Batch 69/210 - Loss: 5.3997\n",
      "  Batch 70/210 - Loss: 5.3501\n",
      "  Batch 71/210 - Loss: 5.4303\n",
      "  Batch 72/210 - Loss: 5.3567\n",
      "  Batch 73/210 - Loss: 5.3823\n",
      "  Batch 74/210 - Loss: 5.2877\n",
      "  Batch 75/210 - Loss: 5.3082\n",
      "  Batch 76/210 - Loss: 5.3193\n",
      "  Batch 77/210 - Loss: 5.3089\n",
      "  Batch 78/210 - Loss: 5.3251\n",
      "  Batch 79/210 - Loss: 5.2226\n",
      "  Batch 80/210 - Loss: 5.2357\n",
      "  Batch 81/210 - Loss: 5.2314\n",
      "  Batch 82/210 - Loss: 5.2191\n",
      "  Batch 83/210 - Loss: 5.2112\n",
      "  Batch 84/210 - Loss: 5.1853\n",
      "  Batch 85/210 - Loss: 5.2192\n",
      "  Batch 86/210 - Loss: 5.2111\n",
      "  Batch 87/210 - Loss: 5.1445\n",
      "  Batch 88/210 - Loss: 5.2205\n",
      "  Batch 89/210 - Loss: 5.1504\n",
      "  Batch 90/210 - Loss: 5.1480\n",
      "  Batch 91/210 - Loss: 5.1582\n",
      "  Batch 92/210 - Loss: 5.2034\n",
      "  Batch 93/210 - Loss: 5.1623\n",
      "  Batch 94/210 - Loss: 5.1164\n",
      "  Batch 95/210 - Loss: 5.0893\n",
      "  Batch 96/210 - Loss: 5.1034\n",
      "  Batch 97/210 - Loss: 5.0776\n",
      "  Batch 98/210 - Loss: 5.1360\n",
      "  Batch 99/210 - Loss: 5.1288\n",
      "  Batch 100/210 - Loss: 5.0135\n",
      "  Batch 101/210 - Loss: 5.0742\n",
      "  Batch 102/210 - Loss: 5.0080\n",
      "  Batch 103/210 - Loss: 5.0654\n",
      "  Batch 104/210 - Loss: 5.0094\n",
      "  Batch 105/210 - Loss: 5.0408\n",
      "  Batch 106/210 - Loss: 5.0361\n",
      "  Batch 107/210 - Loss: 4.9760\n",
      "  Batch 108/210 - Loss: 5.1154\n",
      "  Batch 109/210 - Loss: 5.0303\n",
      "  Batch 110/210 - Loss: 4.9883\n",
      "  Batch 111/210 - Loss: 4.9909\n",
      "  Batch 112/210 - Loss: 5.0357\n",
      "  Batch 113/210 - Loss: 4.9138\n",
      "  Batch 114/210 - Loss: 4.8904\n",
      "  Batch 115/210 - Loss: 4.9764\n",
      "  Batch 116/210 - Loss: 4.9315\n",
      "  Batch 117/210 - Loss: 4.9004\n",
      "  Batch 118/210 - Loss: 4.8501\n",
      "  Batch 119/210 - Loss: 4.8994\n",
      "  Batch 120/210 - Loss: 4.8771\n",
      "  Batch 121/210 - Loss: 4.8930\n",
      "  Batch 122/210 - Loss: 4.9218\n",
      "  Batch 123/210 - Loss: 4.8263\n",
      "  Batch 124/210 - Loss: 4.9247\n",
      "  Batch 125/210 - Loss: 4.8781\n",
      "  Batch 126/210 - Loss: 4.9501\n",
      "  Batch 127/210 - Loss: 4.8712\n",
      "  Batch 128/210 - Loss: 4.8153\n",
      "  Batch 129/210 - Loss: 4.7915\n",
      "  Batch 130/210 - Loss: 4.8576\n",
      "  Batch 131/210 - Loss: 4.9174\n",
      "  Batch 132/210 - Loss: 4.7649\n",
      "  Batch 133/210 - Loss: 4.8479\n",
      "  Batch 134/210 - Loss: 4.7751\n",
      "  Batch 135/210 - Loss: 4.7425\n",
      "  Batch 136/210 - Loss: 4.8329\n",
      "  Batch 137/210 - Loss: 4.8849\n",
      "  Batch 138/210 - Loss: 4.8254\n",
      "  Batch 139/210 - Loss: 4.7795\n",
      "  Batch 140/210 - Loss: 4.8097\n",
      "  Batch 141/210 - Loss: 4.7627\n",
      "  Batch 142/210 - Loss: 4.8507\n",
      "  Batch 143/210 - Loss: 4.6896\n",
      "  Batch 144/210 - Loss: 4.7038\n",
      "  Batch 145/210 - Loss: 4.6794\n",
      "  Batch 146/210 - Loss: 4.7629\n",
      "  Batch 147/210 - Loss: 4.7133\n",
      "  Batch 148/210 - Loss: 4.6813\n",
      "  Batch 149/210 - Loss: 4.6469\n",
      "  Batch 150/210 - Loss: 4.7347\n",
      "  Batch 151/210 - Loss: 4.6789\n",
      "  Batch 152/210 - Loss: 4.7419\n",
      "  Batch 153/210 - Loss: 4.6854\n",
      "  Batch 154/210 - Loss: 4.6769\n",
      "  Batch 155/210 - Loss: 4.6966\n",
      "  Batch 156/210 - Loss: 4.7373\n",
      "  Batch 157/210 - Loss: 4.6604\n",
      "  Batch 158/210 - Loss: 4.6623\n",
      "  Batch 159/210 - Loss: 4.7101\n",
      "  Batch 160/210 - Loss: 4.6674\n",
      "  Batch 161/210 - Loss: 4.6604\n",
      "  Batch 162/210 - Loss: 4.6993\n",
      "  Batch 163/210 - Loss: 4.6511\n",
      "  Batch 164/210 - Loss: 4.6034\n",
      "  Batch 165/210 - Loss: 4.6928\n",
      "  Batch 166/210 - Loss: 4.5878\n",
      "  Batch 167/210 - Loss: 4.6136\n",
      "  Batch 168/210 - Loss: 4.6622\n",
      "  Batch 169/210 - Loss: 4.6394\n",
      "  Batch 170/210 - Loss: 4.6419\n",
      "  Batch 171/210 - Loss: 4.7009\n",
      "  Batch 172/210 - Loss: 4.6807\n",
      "  Batch 173/210 - Loss: 4.5692\n",
      "  Batch 174/210 - Loss: 4.6435\n",
      "  Batch 175/210 - Loss: 4.6138\n",
      "  Batch 176/210 - Loss: 4.5695\n",
      "  Batch 177/210 - Loss: 4.5589\n",
      "  Batch 178/210 - Loss: 4.6022\n",
      "  Batch 179/210 - Loss: 4.6163\n",
      "  Batch 180/210 - Loss: 4.5732\n",
      "  Batch 181/210 - Loss: 4.6037\n",
      "  Batch 182/210 - Loss: 4.5447\n",
      "  Batch 183/210 - Loss: 4.6533\n",
      "  Batch 184/210 - Loss: 4.5816\n",
      "  Batch 185/210 - Loss: 4.4886\n",
      "  Batch 186/210 - Loss: 4.5627\n",
      "  Batch 187/210 - Loss: 4.5075\n",
      "  Batch 188/210 - Loss: 4.4752\n",
      "  Batch 189/210 - Loss: 4.5324\n",
      "  Batch 190/210 - Loss: 4.5497\n",
      "  Batch 191/210 - Loss: 4.5393\n",
      "  Batch 192/210 - Loss: 4.5087\n",
      "  Batch 193/210 - Loss: 4.5455\n",
      "  Batch 194/210 - Loss: 4.5158\n",
      "  Batch 195/210 - Loss: 4.5001\n",
      "  Batch 196/210 - Loss: 4.5175\n",
      "  Batch 197/210 - Loss: 4.5869\n",
      "  Batch 198/210 - Loss: 4.5352\n",
      "  Batch 199/210 - Loss: 4.5275\n",
      "  Batch 200/210 - Loss: 4.5337\n",
      "  Batch 201/210 - Loss: 4.3872\n",
      "  Batch 202/210 - Loss: 4.3839\n",
      "  Batch 203/210 - Loss: 4.4052\n",
      "  Batch 204/210 - Loss: 4.5002\n",
      "  Batch 205/210 - Loss: 4.5474\n",
      "  Batch 206/210 - Loss: 4.4110\n",
      "  Batch 207/210 - Loss: 4.4347\n",
      "  Batch 208/210 - Loss: 4.3925\n",
      "  Batch 209/210 - Loss: 4.4687\n",
      "  Batch 210/210 - Loss: 4.4404\n",
      "Epoch 1 Completed. Train Loss: 5.1772, Val Loss: 4.4587\n",
      "\n",
      "Epoch 2/13\n",
      "  Batch 1/210 - Loss: 4.4685\n",
      "  Batch 2/210 - Loss: 4.4228\n",
      "  Batch 3/210 - Loss: 4.3828\n",
      "  Batch 4/210 - Loss: 4.3841\n",
      "  Batch 5/210 - Loss: 4.4070\n",
      "  Batch 6/210 - Loss: 4.4275\n",
      "  Batch 7/210 - Loss: 4.2998\n",
      "  Batch 8/210 - Loss: 4.3829\n",
      "  Batch 9/210 - Loss: 4.4018\n",
      "  Batch 10/210 - Loss: 4.4032\n",
      "  Batch 11/210 - Loss: 4.3504\n",
      "  Batch 12/210 - Loss: 4.3577\n",
      "  Batch 13/210 - Loss: 4.3645\n",
      "  Batch 14/210 - Loss: 4.3908\n",
      "  Batch 15/210 - Loss: 4.3232\n",
      "  Batch 16/210 - Loss: 4.3671\n",
      "  Batch 17/210 - Loss: 4.4154\n",
      "  Batch 18/210 - Loss: 4.3092\n",
      "  Batch 19/210 - Loss: 4.3428\n",
      "  Batch 20/210 - Loss: 4.2904\n",
      "  Batch 21/210 - Loss: 4.3814\n",
      "  Batch 22/210 - Loss: 4.2903\n",
      "  Batch 23/210 - Loss: 4.2877\n",
      "  Batch 24/210 - Loss: 4.3243\n",
      "  Batch 25/210 - Loss: 4.3746\n",
      "  Batch 26/210 - Loss: 4.2896\n",
      "  Batch 27/210 - Loss: 4.2266\n",
      "  Batch 28/210 - Loss: 4.2831\n",
      "  Batch 29/210 - Loss: 4.3111\n",
      "  Batch 30/210 - Loss: 4.2890\n",
      "  Batch 31/210 - Loss: 4.2874\n",
      "  Batch 32/210 - Loss: 4.2439\n",
      "  Batch 33/210 - Loss: 4.2948\n",
      "  Batch 34/210 - Loss: 4.3096\n",
      "  Batch 35/210 - Loss: 4.2541\n",
      "  Batch 36/210 - Loss: 4.2380\n",
      "  Batch 37/210 - Loss: 4.2732\n",
      "  Batch 38/210 - Loss: 4.3059\n",
      "  Batch 39/210 - Loss: 4.2181\n",
      "  Batch 40/210 - Loss: 4.2644\n",
      "  Batch 41/210 - Loss: 4.1958\n",
      "  Batch 42/210 - Loss: 4.2160\n",
      "  Batch 43/210 - Loss: 4.2761\n",
      "  Batch 44/210 - Loss: 4.2241\n",
      "  Batch 45/210 - Loss: 4.1719\n",
      "  Batch 46/210 - Loss: 4.2425\n",
      "  Batch 47/210 - Loss: 4.2842\n",
      "  Batch 48/210 - Loss: 4.2040\n",
      "  Batch 49/210 - Loss: 4.1909\n",
      "  Batch 50/210 - Loss: 4.1745\n",
      "  Batch 51/210 - Loss: 4.1806\n",
      "  Batch 52/210 - Loss: 4.1749\n",
      "  Batch 53/210 - Loss: 4.2185\n",
      "  Batch 54/210 - Loss: 4.1905\n",
      "  Batch 55/210 - Loss: 4.2039\n",
      "  Batch 56/210 - Loss: 4.2191\n",
      "  Batch 57/210 - Loss: 4.1651\n",
      "  Batch 58/210 - Loss: 4.2407\n",
      "  Batch 59/210 - Loss: 4.3110\n",
      "  Batch 60/210 - Loss: 4.2022\n",
      "  Batch 61/210 - Loss: 4.2076\n",
      "  Batch 62/210 - Loss: 4.1314\n",
      "  Batch 63/210 - Loss: 4.1866\n",
      "  Batch 64/210 - Loss: 4.1981\n",
      "  Batch 65/210 - Loss: 4.2430\n",
      "  Batch 66/210 - Loss: 4.1911\n",
      "  Batch 67/210 - Loss: 4.2050\n",
      "  Batch 68/210 - Loss: 4.2182\n",
      "  Batch 69/210 - Loss: 4.0481\n",
      "  Batch 70/210 - Loss: 4.1686\n",
      "  Batch 71/210 - Loss: 4.0240\n",
      "  Batch 72/210 - Loss: 4.1080\n",
      "  Batch 73/210 - Loss: 4.1662\n",
      "  Batch 74/210 - Loss: 4.1528\n",
      "  Batch 75/210 - Loss: 4.0693\n",
      "  Batch 76/210 - Loss: 4.1205\n",
      "  Batch 77/210 - Loss: 4.1290\n",
      "  Batch 78/210 - Loss: 4.0336\n",
      "  Batch 79/210 - Loss: 4.1961\n",
      "  Batch 80/210 - Loss: 4.2414\n",
      "  Batch 81/210 - Loss: 4.1191\n",
      "  Batch 82/210 - Loss: 4.0846\n",
      "  Batch 83/210 - Loss: 4.0530\n",
      "  Batch 84/210 - Loss: 4.0211\n",
      "  Batch 85/210 - Loss: 4.0445\n",
      "  Batch 86/210 - Loss: 4.0742\n",
      "  Batch 87/210 - Loss: 4.1130\n",
      "  Batch 88/210 - Loss: 4.0400\n",
      "  Batch 89/210 - Loss: 4.1673\n",
      "  Batch 90/210 - Loss: 4.1045\n",
      "  Batch 91/210 - Loss: 4.0683\n",
      "  Batch 92/210 - Loss: 3.9955\n",
      "  Batch 93/210 - Loss: 4.1061\n",
      "  Batch 94/210 - Loss: 4.0339\n",
      "  Batch 95/210 - Loss: 3.9657\n",
      "  Batch 96/210 - Loss: 4.0824\n",
      "  Batch 97/210 - Loss: 4.0209\n",
      "  Batch 98/210 - Loss: 3.9895\n",
      "  Batch 99/210 - Loss: 4.1175\n",
      "  Batch 100/210 - Loss: 4.1357\n",
      "  Batch 101/210 - Loss: 3.9715\n",
      "  Batch 102/210 - Loss: 4.1191\n",
      "  Batch 103/210 - Loss: 4.0893\n",
      "  Batch 104/210 - Loss: 4.0774\n",
      "  Batch 105/210 - Loss: 4.0164\n",
      "  Batch 106/210 - Loss: 4.0165\n",
      "  Batch 107/210 - Loss: 3.9624\n",
      "  Batch 108/210 - Loss: 4.0166\n",
      "  Batch 109/210 - Loss: 4.1255\n",
      "  Batch 110/210 - Loss: 4.0220\n",
      "  Batch 111/210 - Loss: 3.9229\n",
      "  Batch 112/210 - Loss: 3.9647\n",
      "  Batch 113/210 - Loss: 4.0348\n",
      "  Batch 114/210 - Loss: 3.9310\n",
      "  Batch 115/210 - Loss: 3.9613\n",
      "  Batch 116/210 - Loss: 3.9527\n",
      "  Batch 117/210 - Loss: 3.9364\n",
      "  Batch 118/210 - Loss: 3.9664\n",
      "  Batch 119/210 - Loss: 3.9314\n",
      "  Batch 120/210 - Loss: 3.9636\n",
      "  Batch 121/210 - Loss: 3.8740\n",
      "  Batch 122/210 - Loss: 3.8838\n",
      "  Batch 123/210 - Loss: 3.9106\n",
      "  Batch 124/210 - Loss: 3.8382\n",
      "  Batch 125/210 - Loss: 3.7843\n",
      "  Batch 126/210 - Loss: 3.8964\n",
      "  Batch 127/210 - Loss: 3.8038\n",
      "  Batch 128/210 - Loss: 3.8730\n",
      "  Batch 129/210 - Loss: 3.9496\n",
      "  Batch 130/210 - Loss: 3.9635\n",
      "  Batch 131/210 - Loss: 3.8875\n",
      "  Batch 132/210 - Loss: 3.9153\n",
      "  Batch 133/210 - Loss: 3.9227\n",
      "  Batch 134/210 - Loss: 3.9767\n",
      "  Batch 135/210 - Loss: 3.8405\n",
      "  Batch 136/210 - Loss: 3.9700\n",
      "  Batch 137/210 - Loss: 3.7999\n",
      "  Batch 138/210 - Loss: 3.9505\n",
      "  Batch 139/210 - Loss: 3.9627\n",
      "  Batch 140/210 - Loss: 3.9356\n",
      "  Batch 141/210 - Loss: 3.8497\n",
      "  Batch 142/210 - Loss: 3.9750\n",
      "  Batch 143/210 - Loss: 3.9292\n",
      "  Batch 144/210 - Loss: 3.8160\n",
      "  Batch 145/210 - Loss: 3.7899\n",
      "  Batch 146/210 - Loss: 3.7609\n",
      "  Batch 147/210 - Loss: 3.8430\n",
      "  Batch 148/210 - Loss: 3.9244\n",
      "  Batch 149/210 - Loss: 3.7963\n",
      "  Batch 150/210 - Loss: 3.8842\n",
      "  Batch 151/210 - Loss: 3.8325\n",
      "  Batch 152/210 - Loss: 3.8801\n",
      "  Batch 153/210 - Loss: 3.7795\n",
      "  Batch 154/210 - Loss: 3.7900\n",
      "  Batch 155/210 - Loss: 3.9329\n",
      "  Batch 156/210 - Loss: 3.7695\n",
      "  Batch 157/210 - Loss: 3.6842\n",
      "  Batch 158/210 - Loss: 3.9393\n",
      "  Batch 159/210 - Loss: 3.7804\n",
      "  Batch 160/210 - Loss: 3.7958\n",
      "  Batch 161/210 - Loss: 3.7314\n",
      "  Batch 162/210 - Loss: 3.7320\n",
      "  Batch 163/210 - Loss: 3.7697\n",
      "  Batch 164/210 - Loss: 3.7181\n",
      "  Batch 165/210 - Loss: 3.7308\n",
      "  Batch 166/210 - Loss: 3.6954\n",
      "  Batch 167/210 - Loss: 3.7785\n",
      "  Batch 168/210 - Loss: 3.8154\n",
      "  Batch 169/210 - Loss: 3.8224\n",
      "  Batch 170/210 - Loss: 3.7086\n",
      "  Batch 171/210 - Loss: 3.7521\n",
      "  Batch 172/210 - Loss: 3.6009\n",
      "  Batch 173/210 - Loss: 3.7559\n",
      "  Batch 174/210 - Loss: 3.7667\n",
      "  Batch 175/210 - Loss: 3.7450\n",
      "  Batch 176/210 - Loss: 3.6961\n",
      "  Batch 177/210 - Loss: 3.6882\n",
      "  Batch 178/210 - Loss: 3.6557\n",
      "  Batch 179/210 - Loss: 3.7703\n",
      "  Batch 180/210 - Loss: 3.7201\n",
      "  Batch 181/210 - Loss: 3.6892\n",
      "  Batch 182/210 - Loss: 3.7619\n",
      "  Batch 183/210 - Loss: 3.6986\n",
      "  Batch 184/210 - Loss: 3.6191\n",
      "  Batch 185/210 - Loss: 3.7871\n",
      "  Batch 186/210 - Loss: 3.6382\n",
      "  Batch 187/210 - Loss: 3.6510\n",
      "  Batch 188/210 - Loss: 3.7310\n",
      "  Batch 189/210 - Loss: 3.6524\n",
      "  Batch 190/210 - Loss: 3.6758\n",
      "  Batch 191/210 - Loss: 3.6065\n",
      "  Batch 192/210 - Loss: 3.6356\n",
      "  Batch 193/210 - Loss: 3.6669\n",
      "  Batch 194/210 - Loss: 3.6409\n",
      "  Batch 195/210 - Loss: 3.6244\n",
      "  Batch 196/210 - Loss: 3.6854\n",
      "  Batch 197/210 - Loss: 3.6190\n",
      "  Batch 198/210 - Loss: 3.6052\n",
      "  Batch 199/210 - Loss: 3.6097\n",
      "  Batch 200/210 - Loss: 3.5366\n",
      "  Batch 201/210 - Loss: 3.5657\n",
      "  Batch 202/210 - Loss: 3.6259\n",
      "  Batch 203/210 - Loss: 3.6083\n",
      "  Batch 204/210 - Loss: 3.5398\n",
      "  Batch 205/210 - Loss: 3.6065\n",
      "  Batch 206/210 - Loss: 3.6247\n",
      "  Batch 207/210 - Loss: 3.6134\n",
      "  Batch 208/210 - Loss: 3.6183\n",
      "  Batch 209/210 - Loss: 3.4055\n",
      "  Batch 210/210 - Loss: 3.5857\n",
      "Epoch 2 Completed. Train Loss: 4.0007, Val Loss: 3.4459\n",
      "\n",
      "Epoch 3/13\n",
      "  Batch 1/210 - Loss: 3.5074\n",
      "  Batch 2/210 - Loss: 3.4576\n",
      "  Batch 3/210 - Loss: 3.5883\n",
      "  Batch 4/210 - Loss: 3.5873\n",
      "  Batch 5/210 - Loss: 3.5209\n",
      "  Batch 6/210 - Loss: 3.6011\n",
      "  Batch 7/210 - Loss: 3.5121\n",
      "  Batch 8/210 - Loss: 3.4899\n",
      "  Batch 9/210 - Loss: 3.4823\n",
      "  Batch 10/210 - Loss: 3.4994\n",
      "  Batch 11/210 - Loss: 3.3434\n",
      "  Batch 12/210 - Loss: 3.4615\n",
      "  Batch 13/210 - Loss: 3.4631\n",
      "  Batch 14/210 - Loss: 3.4666\n",
      "  Batch 15/210 - Loss: 3.4216\n",
      "  Batch 16/210 - Loss: 3.3639\n",
      "  Batch 17/210 - Loss: 3.4954\n",
      "  Batch 18/210 - Loss: 3.3914\n",
      "  Batch 19/210 - Loss: 3.4149\n",
      "  Batch 20/210 - Loss: 3.4545\n",
      "  Batch 21/210 - Loss: 3.3703\n",
      "  Batch 22/210 - Loss: 3.3840\n",
      "  Batch 23/210 - Loss: 3.3635\n",
      "  Batch 24/210 - Loss: 3.4054\n",
      "  Batch 25/210 - Loss: 3.4210\n",
      "  Batch 26/210 - Loss: 3.4341\n",
      "  Batch 27/210 - Loss: 3.3709\n",
      "  Batch 28/210 - Loss: 3.2897\n",
      "  Batch 29/210 - Loss: 3.2701\n",
      "  Batch 30/210 - Loss: 3.3399\n",
      "  Batch 31/210 - Loss: 3.2582\n",
      "  Batch 32/210 - Loss: 3.3739\n",
      "  Batch 33/210 - Loss: 3.4317\n",
      "  Batch 34/210 - Loss: 3.3249\n",
      "  Batch 35/210 - Loss: 3.2922\n",
      "  Batch 36/210 - Loss: 3.2756\n",
      "  Batch 37/210 - Loss: 3.3611\n",
      "  Batch 38/210 - Loss: 3.3199\n",
      "  Batch 39/210 - Loss: 3.3082\n",
      "  Batch 40/210 - Loss: 3.3590\n",
      "  Batch 41/210 - Loss: 3.2932\n",
      "  Batch 42/210 - Loss: 3.2393\n",
      "  Batch 43/210 - Loss: 3.3937\n",
      "  Batch 44/210 - Loss: 3.3407\n",
      "  Batch 45/210 - Loss: 3.2522\n",
      "  Batch 46/210 - Loss: 3.3420\n",
      "  Batch 47/210 - Loss: 3.2875\n",
      "  Batch 48/210 - Loss: 3.2483\n",
      "  Batch 49/210 - Loss: 3.2350\n",
      "  Batch 50/210 - Loss: 3.1908\n",
      "  Batch 51/210 - Loss: 3.2101\n",
      "  Batch 52/210 - Loss: 3.3526\n",
      "  Batch 53/210 - Loss: 3.2238\n",
      "  Batch 54/210 - Loss: 3.3161\n",
      "  Batch 55/210 - Loss: 3.2370\n",
      "  Batch 56/210 - Loss: 3.2603\n",
      "  Batch 57/210 - Loss: 3.2448\n",
      "  Batch 58/210 - Loss: 3.1574\n",
      "  Batch 59/210 - Loss: 3.2271\n",
      "  Batch 60/210 - Loss: 3.2096\n",
      "  Batch 61/210 - Loss: 3.2221\n",
      "  Batch 62/210 - Loss: 3.2023\n",
      "  Batch 63/210 - Loss: 3.1587\n",
      "  Batch 64/210 - Loss: 3.2137\n",
      "  Batch 65/210 - Loss: 3.0916\n",
      "  Batch 66/210 - Loss: 3.1671\n",
      "  Batch 67/210 - Loss: 3.3088\n",
      "  Batch 68/210 - Loss: 3.2287\n",
      "  Batch 69/210 - Loss: 3.2298\n",
      "  Batch 70/210 - Loss: 3.1437\n",
      "  Batch 71/210 - Loss: 3.1875\n",
      "  Batch 72/210 - Loss: 3.2066\n",
      "  Batch 73/210 - Loss: 3.2041\n",
      "  Batch 74/210 - Loss: 3.1561\n",
      "  Batch 75/210 - Loss: 3.1002\n",
      "  Batch 76/210 - Loss: 3.1146\n",
      "  Batch 77/210 - Loss: 3.2507\n",
      "  Batch 78/210 - Loss: 3.1507\n",
      "  Batch 79/210 - Loss: 3.2197\n",
      "  Batch 80/210 - Loss: 3.2000\n",
      "  Batch 81/210 - Loss: 3.0723\n",
      "  Batch 82/210 - Loss: 3.1633\n",
      "  Batch 83/210 - Loss: 3.0731\n",
      "  Batch 84/210 - Loss: 3.1275\n",
      "  Batch 85/210 - Loss: 3.1432\n",
      "  Batch 86/210 - Loss: 3.2314\n",
      "  Batch 87/210 - Loss: 2.9934\n",
      "  Batch 88/210 - Loss: 3.1170\n",
      "  Batch 89/210 - Loss: 3.0775\n",
      "  Batch 90/210 - Loss: 3.0176\n",
      "  Batch 91/210 - Loss: 3.1482\n",
      "  Batch 92/210 - Loss: 3.1904\n",
      "  Batch 93/210 - Loss: 3.0528\n",
      "  Batch 94/210 - Loss: 3.0710\n",
      "  Batch 95/210 - Loss: 3.0861\n",
      "  Batch 96/210 - Loss: 3.1517\n",
      "  Batch 97/210 - Loss: 3.0815\n",
      "  Batch 98/210 - Loss: 3.0799\n",
      "  Batch 99/210 - Loss: 3.0701\n",
      "  Batch 100/210 - Loss: 3.0734\n",
      "  Batch 101/210 - Loss: 3.0898\n",
      "  Batch 102/210 - Loss: 2.9569\n",
      "  Batch 103/210 - Loss: 3.0020\n",
      "  Batch 104/210 - Loss: 2.9962\n",
      "  Batch 105/210 - Loss: 2.8986\n",
      "  Batch 106/210 - Loss: 3.1396\n",
      "  Batch 107/210 - Loss: 2.9042\n",
      "  Batch 108/210 - Loss: 3.0781\n",
      "  Batch 109/210 - Loss: 2.9981\n",
      "  Batch 110/210 - Loss: 3.0322\n",
      "  Batch 111/210 - Loss: 2.9614\n",
      "  Batch 112/210 - Loss: 2.8593\n",
      "  Batch 113/210 - Loss: 3.0069\n",
      "  Batch 114/210 - Loss: 2.9455\n",
      "  Batch 115/210 - Loss: 2.9979\n",
      "  Batch 116/210 - Loss: 2.9823\n",
      "  Batch 117/210 - Loss: 2.8878\n",
      "  Batch 118/210 - Loss: 3.0248\n",
      "  Batch 119/210 - Loss: 2.8966\n",
      "  Batch 120/210 - Loss: 2.9676\n",
      "  Batch 121/210 - Loss: 2.8817\n",
      "  Batch 122/210 - Loss: 2.9028\n",
      "  Batch 123/210 - Loss: 3.0283\n",
      "  Batch 124/210 - Loss: 2.9036\n",
      "  Batch 125/210 - Loss: 2.8215\n",
      "  Batch 126/210 - Loss: 2.8622\n",
      "  Batch 127/210 - Loss: 2.8279\n",
      "  Batch 128/210 - Loss: 3.0078\n",
      "  Batch 129/210 - Loss: 2.8518\n",
      "  Batch 130/210 - Loss: 3.0485\n",
      "  Batch 131/210 - Loss: 2.8824\n",
      "  Batch 132/210 - Loss: 2.9988\n",
      "  Batch 133/210 - Loss: 2.8230\n",
      "  Batch 134/210 - Loss: 2.7843\n",
      "  Batch 135/210 - Loss: 2.7083\n",
      "  Batch 136/210 - Loss: 2.8098\n",
      "  Batch 137/210 - Loss: 2.7770\n",
      "  Batch 138/210 - Loss: 2.8817\n",
      "  Batch 139/210 - Loss: 2.8079\n",
      "  Batch 140/210 - Loss: 2.7801\n",
      "  Batch 141/210 - Loss: 2.7741\n",
      "  Batch 142/210 - Loss: 2.7492\n",
      "  Batch 143/210 - Loss: 2.8132\n",
      "  Batch 144/210 - Loss: 2.7707\n",
      "  Batch 145/210 - Loss: 2.7851\n",
      "  Batch 146/210 - Loss: 2.8528\n",
      "  Batch 147/210 - Loss: 2.6476\n",
      "  Batch 148/210 - Loss: 2.7654\n",
      "  Batch 149/210 - Loss: 2.7642\n",
      "  Batch 150/210 - Loss: 2.8052\n",
      "  Batch 151/210 - Loss: 2.6807\n",
      "  Batch 152/210 - Loss: 2.6786\n",
      "  Batch 153/210 - Loss: 2.7476\n",
      "  Batch 154/210 - Loss: 2.7423\n",
      "  Batch 155/210 - Loss: 2.8097\n",
      "  Batch 156/210 - Loss: 2.6506\n",
      "  Batch 157/210 - Loss: 2.7353\n",
      "  Batch 158/210 - Loss: 2.7524\n",
      "  Batch 159/210 - Loss: 2.6831\n",
      "  Batch 160/210 - Loss: 2.6705\n",
      "  Batch 161/210 - Loss: 2.6700\n",
      "  Batch 162/210 - Loss: 2.7471\n",
      "  Batch 163/210 - Loss: 2.5568\n",
      "  Batch 164/210 - Loss: 2.6628\n",
      "  Batch 165/210 - Loss: 2.7080\n",
      "  Batch 166/210 - Loss: 2.6211\n",
      "  Batch 167/210 - Loss: 2.6410\n",
      "  Batch 168/210 - Loss: 2.5564\n",
      "  Batch 169/210 - Loss: 2.4964\n",
      "  Batch 170/210 - Loss: 2.5315\n",
      "  Batch 171/210 - Loss: 2.6648\n",
      "  Batch 172/210 - Loss: 2.6205\n",
      "  Batch 173/210 - Loss: 2.6258\n",
      "  Batch 174/210 - Loss: 2.5517\n",
      "  Batch 175/210 - Loss: 2.5584\n",
      "  Batch 176/210 - Loss: 2.5532\n",
      "  Batch 177/210 - Loss: 2.6191\n",
      "  Batch 178/210 - Loss: 2.5579\n",
      "  Batch 179/210 - Loss: 2.5353\n",
      "  Batch 180/210 - Loss: 2.5400\n",
      "  Batch 181/210 - Loss: 2.4462\n",
      "  Batch 182/210 - Loss: 2.5080\n",
      "  Batch 183/210 - Loss: 2.3708\n",
      "  Batch 184/210 - Loss: 2.6065\n",
      "  Batch 185/210 - Loss: 2.6924\n",
      "  Batch 186/210 - Loss: 2.3705\n",
      "  Batch 187/210 - Loss: 2.5189\n",
      "  Batch 188/210 - Loss: 2.6160\n",
      "  Batch 189/210 - Loss: 2.4923\n",
      "  Batch 190/210 - Loss: 2.5213\n",
      "  Batch 191/210 - Loss: 2.4245\n",
      "  Batch 192/210 - Loss: 2.4261\n",
      "  Batch 193/210 - Loss: 2.4275\n",
      "  Batch 194/210 - Loss: 2.5473\n",
      "  Batch 195/210 - Loss: 2.3955\n",
      "  Batch 196/210 - Loss: 2.4502\n",
      "  Batch 197/210 - Loss: 2.5625\n",
      "  Batch 198/210 - Loss: 2.3939\n",
      "  Batch 199/210 - Loss: 2.5246\n",
      "  Batch 200/210 - Loss: 2.5353\n",
      "  Batch 201/210 - Loss: 2.3547\n",
      "  Batch 202/210 - Loss: 2.4738\n",
      "  Batch 203/210 - Loss: 2.3204\n",
      "  Batch 204/210 - Loss: 2.4726\n",
      "  Batch 205/210 - Loss: 2.4591\n",
      "  Batch 206/210 - Loss: 2.3348\n",
      "  Batch 207/210 - Loss: 2.3771\n",
      "  Batch 208/210 - Loss: 2.3371\n",
      "  Batch 209/210 - Loss: 2.4171\n",
      "  Batch 210/210 - Loss: 2.1190\n",
      "Epoch 3 Completed. Train Loss: 2.9758, Val Loss: 2.1932\n",
      "\n",
      "Epoch 4/13\n",
      "  Batch 1/210 - Loss: 2.4288\n",
      "  Batch 2/210 - Loss: 2.4005\n",
      "  Batch 3/210 - Loss: 2.3407\n",
      "  Batch 4/210 - Loss: 2.3600\n",
      "  Batch 5/210 - Loss: 2.3183\n",
      "  Batch 6/210 - Loss: 2.3824\n",
      "  Batch 7/210 - Loss: 2.2831\n",
      "  Batch 8/210 - Loss: 2.3125\n",
      "  Batch 9/210 - Loss: 2.3967\n",
      "  Batch 10/210 - Loss: 2.3557\n",
      "  Batch 11/210 - Loss: 2.4268\n",
      "  Batch 12/210 - Loss: 2.2804\n",
      "  Batch 13/210 - Loss: 2.3671\n",
      "  Batch 14/210 - Loss: 2.3369\n",
      "  Batch 15/210 - Loss: 2.2378\n",
      "  Batch 16/210 - Loss: 2.2996\n",
      "  Batch 17/210 - Loss: 2.1637\n",
      "  Batch 18/210 - Loss: 2.1663\n",
      "  Batch 19/210 - Loss: 2.2764\n",
      "  Batch 20/210 - Loss: 2.3345\n",
      "  Batch 21/210 - Loss: 2.3031\n",
      "  Batch 22/210 - Loss: 2.3924\n",
      "  Batch 23/210 - Loss: 2.3046\n",
      "  Batch 24/210 - Loss: 2.0653\n",
      "  Batch 25/210 - Loss: 2.2733\n",
      "  Batch 26/210 - Loss: 2.1824\n",
      "  Batch 27/210 - Loss: 2.2593\n",
      "  Batch 28/210 - Loss: 2.3127\n",
      "  Batch 29/210 - Loss: 2.1668\n",
      "  Batch 30/210 - Loss: 2.2765\n",
      "  Batch 31/210 - Loss: 2.2781\n",
      "  Batch 32/210 - Loss: 2.1310\n",
      "  Batch 33/210 - Loss: 2.2452\n",
      "  Batch 34/210 - Loss: 2.2582\n",
      "  Batch 35/210 - Loss: 2.0322\n",
      "  Batch 36/210 - Loss: 2.3069\n",
      "  Batch 37/210 - Loss: 2.1334\n",
      "  Batch 38/210 - Loss: 2.1998\n",
      "  Batch 39/210 - Loss: 2.1907\n",
      "  Batch 40/210 - Loss: 2.1246\n",
      "  Batch 41/210 - Loss: 2.2959\n",
      "  Batch 42/210 - Loss: 2.0578\n",
      "  Batch 43/210 - Loss: 2.1100\n",
      "  Batch 44/210 - Loss: 2.0567\n",
      "  Batch 45/210 - Loss: 2.1645\n",
      "  Batch 46/210 - Loss: 2.0772\n",
      "  Batch 47/210 - Loss: 2.2408\n",
      "  Batch 48/210 - Loss: 2.2622\n",
      "  Batch 49/210 - Loss: 1.9837\n",
      "  Batch 50/210 - Loss: 2.1604\n",
      "  Batch 51/210 - Loss: 2.0572\n",
      "  Batch 52/210 - Loss: 2.0113\n",
      "  Batch 53/210 - Loss: 2.1725\n",
      "  Batch 54/210 - Loss: 2.0212\n",
      "  Batch 55/210 - Loss: 2.1196\n",
      "  Batch 56/210 - Loss: 2.0658\n",
      "  Batch 57/210 - Loss: 2.1844\n",
      "  Batch 58/210 - Loss: 2.0835\n",
      "  Batch 59/210 - Loss: 1.9714\n",
      "  Batch 60/210 - Loss: 2.0092\n",
      "  Batch 61/210 - Loss: 2.0430\n",
      "  Batch 62/210 - Loss: 2.0266\n",
      "  Batch 63/210 - Loss: 2.0948\n",
      "  Batch 64/210 - Loss: 1.9675\n",
      "  Batch 65/210 - Loss: 1.9700\n",
      "  Batch 66/210 - Loss: 1.9551\n",
      "  Batch 67/210 - Loss: 2.0260\n",
      "  Batch 68/210 - Loss: 1.9291\n",
      "  Batch 69/210 - Loss: 1.9541\n",
      "  Batch 70/210 - Loss: 1.9570\n",
      "  Batch 71/210 - Loss: 2.0707\n",
      "  Batch 72/210 - Loss: 1.8246\n",
      "  Batch 73/210 - Loss: 1.9401\n",
      "  Batch 74/210 - Loss: 2.0020\n",
      "  Batch 75/210 - Loss: 1.9284\n",
      "  Batch 76/210 - Loss: 1.9511\n",
      "  Batch 77/210 - Loss: 1.9486\n",
      "  Batch 78/210 - Loss: 1.9787\n",
      "  Batch 79/210 - Loss: 1.9718\n",
      "  Batch 80/210 - Loss: 1.9228\n",
      "  Batch 81/210 - Loss: 2.0866\n",
      "  Batch 82/210 - Loss: 1.8947\n",
      "  Batch 83/210 - Loss: 1.9866\n",
      "  Batch 84/210 - Loss: 1.9249\n",
      "  Batch 85/210 - Loss: 1.9854\n",
      "  Batch 86/210 - Loss: 1.7691\n",
      "  Batch 87/210 - Loss: 1.8603\n",
      "  Batch 88/210 - Loss: 1.9764\n",
      "  Batch 89/210 - Loss: 1.9155\n",
      "  Batch 90/210 - Loss: 1.9387\n",
      "  Batch 91/210 - Loss: 1.8392\n",
      "  Batch 92/210 - Loss: 1.8351\n",
      "  Batch 93/210 - Loss: 1.8246\n",
      "  Batch 94/210 - Loss: 1.9389\n",
      "  Batch 95/210 - Loss: 1.8490\n",
      "  Batch 96/210 - Loss: 1.7938\n",
      "  Batch 97/210 - Loss: 1.8623\n",
      "  Batch 98/210 - Loss: 1.8314\n",
      "  Batch 99/210 - Loss: 1.8891\n",
      "  Batch 100/210 - Loss: 1.8553\n",
      "  Batch 101/210 - Loss: 1.9430\n",
      "  Batch 102/210 - Loss: 1.8713\n",
      "  Batch 103/210 - Loss: 1.9233\n",
      "  Batch 104/210 - Loss: 1.7142\n",
      "  Batch 105/210 - Loss: 1.7754\n",
      "  Batch 106/210 - Loss: 1.8021\n",
      "  Batch 107/210 - Loss: 1.7482\n",
      "  Batch 108/210 - Loss: 1.7985\n",
      "  Batch 109/210 - Loss: 1.7111\n",
      "  Batch 110/210 - Loss: 1.8201\n",
      "  Batch 111/210 - Loss: 1.7789\n",
      "  Batch 112/210 - Loss: 1.7830\n",
      "  Batch 113/210 - Loss: 1.7611\n",
      "  Batch 114/210 - Loss: 1.7334\n",
      "  Batch 115/210 - Loss: 1.6947\n",
      "  Batch 116/210 - Loss: 1.7357\n",
      "  Batch 117/210 - Loss: 1.8549\n",
      "  Batch 118/210 - Loss: 1.7570\n",
      "  Batch 119/210 - Loss: 1.7288\n",
      "  Batch 120/210 - Loss: 1.7486\n",
      "  Batch 121/210 - Loss: 1.6513\n",
      "  Batch 122/210 - Loss: 1.6957\n",
      "  Batch 123/210 - Loss: 1.7306\n",
      "  Batch 124/210 - Loss: 1.5580\n",
      "  Batch 125/210 - Loss: 1.7542\n",
      "  Batch 126/210 - Loss: 1.7120\n",
      "  Batch 127/210 - Loss: 1.7567\n",
      "  Batch 128/210 - Loss: 1.7560\n",
      "  Batch 129/210 - Loss: 1.6270\n",
      "  Batch 130/210 - Loss: 1.6855\n",
      "  Batch 131/210 - Loss: 1.7340\n",
      "  Batch 132/210 - Loss: 1.7012\n",
      "  Batch 133/210 - Loss: 1.7145\n",
      "  Batch 134/210 - Loss: 1.8360\n",
      "  Batch 135/210 - Loss: 1.7514\n",
      "  Batch 136/210 - Loss: 1.7179\n",
      "  Batch 137/210 - Loss: 1.7384\n",
      "  Batch 138/210 - Loss: 1.6653\n",
      "  Batch 139/210 - Loss: 1.6883\n",
      "  Batch 140/210 - Loss: 1.7030\n",
      "  Batch 141/210 - Loss: 1.6237\n",
      "  Batch 142/210 - Loss: 1.7653\n",
      "  Batch 143/210 - Loss: 1.6400\n",
      "  Batch 144/210 - Loss: 1.6921\n",
      "  Batch 145/210 - Loss: 1.5324\n",
      "  Batch 146/210 - Loss: 1.7573\n",
      "  Batch 147/210 - Loss: 1.6544\n",
      "  Batch 148/210 - Loss: 1.5963\n",
      "  Batch 149/210 - Loss: 1.6766\n",
      "  Batch 150/210 - Loss: 1.6671\n",
      "  Batch 151/210 - Loss: 1.5720\n",
      "  Batch 152/210 - Loss: 1.5829\n",
      "  Batch 153/210 - Loss: 1.5248\n",
      "  Batch 154/210 - Loss: 1.6315\n",
      "  Batch 155/210 - Loss: 1.6723\n",
      "  Batch 156/210 - Loss: 1.5880\n",
      "  Batch 157/210 - Loss: 1.5331\n",
      "  Batch 158/210 - Loss: 1.6764\n",
      "  Batch 159/210 - Loss: 1.5598\n",
      "  Batch 160/210 - Loss: 1.6085\n",
      "  Batch 161/210 - Loss: 1.6632\n",
      "  Batch 162/210 - Loss: 1.5134\n",
      "  Batch 163/210 - Loss: 1.5613\n",
      "  Batch 164/210 - Loss: 1.5635\n",
      "  Batch 165/210 - Loss: 1.5800\n",
      "  Batch 166/210 - Loss: 1.6327\n",
      "  Batch 167/210 - Loss: 1.5332\n",
      "  Batch 168/210 - Loss: 1.5434\n",
      "  Batch 169/210 - Loss: 1.6274\n",
      "  Batch 170/210 - Loss: 1.5889\n",
      "  Batch 171/210 - Loss: 1.6165\n",
      "  Batch 172/210 - Loss: 1.5701\n",
      "  Batch 173/210 - Loss: 1.4506\n",
      "  Batch 174/210 - Loss: 1.5869\n",
      "  Batch 175/210 - Loss: 1.4264\n",
      "  Batch 176/210 - Loss: 1.5010\n",
      "  Batch 177/210 - Loss: 1.5041\n",
      "  Batch 178/210 - Loss: 1.5339\n",
      "  Batch 179/210 - Loss: 1.5217\n",
      "  Batch 180/210 - Loss: 1.6087\n",
      "  Batch 181/210 - Loss: 1.6075\n",
      "  Batch 182/210 - Loss: 1.4568\n",
      "  Batch 183/210 - Loss: 1.5429\n",
      "  Batch 184/210 - Loss: 1.4719\n",
      "  Batch 185/210 - Loss: 1.4523\n",
      "  Batch 186/210 - Loss: 1.5654\n",
      "  Batch 187/210 - Loss: 1.5020\n",
      "  Batch 188/210 - Loss: 1.5148\n",
      "  Batch 189/210 - Loss: 1.6054\n",
      "  Batch 190/210 - Loss: 1.4891\n",
      "  Batch 191/210 - Loss: 1.4226\n",
      "  Batch 192/210 - Loss: 1.4739\n",
      "  Batch 193/210 - Loss: 1.6236\n",
      "  Batch 194/210 - Loss: 1.5233\n",
      "  Batch 195/210 - Loss: 1.4014\n",
      "  Batch 196/210 - Loss: 1.5264\n",
      "  Batch 197/210 - Loss: 1.4015\n",
      "  Batch 198/210 - Loss: 1.4526\n",
      "  Batch 199/210 - Loss: 1.4316\n",
      "  Batch 200/210 - Loss: 1.4774\n",
      "  Batch 201/210 - Loss: 1.3730\n",
      "  Batch 202/210 - Loss: 1.3345\n",
      "  Batch 203/210 - Loss: 1.4720\n",
      "  Batch 204/210 - Loss: 1.5561\n",
      "  Batch 205/210 - Loss: 1.3778\n",
      "  Batch 206/210 - Loss: 1.4763\n",
      "  Batch 207/210 - Loss: 1.3587\n",
      "  Batch 208/210 - Loss: 1.4967\n",
      "  Batch 209/210 - Loss: 1.4220\n",
      "  Batch 210/210 - Loss: 1.3796\n",
      "Epoch 4 Completed. Train Loss: 1.8479, Val Loss: 1.1393\n",
      "\n",
      "Epoch 5/13\n",
      "  Batch 1/210 - Loss: 1.3043\n",
      "  Batch 2/210 - Loss: 1.3188\n",
      "  Batch 3/210 - Loss: 1.4018\n",
      "  Batch 4/210 - Loss: 1.4492\n",
      "  Batch 5/210 - Loss: 1.2976\n",
      "  Batch 6/210 - Loss: 1.2700\n",
      "  Batch 7/210 - Loss: 1.2834\n",
      "  Batch 8/210 - Loss: 1.2090\n",
      "  Batch 9/210 - Loss: 1.3087\n",
      "  Batch 10/210 - Loss: 1.2986\n",
      "  Batch 11/210 - Loss: 1.4182\n",
      "  Batch 12/210 - Loss: 1.2585\n",
      "  Batch 13/210 - Loss: 1.3792\n",
      "  Batch 14/210 - Loss: 1.3921\n",
      "  Batch 15/210 - Loss: 1.3826\n",
      "  Batch 16/210 - Loss: 1.3805\n",
      "  Batch 17/210 - Loss: 1.3626\n",
      "  Batch 18/210 - Loss: 1.3700\n",
      "  Batch 19/210 - Loss: 1.2581\n",
      "  Batch 20/210 - Loss: 1.2438\n",
      "  Batch 21/210 - Loss: 1.2459\n",
      "  Batch 22/210 - Loss: 1.1830\n",
      "  Batch 23/210 - Loss: 1.2361\n",
      "  Batch 24/210 - Loss: 1.2733\n",
      "  Batch 25/210 - Loss: 1.2758\n",
      "  Batch 26/210 - Loss: 1.2612\n",
      "  Batch 27/210 - Loss: 1.3148\n",
      "  Batch 28/210 - Loss: 1.2887\n",
      "  Batch 29/210 - Loss: 1.3649\n",
      "  Batch 30/210 - Loss: 1.2648\n",
      "  Batch 31/210 - Loss: 1.1422\n",
      "  Batch 32/210 - Loss: 1.4203\n",
      "  Batch 33/210 - Loss: 1.3281\n",
      "  Batch 34/210 - Loss: 1.2550\n",
      "  Batch 35/210 - Loss: 1.2446\n",
      "  Batch 36/210 - Loss: 1.1904\n",
      "  Batch 37/210 - Loss: 1.1739\n",
      "  Batch 38/210 - Loss: 1.1646\n",
      "  Batch 39/210 - Loss: 1.2178\n",
      "  Batch 40/210 - Loss: 1.2922\n",
      "  Batch 41/210 - Loss: 1.2282\n",
      "  Batch 42/210 - Loss: 1.2254\n",
      "  Batch 43/210 - Loss: 1.1361\n",
      "  Batch 44/210 - Loss: 1.1084\n",
      "  Batch 45/210 - Loss: 1.3216\n",
      "  Batch 46/210 - Loss: 1.2228\n",
      "  Batch 47/210 - Loss: 1.2208\n",
      "  Batch 48/210 - Loss: 1.1999\n",
      "  Batch 49/210 - Loss: 1.1651\n",
      "  Batch 50/210 - Loss: 1.2271\n",
      "  Batch 51/210 - Loss: 1.2249\n",
      "  Batch 52/210 - Loss: 1.2515\n",
      "  Batch 53/210 - Loss: 1.2236\n",
      "  Batch 54/210 - Loss: 1.2439\n",
      "  Batch 55/210 - Loss: 1.1043\n",
      "  Batch 56/210 - Loss: 1.0111\n",
      "  Batch 57/210 - Loss: 1.2042\n",
      "  Batch 58/210 - Loss: 1.1765\n",
      "  Batch 59/210 - Loss: 1.0485\n",
      "  Batch 60/210 - Loss: 1.0702\n",
      "  Batch 61/210 - Loss: 1.0197\n",
      "  Batch 62/210 - Loss: 1.1074\n",
      "  Batch 63/210 - Loss: 1.2113\n",
      "  Batch 64/210 - Loss: 1.1422\n",
      "  Batch 65/210 - Loss: 1.1651\n",
      "  Batch 66/210 - Loss: 1.2299\n",
      "  Batch 67/210 - Loss: 1.0317\n",
      "  Batch 68/210 - Loss: 1.0631\n",
      "  Batch 69/210 - Loss: 1.1490\n",
      "  Batch 70/210 - Loss: 1.1984\n",
      "  Batch 71/210 - Loss: 1.1615\n",
      "  Batch 72/210 - Loss: 1.1999\n",
      "  Batch 73/210 - Loss: 1.0811\n",
      "  Batch 74/210 - Loss: 1.1071\n",
      "  Batch 75/210 - Loss: 1.1570\n",
      "  Batch 76/210 - Loss: 1.1512\n",
      "  Batch 77/210 - Loss: 1.0185\n",
      "  Batch 78/210 - Loss: 1.2009\n",
      "  Batch 79/210 - Loss: 1.0759\n",
      "  Batch 80/210 - Loss: 0.9746\n",
      "  Batch 81/210 - Loss: 1.1083\n",
      "  Batch 82/210 - Loss: 1.1366\n",
      "  Batch 83/210 - Loss: 1.1813\n",
      "  Batch 84/210 - Loss: 1.1217\n",
      "  Batch 85/210 - Loss: 1.1409\n",
      "  Batch 86/210 - Loss: 1.0970\n",
      "  Batch 87/210 - Loss: 1.0564\n",
      "  Batch 88/210 - Loss: 1.0773\n",
      "  Batch 89/210 - Loss: 1.1164\n",
      "  Batch 90/210 - Loss: 1.1409\n",
      "  Batch 91/210 - Loss: 1.0266\n",
      "  Batch 92/210 - Loss: 1.1038\n",
      "  Batch 93/210 - Loss: 1.0249\n",
      "  Batch 94/210 - Loss: 1.1486\n",
      "  Batch 95/210 - Loss: 1.0784\n",
      "  Batch 96/210 - Loss: 1.0182\n",
      "  Batch 97/210 - Loss: 0.9408\n",
      "  Batch 98/210 - Loss: 1.1308\n",
      "  Batch 99/210 - Loss: 1.1205\n",
      "  Batch 100/210 - Loss: 1.1494\n",
      "  Batch 101/210 - Loss: 1.0212\n",
      "  Batch 102/210 - Loss: 1.1023\n",
      "  Batch 103/210 - Loss: 1.0507\n",
      "  Batch 104/210 - Loss: 0.9701\n",
      "  Batch 105/210 - Loss: 1.0473\n",
      "  Batch 106/210 - Loss: 1.0428\n",
      "  Batch 107/210 - Loss: 1.0378\n",
      "  Batch 108/210 - Loss: 1.0234\n",
      "  Batch 109/210 - Loss: 1.0383\n",
      "  Batch 110/210 - Loss: 0.9409\n",
      "  Batch 111/210 - Loss: 0.9260\n",
      "  Batch 112/210 - Loss: 1.0856\n",
      "  Batch 113/210 - Loss: 0.9387\n",
      "  Batch 114/210 - Loss: 1.0522\n",
      "  Batch 115/210 - Loss: 0.9311\n",
      "  Batch 116/210 - Loss: 0.9843\n",
      "  Batch 117/210 - Loss: 0.9760\n",
      "  Batch 118/210 - Loss: 0.9351\n",
      "  Batch 119/210 - Loss: 1.0328\n",
      "  Batch 120/210 - Loss: 0.9412\n",
      "  Batch 121/210 - Loss: 0.9776\n",
      "  Batch 122/210 - Loss: 0.8848\n",
      "  Batch 123/210 - Loss: 0.9500\n",
      "  Batch 124/210 - Loss: 0.9489\n",
      "  Batch 125/210 - Loss: 0.8834\n",
      "  Batch 126/210 - Loss: 0.8588\n",
      "  Batch 127/210 - Loss: 1.0547\n",
      "  Batch 128/210 - Loss: 0.9533\n",
      "  Batch 129/210 - Loss: 0.9777\n",
      "  Batch 130/210 - Loss: 1.0465\n",
      "  Batch 131/210 - Loss: 0.9780\n",
      "  Batch 132/210 - Loss: 0.9739\n",
      "  Batch 133/210 - Loss: 0.9336\n",
      "  Batch 134/210 - Loss: 1.0425\n",
      "  Batch 135/210 - Loss: 0.8928\n",
      "  Batch 136/210 - Loss: 0.8995\n",
      "  Batch 137/210 - Loss: 0.9781\n",
      "  Batch 138/210 - Loss: 0.9250\n",
      "  Batch 139/210 - Loss: 0.9535\n",
      "  Batch 140/210 - Loss: 0.9329\n",
      "  Batch 141/210 - Loss: 0.8995\n",
      "  Batch 142/210 - Loss: 0.9716\n",
      "  Batch 143/210 - Loss: 1.0076\n",
      "  Batch 144/210 - Loss: 0.9430\n",
      "  Batch 145/210 - Loss: 0.8758\n",
      "  Batch 146/210 - Loss: 0.9274\n",
      "  Batch 147/210 - Loss: 0.9753\n",
      "  Batch 148/210 - Loss: 0.9983\n",
      "  Batch 149/210 - Loss: 0.9227\n",
      "  Batch 150/210 - Loss: 0.9293\n",
      "  Batch 151/210 - Loss: 0.9547\n",
      "  Batch 152/210 - Loss: 0.8292\n",
      "  Batch 153/210 - Loss: 0.9593\n",
      "  Batch 154/210 - Loss: 0.9386\n",
      "  Batch 155/210 - Loss: 0.9314\n",
      "  Batch 156/210 - Loss: 0.8297\n",
      "  Batch 157/210 - Loss: 0.9626\n",
      "  Batch 158/210 - Loss: 0.9003\n",
      "  Batch 159/210 - Loss: 0.8966\n",
      "  Batch 160/210 - Loss: 0.8653\n",
      "  Batch 161/210 - Loss: 0.8986\n",
      "  Batch 162/210 - Loss: 0.8512\n",
      "  Batch 163/210 - Loss: 0.8840\n",
      "  Batch 164/210 - Loss: 0.9882\n",
      "  Batch 165/210 - Loss: 0.8442\n",
      "  Batch 166/210 - Loss: 0.7516\n",
      "  Batch 167/210 - Loss: 0.9567\n",
      "  Batch 168/210 - Loss: 0.8626\n",
      "  Batch 169/210 - Loss: 0.8347\n",
      "  Batch 170/210 - Loss: 0.8206\n",
      "  Batch 171/210 - Loss: 0.8972\n",
      "  Batch 172/210 - Loss: 0.8553\n",
      "  Batch 173/210 - Loss: 0.8767\n",
      "  Batch 174/210 - Loss: 0.8790\n",
      "  Batch 175/210 - Loss: 0.8146\n",
      "  Batch 176/210 - Loss: 0.8244\n",
      "  Batch 177/210 - Loss: 0.8149\n",
      "  Batch 178/210 - Loss: 0.8289\n",
      "  Batch 179/210 - Loss: 0.7882\n",
      "  Batch 180/210 - Loss: 0.9425\n",
      "  Batch 181/210 - Loss: 0.7951\n",
      "  Batch 182/210 - Loss: 0.7341\n",
      "  Batch 183/210 - Loss: 0.7735\n",
      "  Batch 184/210 - Loss: 0.9109\n",
      "  Batch 185/210 - Loss: 0.8503\n",
      "  Batch 186/210 - Loss: 0.8142\n",
      "  Batch 187/210 - Loss: 0.7757\n",
      "  Batch 188/210 - Loss: 0.8089\n",
      "  Batch 189/210 - Loss: 0.7948\n",
      "  Batch 190/210 - Loss: 0.7918\n",
      "  Batch 191/210 - Loss: 0.7126\n",
      "  Batch 192/210 - Loss: 0.7884\n",
      "  Batch 193/210 - Loss: 0.7778\n",
      "  Batch 194/210 - Loss: 0.7901\n",
      "  Batch 195/210 - Loss: 0.7756\n",
      "  Batch 196/210 - Loss: 0.7932\n",
      "  Batch 197/210 - Loss: 0.8301\n",
      "  Batch 198/210 - Loss: 0.8264\n",
      "  Batch 199/210 - Loss: 0.7138\n",
      "  Batch 200/210 - Loss: 0.8024\n",
      "  Batch 201/210 - Loss: 0.7076\n",
      "  Batch 202/210 - Loss: 0.7947\n",
      "  Batch 203/210 - Loss: 0.7888\n",
      "  Batch 204/210 - Loss: 0.7174\n",
      "  Batch 205/210 - Loss: 0.7915\n",
      "  Batch 206/210 - Loss: 0.7483\n",
      "  Batch 207/210 - Loss: 0.7787\n",
      "  Batch 208/210 - Loss: 0.6954\n",
      "  Batch 209/210 - Loss: 0.7783\n",
      "  Batch 210/210 - Loss: 0.7626\n",
      "Epoch 5 Completed. Train Loss: 1.0380, Val Loss: 0.5459\n",
      "\n",
      "Epoch 6/13\n",
      "  Batch 1/210 - Loss: 0.7600\n",
      "  Batch 2/210 - Loss: 0.7626\n",
      "  Batch 3/210 - Loss: 0.7569\n",
      "  Batch 4/210 - Loss: 0.7401\n",
      "  Batch 5/210 - Loss: 0.7487\n",
      "  Batch 6/210 - Loss: 0.7661\n",
      "  Batch 7/210 - Loss: 0.6848\n",
      "  Batch 8/210 - Loss: 0.5926\n",
      "  Batch 9/210 - Loss: 0.7548\n",
      "  Batch 10/210 - Loss: 0.7420\n",
      "  Batch 11/210 - Loss: 0.7025\n",
      "  Batch 12/210 - Loss: 0.7659\n",
      "  Batch 13/210 - Loss: 0.7662\n",
      "  Batch 14/210 - Loss: 0.7579\n",
      "  Batch 15/210 - Loss: 0.6627\n",
      "  Batch 16/210 - Loss: 0.6874\n",
      "  Batch 17/210 - Loss: 0.6133\n",
      "  Batch 18/210 - Loss: 0.6826\n",
      "  Batch 19/210 - Loss: 0.7636\n",
      "  Batch 20/210 - Loss: 0.7310\n",
      "  Batch 21/210 - Loss: 0.6783\n",
      "  Batch 22/210 - Loss: 0.6799\n",
      "  Batch 23/210 - Loss: 0.6799\n",
      "  Batch 24/210 - Loss: 0.6454\n",
      "  Batch 25/210 - Loss: 0.7118\n",
      "  Batch 26/210 - Loss: 0.7564\n",
      "  Batch 27/210 - Loss: 0.7218\n",
      "  Batch 28/210 - Loss: 0.6651\n",
      "  Batch 29/210 - Loss: 0.7234\n",
      "  Batch 30/210 - Loss: 0.6135\n",
      "  Batch 31/210 - Loss: 0.7352\n",
      "  Batch 32/210 - Loss: 0.6824\n",
      "  Batch 33/210 - Loss: 0.6147\n",
      "  Batch 34/210 - Loss: 0.6323\n",
      "  Batch 35/210 - Loss: 0.6726\n",
      "  Batch 36/210 - Loss: 0.5767\n",
      "  Batch 37/210 - Loss: 0.6454\n",
      "  Batch 38/210 - Loss: 0.6927\n",
      "  Batch 39/210 - Loss: 0.6281\n",
      "  Batch 40/210 - Loss: 0.6548\n",
      "  Batch 41/210 - Loss: 0.6001\n",
      "  Batch 42/210 - Loss: 0.6954\n",
      "  Batch 43/210 - Loss: 0.6457\n",
      "  Batch 44/210 - Loss: 0.6773\n",
      "  Batch 45/210 - Loss: 0.6888\n",
      "  Batch 46/210 - Loss: 0.6722\n",
      "  Batch 47/210 - Loss: 0.6797\n",
      "  Batch 48/210 - Loss: 0.7296\n",
      "  Batch 49/210 - Loss: 0.6202\n",
      "  Batch 50/210 - Loss: 0.5947\n",
      "  Batch 51/210 - Loss: 0.6542\n",
      "  Batch 52/210 - Loss: 0.6845\n",
      "  Batch 53/210 - Loss: 0.7146\n",
      "  Batch 54/210 - Loss: 0.6352\n",
      "  Batch 55/210 - Loss: 0.5968\n",
      "  Batch 56/210 - Loss: 0.5708\n",
      "  Batch 57/210 - Loss: 0.6660\n",
      "  Batch 58/210 - Loss: 0.6570\n",
      "  Batch 59/210 - Loss: 0.5705\n",
      "  Batch 60/210 - Loss: 0.6655\n",
      "  Batch 61/210 - Loss: 0.5931\n",
      "  Batch 62/210 - Loss: 0.6304\n",
      "  Batch 63/210 - Loss: 0.6139\n",
      "  Batch 64/210 - Loss: 0.5394\n",
      "  Batch 65/210 - Loss: 0.6419\n",
      "  Batch 66/210 - Loss: 0.5784\n",
      "  Batch 67/210 - Loss: 0.6175\n",
      "  Batch 68/210 - Loss: 0.6469\n",
      "  Batch 69/210 - Loss: 0.5967\n",
      "  Batch 70/210 - Loss: 0.6173\n",
      "  Batch 71/210 - Loss: 0.6434\n",
      "  Batch 72/210 - Loss: 0.6156\n",
      "  Batch 73/210 - Loss: 0.6525\n",
      "  Batch 74/210 - Loss: 0.6098\n",
      "  Batch 75/210 - Loss: 0.6455\n",
      "  Batch 76/210 - Loss: 0.6002\n",
      "  Batch 77/210 - Loss: 0.6532\n",
      "  Batch 78/210 - Loss: 0.5703\n",
      "  Batch 79/210 - Loss: 0.6560\n",
      "  Batch 80/210 - Loss: 0.6009\n",
      "  Batch 81/210 - Loss: 0.5991\n",
      "  Batch 82/210 - Loss: 0.5912\n",
      "  Batch 83/210 - Loss: 0.6092\n",
      "  Batch 84/210 - Loss: 0.5812\n",
      "  Batch 85/210 - Loss: 0.5488\n",
      "  Batch 86/210 - Loss: 0.5800\n",
      "  Batch 87/210 - Loss: 0.5936\n",
      "  Batch 88/210 - Loss: 0.6028\n",
      "  Batch 89/210 - Loss: 0.5899\n",
      "  Batch 90/210 - Loss: 0.6003\n",
      "  Batch 91/210 - Loss: 0.6172\n",
      "  Batch 92/210 - Loss: 0.6007\n",
      "  Batch 93/210 - Loss: 0.5762\n",
      "  Batch 94/210 - Loss: 0.5762\n",
      "  Batch 95/210 - Loss: 0.6410\n",
      "  Batch 96/210 - Loss: 0.5283\n",
      "  Batch 97/210 - Loss: 0.5637\n",
      "  Batch 98/210 - Loss: 0.6202\n",
      "  Batch 99/210 - Loss: 0.6198\n",
      "  Batch 100/210 - Loss: 0.6566\n",
      "  Batch 101/210 - Loss: 0.6344\n",
      "  Batch 102/210 - Loss: 0.5965\n",
      "  Batch 103/210 - Loss: 0.5350\n",
      "  Batch 104/210 - Loss: 0.5085\n",
      "  Batch 105/210 - Loss: 0.5766\n",
      "  Batch 106/210 - Loss: 0.5905\n",
      "  Batch 107/210 - Loss: 0.5490\n",
      "  Batch 108/210 - Loss: 0.5035\n",
      "  Batch 109/210 - Loss: 0.4954\n",
      "  Batch 110/210 - Loss: 0.5680\n",
      "  Batch 111/210 - Loss: 0.5703\n",
      "  Batch 112/210 - Loss: 0.5006\n",
      "  Batch 113/210 - Loss: 0.6062\n",
      "  Batch 114/210 - Loss: 0.5454\n",
      "  Batch 115/210 - Loss: 0.5507\n",
      "  Batch 116/210 - Loss: 0.6101\n",
      "  Batch 117/210 - Loss: 0.5371\n",
      "  Batch 118/210 - Loss: 0.5440\n",
      "  Batch 119/210 - Loss: 0.5027\n",
      "  Batch 120/210 - Loss: 0.5872\n",
      "  Batch 121/210 - Loss: 0.5203\n",
      "  Batch 122/210 - Loss: 0.5257\n",
      "  Batch 123/210 - Loss: 0.5345\n",
      "  Batch 124/210 - Loss: 0.5134\n",
      "  Batch 125/210 - Loss: 0.5508\n",
      "  Batch 126/210 - Loss: 0.5781\n",
      "  Batch 127/210 - Loss: 0.5270\n",
      "  Batch 128/210 - Loss: 0.5175\n",
      "  Batch 129/210 - Loss: 0.5064\n",
      "  Batch 130/210 - Loss: 0.5758\n",
      "  Batch 131/210 - Loss: 0.4861\n",
      "  Batch 132/210 - Loss: 0.5119\n",
      "  Batch 133/210 - Loss: 0.3991\n",
      "  Batch 134/210 - Loss: 0.5309\n",
      "  Batch 135/210 - Loss: 0.4828\n",
      "  Batch 136/210 - Loss: 0.4932\n",
      "  Batch 137/210 - Loss: 0.5600\n",
      "  Batch 138/210 - Loss: 0.5323\n",
      "  Batch 139/210 - Loss: 0.5066\n",
      "  Batch 140/210 - Loss: 0.5287\n",
      "  Batch 141/210 - Loss: 0.4329\n",
      "  Batch 142/210 - Loss: 0.5283\n",
      "  Batch 143/210 - Loss: 0.4743\n",
      "  Batch 144/210 - Loss: 0.5018\n",
      "  Batch 145/210 - Loss: 0.4745\n",
      "  Batch 146/210 - Loss: 0.4849\n",
      "  Batch 147/210 - Loss: 0.4290\n",
      "  Batch 148/210 - Loss: 0.4766\n",
      "  Batch 149/210 - Loss: 0.4909\n",
      "  Batch 150/210 - Loss: 0.5198\n",
      "  Batch 151/210 - Loss: 0.4589\n",
      "  Batch 152/210 - Loss: 0.5156\n",
      "  Batch 153/210 - Loss: 0.5332\n",
      "  Batch 154/210 - Loss: 0.4643\n",
      "  Batch 155/210 - Loss: 0.5199\n",
      "  Batch 156/210 - Loss: 0.5224\n",
      "  Batch 157/210 - Loss: 0.4543\n",
      "  Batch 158/210 - Loss: 0.4802\n",
      "  Batch 159/210 - Loss: 0.5949\n",
      "  Batch 160/210 - Loss: 0.5475\n",
      "  Batch 161/210 - Loss: 0.4964\n",
      "  Batch 162/210 - Loss: 0.5118\n",
      "  Batch 163/210 - Loss: 0.5310\n",
      "  Batch 164/210 - Loss: 0.4955\n",
      "  Batch 165/210 - Loss: 0.4880\n",
      "  Batch 166/210 - Loss: 0.4493\n",
      "  Batch 167/210 - Loss: 0.5172\n",
      "  Batch 168/210 - Loss: 0.5192\n",
      "  Batch 169/210 - Loss: 0.4987\n",
      "  Batch 170/210 - Loss: 0.4659\n",
      "  Batch 171/210 - Loss: 0.6227\n",
      "  Batch 172/210 - Loss: 0.5038\n",
      "  Batch 173/210 - Loss: 0.5481\n",
      "  Batch 174/210 - Loss: 0.4579\n",
      "  Batch 175/210 - Loss: 0.4301\n",
      "  Batch 176/210 - Loss: 0.4748\n",
      "  Batch 177/210 - Loss: 0.4317\n",
      "  Batch 178/210 - Loss: 0.5012\n",
      "  Batch 179/210 - Loss: 0.4423\n",
      "  Batch 180/210 - Loss: 0.4703\n",
      "  Batch 181/210 - Loss: 0.4506\n",
      "  Batch 182/210 - Loss: 0.5179\n",
      "  Batch 183/210 - Loss: 0.3829\n",
      "  Batch 184/210 - Loss: 0.4385\n",
      "  Batch 185/210 - Loss: 0.4066\n",
      "  Batch 186/210 - Loss: 0.4789\n",
      "  Batch 187/210 - Loss: 0.4903\n",
      "  Batch 188/210 - Loss: 0.4381\n",
      "  Batch 189/210 - Loss: 0.4807\n",
      "  Batch 190/210 - Loss: 0.4088\n",
      "  Batch 191/210 - Loss: 0.4357\n",
      "  Batch 192/210 - Loss: 0.4375\n",
      "  Batch 193/210 - Loss: 0.4516\n",
      "  Batch 194/210 - Loss: 0.4431\n",
      "  Batch 195/210 - Loss: 0.4385\n",
      "  Batch 196/210 - Loss: 0.4777\n",
      "  Batch 197/210 - Loss: 0.4749\n",
      "  Batch 198/210 - Loss: 0.4344\n",
      "  Batch 199/210 - Loss: 0.4297\n",
      "  Batch 200/210 - Loss: 0.3770\n",
      "  Batch 201/210 - Loss: 0.4862\n",
      "  Batch 202/210 - Loss: 0.4965\n",
      "  Batch 203/210 - Loss: 0.4689\n",
      "  Batch 204/210 - Loss: 0.4604\n",
      "  Batch 205/210 - Loss: 0.3752\n",
      "  Batch 206/210 - Loss: 0.4095\n",
      "  Batch 207/210 - Loss: 0.3980\n",
      "  Batch 208/210 - Loss: 0.4607\n",
      "  Batch 209/210 - Loss: 0.4343\n",
      "  Batch 210/210 - Loss: 0.3798\n",
      "Epoch 6 Completed. Train Loss: 0.5695, Val Loss: 0.2771\n",
      "\n",
      "Epoch 7/13\n",
      "  Batch 1/210 - Loss: 0.4107\n",
      "  Batch 2/210 - Loss: 0.3805\n",
      "  Batch 3/210 - Loss: 0.3598\n",
      "  Batch 4/210 - Loss: 0.4009\n",
      "  Batch 5/210 - Loss: 0.4079\n",
      "  Batch 6/210 - Loss: 0.3576\n",
      "  Batch 7/210 - Loss: 0.4154\n",
      "  Batch 8/210 - Loss: 0.4550\n",
      "  Batch 9/210 - Loss: 0.4078\n",
      "  Batch 10/210 - Loss: 0.4210\n",
      "  Batch 11/210 - Loss: 0.4091\n",
      "  Batch 12/210 - Loss: 0.3769\n",
      "  Batch 13/210 - Loss: 0.3739\n",
      "  Batch 14/210 - Loss: 0.4312\n",
      "  Batch 15/210 - Loss: 0.4079\n",
      "  Batch 16/210 - Loss: 0.4010\n",
      "  Batch 17/210 - Loss: 0.3998\n",
      "  Batch 18/210 - Loss: 0.4109\n",
      "  Batch 19/210 - Loss: 0.4174\n",
      "  Batch 20/210 - Loss: 0.4198\n",
      "  Batch 21/210 - Loss: 0.3735\n",
      "  Batch 22/210 - Loss: 0.3708\n",
      "  Batch 23/210 - Loss: 0.3680\n",
      "  Batch 24/210 - Loss: 0.3989\n",
      "  Batch 25/210 - Loss: 0.4340\n",
      "  Batch 26/210 - Loss: 0.3556\n",
      "  Batch 27/210 - Loss: 0.3633\n",
      "  Batch 28/210 - Loss: 0.3698\n",
      "  Batch 29/210 - Loss: 0.4176\n",
      "  Batch 30/210 - Loss: 0.3493\n",
      "  Batch 31/210 - Loss: 0.3575\n",
      "  Batch 32/210 - Loss: 0.3228\n",
      "  Batch 33/210 - Loss: 0.3711\n",
      "  Batch 34/210 - Loss: 0.3540\n",
      "  Batch 35/210 - Loss: 0.4018\n",
      "  Batch 36/210 - Loss: 0.4113\n",
      "  Batch 37/210 - Loss: 0.3365\n",
      "  Batch 38/210 - Loss: 0.3710\n",
      "  Batch 39/210 - Loss: 0.3161\n",
      "  Batch 40/210 - Loss: 0.3841\n",
      "  Batch 41/210 - Loss: 0.3374\n",
      "  Batch 42/210 - Loss: 0.3698\n",
      "  Batch 43/210 - Loss: 0.3777\n",
      "  Batch 44/210 - Loss: 0.4405\n",
      "  Batch 45/210 - Loss: 0.3704\n",
      "  Batch 46/210 - Loss: 0.3292\n",
      "  Batch 47/210 - Loss: 0.3962\n",
      "  Batch 48/210 - Loss: 0.3528\n",
      "  Batch 49/210 - Loss: 0.3525\n",
      "  Batch 50/210 - Loss: 0.3837\n",
      "  Batch 51/210 - Loss: 0.3220\n",
      "  Batch 52/210 - Loss: 0.3764\n",
      "  Batch 53/210 - Loss: 0.3586\n",
      "  Batch 54/210 - Loss: 0.3595\n",
      "  Batch 55/210 - Loss: 0.3593\n",
      "  Batch 56/210 - Loss: 0.3643\n",
      "  Batch 57/210 - Loss: 0.3505\n",
      "  Batch 58/210 - Loss: 0.3310\n",
      "  Batch 59/210 - Loss: 0.3873\n",
      "  Batch 60/210 - Loss: 0.3538\n",
      "  Batch 61/210 - Loss: 0.3690\n",
      "  Batch 62/210 - Loss: 0.4024\n",
      "  Batch 63/210 - Loss: 0.3487\n",
      "  Batch 64/210 - Loss: 0.3298\n",
      "  Batch 65/210 - Loss: 0.3989\n",
      "  Batch 66/210 - Loss: 0.3469\n",
      "  Batch 67/210 - Loss: 0.3314\n",
      "  Batch 68/210 - Loss: 0.3604\n",
      "  Batch 69/210 - Loss: 0.3560\n",
      "  Batch 70/210 - Loss: 0.3043\n",
      "  Batch 71/210 - Loss: 0.3216\n",
      "  Batch 72/210 - Loss: 0.3282\n",
      "  Batch 73/210 - Loss: 0.3675\n",
      "  Batch 74/210 - Loss: 0.3713\n",
      "  Batch 75/210 - Loss: 0.3404\n",
      "  Batch 76/210 - Loss: 0.3006\n",
      "  Batch 77/210 - Loss: 0.3758\n",
      "  Batch 78/210 - Loss: 0.3825\n",
      "  Batch 79/210 - Loss: 0.3069\n",
      "  Batch 80/210 - Loss: 0.3404\n",
      "  Batch 81/210 - Loss: 0.3490\n",
      "  Batch 82/210 - Loss: 0.2963\n",
      "  Batch 83/210 - Loss: 0.3450\n",
      "  Batch 84/210 - Loss: 0.3533\n",
      "  Batch 85/210 - Loss: 0.3675\n",
      "  Batch 86/210 - Loss: 0.3031\n",
      "  Batch 87/210 - Loss: 0.3263\n",
      "  Batch 88/210 - Loss: 0.2855\n",
      "  Batch 89/210 - Loss: 0.3208\n",
      "  Batch 90/210 - Loss: 0.3137\n",
      "  Batch 91/210 - Loss: 0.3318\n",
      "  Batch 92/210 - Loss: 0.3256\n",
      "  Batch 93/210 - Loss: 0.3228\n",
      "  Batch 94/210 - Loss: 0.3491\n",
      "  Batch 95/210 - Loss: 0.2938\n",
      "  Batch 96/210 - Loss: 0.3465\n",
      "  Batch 97/210 - Loss: 0.3442\n",
      "  Batch 98/210 - Loss: 0.2858\n",
      "  Batch 99/210 - Loss: 0.3294\n",
      "  Batch 100/210 - Loss: 0.2912\n",
      "  Batch 101/210 - Loss: 0.3218\n",
      "  Batch 102/210 - Loss: 0.3220\n",
      "  Batch 103/210 - Loss: 0.3013\n",
      "  Batch 104/210 - Loss: 0.3226\n",
      "  Batch 105/210 - Loss: 0.3630\n",
      "  Batch 106/210 - Loss: 0.3046\n",
      "  Batch 107/210 - Loss: 0.3571\n",
      "  Batch 108/210 - Loss: 0.3413\n",
      "  Batch 109/210 - Loss: 0.3058\n",
      "  Batch 110/210 - Loss: 0.3045\n",
      "  Batch 111/210 - Loss: 0.3044\n",
      "  Batch 112/210 - Loss: 0.3336\n",
      "  Batch 113/210 - Loss: 0.2978\n",
      "  Batch 114/210 - Loss: 0.3026\n",
      "  Batch 115/210 - Loss: 0.2611\n",
      "  Batch 116/210 - Loss: 0.3061\n",
      "  Batch 117/210 - Loss: 0.3387\n",
      "  Batch 118/210 - Loss: 0.2743\n",
      "  Batch 119/210 - Loss: 0.3331\n",
      "  Batch 120/210 - Loss: 0.3222\n",
      "  Batch 121/210 - Loss: 0.2984\n",
      "  Batch 122/210 - Loss: 0.2977\n",
      "  Batch 123/210 - Loss: 0.2978\n",
      "  Batch 124/210 - Loss: 0.2781\n",
      "  Batch 125/210 - Loss: 0.3874\n",
      "  Batch 126/210 - Loss: 0.2899\n",
      "  Batch 127/210 - Loss: 0.2600\n",
      "  Batch 128/210 - Loss: 0.2717\n",
      "  Batch 129/210 - Loss: 0.3033\n",
      "  Batch 130/210 - Loss: 0.2635\n",
      "  Batch 131/210 - Loss: 0.2709\n",
      "  Batch 132/210 - Loss: 0.3665\n",
      "  Batch 133/210 - Loss: 0.3017\n",
      "  Batch 134/210 - Loss: 0.3333\n",
      "  Batch 135/210 - Loss: 0.2480\n",
      "  Batch 136/210 - Loss: 0.3175\n",
      "  Batch 137/210 - Loss: 0.2916\n",
      "  Batch 138/210 - Loss: 0.2966\n",
      "  Batch 139/210 - Loss: 0.2759\n",
      "  Batch 140/210 - Loss: 0.2562\n",
      "  Batch 141/210 - Loss: 0.2800\n",
      "  Batch 142/210 - Loss: 0.2595\n",
      "  Batch 143/210 - Loss: 0.2760\n",
      "  Batch 144/210 - Loss: 0.2871\n",
      "  Batch 145/210 - Loss: 0.2599\n",
      "  Batch 146/210 - Loss: 0.2678\n",
      "  Batch 147/210 - Loss: 0.2504\n",
      "  Batch 148/210 - Loss: 0.2745\n",
      "  Batch 149/210 - Loss: 0.3147\n",
      "  Batch 150/210 - Loss: 0.2960\n",
      "  Batch 151/210 - Loss: 0.2678\n",
      "  Batch 152/210 - Loss: 0.2949\n",
      "  Batch 153/210 - Loss: 0.3174\n",
      "  Batch 154/210 - Loss: 0.3483\n",
      "  Batch 155/210 - Loss: 0.2605\n",
      "  Batch 156/210 - Loss: 0.2915\n",
      "  Batch 157/210 - Loss: 0.3000\n",
      "  Batch 158/210 - Loss: 0.2955\n",
      "  Batch 159/210 - Loss: 0.2767\n",
      "  Batch 160/210 - Loss: 0.2645\n",
      "  Batch 161/210 - Loss: 0.2864\n",
      "  Batch 162/210 - Loss: 0.2790\n",
      "  Batch 163/210 - Loss: 0.2797\n",
      "  Batch 164/210 - Loss: 0.3010\n",
      "  Batch 165/210 - Loss: 0.2766\n",
      "  Batch 166/210 - Loss: 0.2195\n",
      "  Batch 167/210 - Loss: 0.2712\n",
      "  Batch 168/210 - Loss: 0.2759\n",
      "  Batch 169/210 - Loss: 0.3068\n",
      "  Batch 170/210 - Loss: 0.2683\n",
      "  Batch 171/210 - Loss: 0.2755\n",
      "  Batch 172/210 - Loss: 0.2684\n",
      "  Batch 173/210 - Loss: 0.3186\n",
      "  Batch 174/210 - Loss: 0.2952\n",
      "  Batch 175/210 - Loss: 0.2852\n",
      "  Batch 176/210 - Loss: 0.2427\n",
      "  Batch 177/210 - Loss: 0.2951\n",
      "  Batch 178/210 - Loss: 0.2613\n",
      "  Batch 179/210 - Loss: 0.2705\n",
      "  Batch 180/210 - Loss: 0.2371\n",
      "  Batch 181/210 - Loss: 0.2482\n",
      "  Batch 182/210 - Loss: 0.2931\n",
      "  Batch 183/210 - Loss: 0.2561\n",
      "  Batch 184/210 - Loss: 0.2114\n",
      "  Batch 185/210 - Loss: 0.2644\n",
      "  Batch 186/210 - Loss: 0.2341\n",
      "  Batch 187/210 - Loss: 0.2240\n",
      "  Batch 188/210 - Loss: 0.2262\n",
      "  Batch 189/210 - Loss: 0.2431\n",
      "  Batch 190/210 - Loss: 0.2273\n",
      "  Batch 191/210 - Loss: 0.2616\n",
      "  Batch 192/210 - Loss: 0.2685\n",
      "  Batch 193/210 - Loss: 0.2087\n",
      "  Batch 194/210 - Loss: 0.2201\n",
      "  Batch 195/210 - Loss: 0.2455\n",
      "  Batch 196/210 - Loss: 0.2176\n",
      "  Batch 197/210 - Loss: 0.2257\n",
      "  Batch 198/210 - Loss: 0.2370\n",
      "  Batch 199/210 - Loss: 0.2856\n",
      "  Batch 200/210 - Loss: 0.2120\n",
      "  Batch 201/210 - Loss: 0.2374\n",
      "  Batch 202/210 - Loss: 0.2407\n",
      "  Batch 203/210 - Loss: 0.2318\n",
      "  Batch 204/210 - Loss: 0.2095\n",
      "  Batch 205/210 - Loss: 0.2980\n",
      "  Batch 206/210 - Loss: 0.2570\n",
      "  Batch 207/210 - Loss: 0.2594\n",
      "  Batch 208/210 - Loss: 0.2598\n",
      "  Batch 209/210 - Loss: 0.2456\n",
      "  Batch 210/210 - Loss: 0.2537\n",
      "Epoch 7 Completed. Train Loss: 0.3188, Val Loss: 0.1401\n",
      "\n",
      "Epoch 8/13\n",
      "  Batch 1/210 - Loss: 0.2096\n",
      "  Batch 2/210 - Loss: 0.2175\n",
      "  Batch 3/210 - Loss: 0.2323\n",
      "  Batch 4/210 - Loss: 0.2149\n",
      "  Batch 5/210 - Loss: 0.2403\n",
      "  Batch 6/210 - Loss: 0.2458\n",
      "  Batch 7/210 - Loss: 0.2070\n",
      "  Batch 8/210 - Loss: 0.2486\n",
      "  Batch 9/210 - Loss: 0.2590\n",
      "  Batch 10/210 - Loss: 0.2122\n",
      "  Batch 11/210 - Loss: 0.2042\n",
      "  Batch 12/210 - Loss: 0.2242\n",
      "  Batch 13/210 - Loss: 0.2158\n",
      "  Batch 14/210 - Loss: 0.2537\n",
      "  Batch 15/210 - Loss: 0.2371\n",
      "  Batch 16/210 - Loss: 0.1959\n",
      "  Batch 17/210 - Loss: 0.2562\n",
      "  Batch 18/210 - Loss: 0.2378\n",
      "  Batch 19/210 - Loss: 0.2806\n",
      "  Batch 20/210 - Loss: 0.2173\n",
      "  Batch 21/210 - Loss: 0.2040\n",
      "  Batch 22/210 - Loss: 0.2403\n",
      "  Batch 23/210 - Loss: 0.2481\n",
      "  Batch 24/210 - Loss: 0.2217\n",
      "  Batch 25/210 - Loss: 0.2004\n",
      "  Batch 26/210 - Loss: 0.2113\n",
      "  Batch 27/210 - Loss: 0.2483\n",
      "  Batch 28/210 - Loss: 0.2378\n",
      "  Batch 29/210 - Loss: 0.2214\n",
      "  Batch 30/210 - Loss: 0.2001\n",
      "  Batch 31/210 - Loss: 0.2029\n",
      "  Batch 32/210 - Loss: 0.2356\n",
      "  Batch 33/210 - Loss: 0.1719\n",
      "  Batch 34/210 - Loss: 0.1849\n",
      "  Batch 35/210 - Loss: 0.1885\n",
      "  Batch 36/210 - Loss: 0.1904\n",
      "  Batch 37/210 - Loss: 0.2216\n",
      "  Batch 38/210 - Loss: 0.2106\n",
      "  Batch 39/210 - Loss: 0.2284\n",
      "  Batch 40/210 - Loss: 0.2032\n",
      "  Batch 41/210 - Loss: 0.1853\n",
      "  Batch 42/210 - Loss: 0.2207\n",
      "  Batch 43/210 - Loss: 0.2178\n",
      "  Batch 44/210 - Loss: 0.2025\n",
      "  Batch 45/210 - Loss: 0.1949\n",
      "  Batch 46/210 - Loss: 0.2072\n",
      "  Batch 47/210 - Loss: 0.1802\n",
      "  Batch 48/210 - Loss: 0.1812\n",
      "  Batch 49/210 - Loss: 0.2109\n",
      "  Batch 50/210 - Loss: 0.2119\n",
      "  Batch 51/210 - Loss: 0.2221\n",
      "  Batch 52/210 - Loss: 0.1622\n",
      "  Batch 53/210 - Loss: 0.2283\n",
      "  Batch 54/210 - Loss: 0.1944\n",
      "  Batch 55/210 - Loss: 0.2074\n",
      "  Batch 56/210 - Loss: 0.1960\n",
      "  Batch 57/210 - Loss: 0.1990\n",
      "  Batch 58/210 - Loss: 0.2138\n",
      "  Batch 59/210 - Loss: 0.2121\n",
      "  Batch 60/210 - Loss: 0.1865\n",
      "  Batch 61/210 - Loss: 0.1973\n",
      "  Batch 62/210 - Loss: 0.1879\n",
      "  Batch 63/210 - Loss: 0.2155\n",
      "  Batch 64/210 - Loss: 0.1879\n",
      "  Batch 65/210 - Loss: 0.1755\n",
      "  Batch 66/210 - Loss: 0.2272\n",
      "  Batch 67/210 - Loss: 0.1712\n",
      "  Batch 68/210 - Loss: 0.1924\n",
      "  Batch 69/210 - Loss: 0.2195\n",
      "  Batch 70/210 - Loss: 0.2083\n",
      "  Batch 71/210 - Loss: 0.2058\n",
      "  Batch 72/210 - Loss: 0.2312\n",
      "  Batch 73/210 - Loss: 0.2193\n",
      "  Batch 74/210 - Loss: 0.1981\n",
      "  Batch 75/210 - Loss: 0.1893\n",
      "  Batch 76/210 - Loss: 0.1900\n",
      "  Batch 77/210 - Loss: 0.1974\n",
      "  Batch 78/210 - Loss: 0.1873\n",
      "  Batch 79/210 - Loss: 0.1940\n",
      "  Batch 80/210 - Loss: 0.1827\n",
      "  Batch 81/210 - Loss: 0.1851\n",
      "  Batch 82/210 - Loss: 0.2425\n",
      "  Batch 83/210 - Loss: 0.1940\n",
      "  Batch 84/210 - Loss: 0.1923\n",
      "  Batch 85/210 - Loss: 0.1989\n",
      "  Batch 86/210 - Loss: 0.1791\n",
      "  Batch 87/210 - Loss: 0.2222\n",
      "  Batch 88/210 - Loss: 0.2099\n",
      "  Batch 89/210 - Loss: 0.1460\n",
      "  Batch 90/210 - Loss: 0.1711\n",
      "  Batch 91/210 - Loss: 0.1679\n",
      "  Batch 92/210 - Loss: 0.1829\n",
      "  Batch 93/210 - Loss: 0.1660\n",
      "  Batch 94/210 - Loss: 0.2026\n",
      "  Batch 95/210 - Loss: 0.2066\n",
      "  Batch 96/210 - Loss: 0.1948\n",
      "  Batch 97/210 - Loss: 0.1621\n",
      "  Batch 98/210 - Loss: 0.1813\n",
      "  Batch 99/210 - Loss: 0.1738\n",
      "  Batch 100/210 - Loss: 0.1796\n",
      "  Batch 101/210 - Loss: 0.1892\n",
      "  Batch 102/210 - Loss: 0.1622\n",
      "  Batch 103/210 - Loss: 0.1422\n",
      "  Batch 104/210 - Loss: 0.1859\n",
      "  Batch 105/210 - Loss: 0.1736\n",
      "  Batch 106/210 - Loss: 0.2037\n",
      "  Batch 107/210 - Loss: 0.1772\n",
      "  Batch 108/210 - Loss: 0.1749\n",
      "  Batch 109/210 - Loss: 0.1808\n",
      "  Batch 110/210 - Loss: 0.1714\n",
      "  Batch 111/210 - Loss: 0.1819\n",
      "  Batch 112/210 - Loss: 0.1601\n",
      "  Batch 113/210 - Loss: 0.1398\n",
      "  Batch 114/210 - Loss: 0.1779\n",
      "  Batch 115/210 - Loss: 0.1641\n",
      "  Batch 116/210 - Loss: 0.1766\n",
      "  Batch 117/210 - Loss: 0.2010\n",
      "  Batch 118/210 - Loss: 0.1907\n",
      "  Batch 119/210 - Loss: 0.1891\n",
      "  Batch 120/210 - Loss: 0.1714\n",
      "  Batch 121/210 - Loss: 0.2422\n",
      "  Batch 122/210 - Loss: 0.1893\n",
      "  Batch 123/210 - Loss: 0.1811\n",
      "  Batch 124/210 - Loss: 0.1616\n",
      "  Batch 125/210 - Loss: 0.1667\n",
      "  Batch 126/210 - Loss: 0.1454\n",
      "  Batch 127/210 - Loss: 0.1745\n",
      "  Batch 128/210 - Loss: 0.1631\n",
      "  Batch 129/210 - Loss: 0.1603\n",
      "  Batch 130/210 - Loss: 0.1837\n",
      "  Batch 131/210 - Loss: 0.1935\n",
      "  Batch 132/210 - Loss: 0.1602\n",
      "  Batch 133/210 - Loss: 0.1915\n",
      "  Batch 134/210 - Loss: 0.1869\n",
      "  Batch 135/210 - Loss: 0.1685\n",
      "  Batch 136/210 - Loss: 0.1577\n",
      "  Batch 137/210 - Loss: 0.1593\n",
      "  Batch 138/210 - Loss: 0.1800\n",
      "  Batch 139/210 - Loss: 0.1962\n",
      "  Batch 140/210 - Loss: 0.1636\n",
      "  Batch 141/210 - Loss: 0.1441\n",
      "  Batch 142/210 - Loss: 0.1461\n",
      "  Batch 143/210 - Loss: 0.1482\n",
      "  Batch 144/210 - Loss: 0.1791\n",
      "  Batch 145/210 - Loss: 0.1806\n",
      "  Batch 146/210 - Loss: 0.1682\n",
      "  Batch 147/210 - Loss: 0.1685\n",
      "  Batch 148/210 - Loss: 0.1698\n",
      "  Batch 149/210 - Loss: 0.1570\n",
      "  Batch 150/210 - Loss: 0.1714\n",
      "  Batch 151/210 - Loss: 0.1650\n",
      "  Batch 152/210 - Loss: 0.1399\n",
      "  Batch 153/210 - Loss: 0.1503\n",
      "  Batch 154/210 - Loss: 0.1839\n",
      "  Batch 155/210 - Loss: 0.1741\n",
      "  Batch 156/210 - Loss: 0.1793\n",
      "  Batch 157/210 - Loss: 0.1842\n",
      "  Batch 158/210 - Loss: 0.1807\n",
      "  Batch 159/210 - Loss: 0.1532\n",
      "  Batch 160/210 - Loss: 0.2179\n",
      "  Batch 161/210 - Loss: 0.1711\n",
      "  Batch 162/210 - Loss: 0.1609\n",
      "  Batch 163/210 - Loss: 0.1488\n",
      "  Batch 164/210 - Loss: 0.1649\n",
      "  Batch 165/210 - Loss: 0.1300\n",
      "  Batch 166/210 - Loss: 0.1863\n",
      "  Batch 167/210 - Loss: 0.1275\n",
      "  Batch 168/210 - Loss: 0.1454\n",
      "  Batch 169/210 - Loss: 0.1988\n",
      "  Batch 170/210 - Loss: 0.1655\n",
      "  Batch 171/210 - Loss: 0.1624\n",
      "  Batch 172/210 - Loss: 0.1505\n",
      "  Batch 173/210 - Loss: 0.1686\n",
      "  Batch 174/210 - Loss: 0.1552\n",
      "  Batch 175/210 - Loss: 0.1247\n",
      "  Batch 176/210 - Loss: 0.1602\n",
      "  Batch 177/210 - Loss: 0.1752\n",
      "  Batch 178/210 - Loss: 0.1678\n",
      "  Batch 179/210 - Loss: 0.1446\n",
      "  Batch 180/210 - Loss: 0.1527\n",
      "  Batch 181/210 - Loss: 0.1773\n",
      "  Batch 182/210 - Loss: 0.1681\n",
      "  Batch 183/210 - Loss: 0.1719\n",
      "  Batch 184/210 - Loss: 0.1821\n",
      "  Batch 185/210 - Loss: 0.1544\n",
      "  Batch 186/210 - Loss: 0.1355\n",
      "  Batch 187/210 - Loss: 0.1781\n",
      "  Batch 188/210 - Loss: 0.1680\n",
      "  Batch 189/210 - Loss: 0.1239\n",
      "  Batch 190/210 - Loss: 0.1753\n",
      "  Batch 191/210 - Loss: 0.1510\n",
      "  Batch 192/210 - Loss: 0.1223\n",
      "  Batch 193/210 - Loss: 0.1660\n",
      "  Batch 194/210 - Loss: 0.1351\n",
      "  Batch 195/210 - Loss: 0.1652\n",
      "  Batch 196/210 - Loss: 0.1536\n",
      "  Batch 197/210 - Loss: 0.1370\n",
      "  Batch 198/210 - Loss: 0.1487\n",
      "  Batch 199/210 - Loss: 0.1796\n",
      "  Batch 200/210 - Loss: 0.1444\n",
      "  Batch 201/210 - Loss: 0.1157\n",
      "  Batch 202/210 - Loss: 0.1587\n",
      "  Batch 203/210 - Loss: 0.1696\n",
      "  Batch 204/210 - Loss: 0.1287\n",
      "  Batch 205/210 - Loss: 0.1407\n",
      "  Batch 206/210 - Loss: 0.1561\n",
      "  Batch 207/210 - Loss: 0.1601\n",
      "  Batch 208/210 - Loss: 0.1410\n",
      "  Batch 209/210 - Loss: 0.1575\n",
      "  Batch 210/210 - Loss: 0.1475\n",
      "Epoch 8 Completed. Train Loss: 0.1849, Val Loss: 0.0776\n",
      "\n",
      "Epoch 9/13\n",
      "  Batch 1/210 - Loss: 0.1310\n",
      "  Batch 2/210 - Loss: 0.1703\n",
      "  Batch 3/210 - Loss: 0.1342\n",
      "  Batch 4/210 - Loss: 0.1161\n",
      "  Batch 5/210 - Loss: 0.1085\n",
      "  Batch 6/210 - Loss: 0.1166\n",
      "  Batch 7/210 - Loss: 0.1323\n",
      "  Batch 8/210 - Loss: 0.1027\n",
      "  Batch 9/210 - Loss: 0.1473\n",
      "  Batch 10/210 - Loss: 0.1476\n",
      "  Batch 11/210 - Loss: 0.1477\n",
      "  Batch 12/210 - Loss: 0.1335\n",
      "  Batch 13/210 - Loss: 0.1391\n",
      "  Batch 14/210 - Loss: 0.1615\n",
      "  Batch 15/210 - Loss: 0.1318\n",
      "  Batch 16/210 - Loss: 0.1347\n",
      "  Batch 17/210 - Loss: 0.1077\n",
      "  Batch 18/210 - Loss: 0.1676\n",
      "  Batch 19/210 - Loss: 0.1010\n",
      "  Batch 20/210 - Loss: 0.1534\n",
      "  Batch 21/210 - Loss: 0.1242\n",
      "  Batch 22/210 - Loss: 0.1417\n",
      "  Batch 23/210 - Loss: 0.1304\n",
      "  Batch 24/210 - Loss: 0.1180\n",
      "  Batch 25/210 - Loss: 0.1382\n",
      "  Batch 26/210 - Loss: 0.1705\n",
      "  Batch 27/210 - Loss: 0.1147\n",
      "  Batch 28/210 - Loss: 0.1353\n",
      "  Batch 29/210 - Loss: 0.1205\n",
      "  Batch 30/210 - Loss: 0.1188\n",
      "  Batch 31/210 - Loss: 0.1370\n",
      "  Batch 32/210 - Loss: 0.1238\n",
      "  Batch 33/210 - Loss: 0.1271\n",
      "  Batch 34/210 - Loss: 0.1103\n",
      "  Batch 35/210 - Loss: 0.1436\n",
      "  Batch 36/210 - Loss: 0.1439\n",
      "  Batch 37/210 - Loss: 0.1270\n",
      "  Batch 38/210 - Loss: 0.1151\n",
      "  Batch 39/210 - Loss: 0.1275\n",
      "  Batch 40/210 - Loss: 0.1189\n",
      "  Batch 41/210 - Loss: 0.1242\n",
      "  Batch 42/210 - Loss: 0.1291\n",
      "  Batch 43/210 - Loss: 0.1097\n",
      "  Batch 44/210 - Loss: 0.1186\n",
      "  Batch 45/210 - Loss: 0.1157\n",
      "  Batch 46/210 - Loss: 0.1068\n",
      "  Batch 47/210 - Loss: 0.1284\n",
      "  Batch 48/210 - Loss: 0.1185\n",
      "  Batch 49/210 - Loss: 0.1416\n",
      "  Batch 50/210 - Loss: 0.1144\n",
      "  Batch 51/210 - Loss: 0.1592\n",
      "  Batch 52/210 - Loss: 0.1188\n",
      "  Batch 53/210 - Loss: 0.1215\n",
      "  Batch 54/210 - Loss: 0.1289\n",
      "  Batch 55/210 - Loss: 0.1226\n",
      "  Batch 56/210 - Loss: 0.1158\n",
      "  Batch 57/210 - Loss: 0.0883\n",
      "  Batch 58/210 - Loss: 0.1033\n",
      "  Batch 59/210 - Loss: 0.1420\n",
      "  Batch 60/210 - Loss: 0.1374\n",
      "  Batch 61/210 - Loss: 0.1305\n",
      "  Batch 62/210 - Loss: 0.1257\n",
      "  Batch 63/210 - Loss: 0.1402\n",
      "  Batch 64/210 - Loss: 0.1175\n",
      "  Batch 65/210 - Loss: 0.1272\n",
      "  Batch 66/210 - Loss: 0.1289\n",
      "  Batch 67/210 - Loss: 0.1279\n",
      "  Batch 68/210 - Loss: 0.1316\n",
      "  Batch 69/210 - Loss: 0.0948\n",
      "  Batch 70/210 - Loss: 0.1446\n",
      "  Batch 71/210 - Loss: 0.1040\n",
      "  Batch 72/210 - Loss: 0.1287\n",
      "  Batch 73/210 - Loss: 0.1355\n",
      "  Batch 74/210 - Loss: 0.1497\n",
      "  Batch 75/210 - Loss: 0.1468\n",
      "  Batch 76/210 - Loss: 0.1355\n",
      "  Batch 77/210 - Loss: 0.1264\n",
      "  Batch 78/210 - Loss: 0.1435\n",
      "  Batch 79/210 - Loss: 0.1153\n",
      "  Batch 80/210 - Loss: 0.1094\n",
      "  Batch 81/210 - Loss: 0.1028\n",
      "  Batch 82/210 - Loss: 0.1004\n",
      "  Batch 83/210 - Loss: 0.1297\n",
      "  Batch 84/210 - Loss: 0.1228\n",
      "  Batch 85/210 - Loss: 0.1077\n",
      "  Batch 86/210 - Loss: 0.1253\n",
      "  Batch 87/210 - Loss: 0.1130\n",
      "  Batch 88/210 - Loss: 0.1130\n",
      "  Batch 89/210 - Loss: 0.1202\n",
      "  Batch 90/210 - Loss: 0.1102\n",
      "  Batch 91/210 - Loss: 0.1320\n",
      "  Batch 92/210 - Loss: 0.0969\n",
      "  Batch 93/210 - Loss: 0.1144\n",
      "  Batch 94/210 - Loss: 0.1078\n",
      "  Batch 95/210 - Loss: 0.1180\n",
      "  Batch 96/210 - Loss: 0.1207\n",
      "  Batch 97/210 - Loss: 0.1059\n",
      "  Batch 98/210 - Loss: 0.1231\n",
      "  Batch 99/210 - Loss: 0.1264\n",
      "  Batch 100/210 - Loss: 0.1205\n",
      "  Batch 101/210 - Loss: 0.1256\n",
      "  Batch 102/210 - Loss: 0.1254\n",
      "  Batch 103/210 - Loss: 0.1242\n",
      "  Batch 104/210 - Loss: 0.1316\n",
      "  Batch 105/210 - Loss: 0.1001\n",
      "  Batch 106/210 - Loss: 0.0860\n",
      "  Batch 107/210 - Loss: 0.1281\n",
      "  Batch 108/210 - Loss: 0.1258\n",
      "  Batch 109/210 - Loss: 0.1250\n",
      "  Batch 110/210 - Loss: 0.1262\n",
      "  Batch 111/210 - Loss: 0.0904\n",
      "  Batch 112/210 - Loss: 0.1115\n",
      "  Batch 113/210 - Loss: 0.1004\n",
      "  Batch 114/210 - Loss: 0.1013\n",
      "  Batch 115/210 - Loss: 0.1362\n",
      "  Batch 116/210 - Loss: 0.1075\n",
      "  Batch 117/210 - Loss: 0.1006\n",
      "  Batch 118/210 - Loss: 0.1163\n",
      "  Batch 119/210 - Loss: 0.0949\n",
      "  Batch 120/210 - Loss: 0.0862\n",
      "  Batch 121/210 - Loss: 0.1191\n",
      "  Batch 122/210 - Loss: 0.1034\n",
      "  Batch 123/210 - Loss: 0.1305\n",
      "  Batch 124/210 - Loss: 0.0943\n",
      "  Batch 125/210 - Loss: 0.1217\n",
      "  Batch 126/210 - Loss: 0.1344\n",
      "  Batch 127/210 - Loss: 0.1075\n",
      "  Batch 128/210 - Loss: 0.1093\n",
      "  Batch 129/210 - Loss: 0.1012\n",
      "  Batch 130/210 - Loss: 0.1028\n",
      "  Batch 131/210 - Loss: 0.0813\n",
      "  Batch 132/210 - Loss: 0.1025\n",
      "  Batch 133/210 - Loss: 0.0961\n",
      "  Batch 134/210 - Loss: 0.1201\n",
      "  Batch 135/210 - Loss: 0.1029\n",
      "  Batch 136/210 - Loss: 0.1305\n",
      "  Batch 137/210 - Loss: 0.1258\n",
      "  Batch 138/210 - Loss: 0.1136\n",
      "  Batch 139/210 - Loss: 0.1057\n",
      "  Batch 140/210 - Loss: 0.1012\n",
      "  Batch 141/210 - Loss: 0.0991\n",
      "  Batch 142/210 - Loss: 0.0995\n",
      "  Batch 143/210 - Loss: 0.1023\n",
      "  Batch 144/210 - Loss: 0.1319\n",
      "  Batch 145/210 - Loss: 0.0797\n",
      "  Batch 146/210 - Loss: 0.0967\n",
      "  Batch 147/210 - Loss: 0.1151\n",
      "  Batch 148/210 - Loss: 0.0916\n",
      "  Batch 149/210 - Loss: 0.1061\n",
      "  Batch 150/210 - Loss: 0.0949\n",
      "  Batch 151/210 - Loss: 0.0865\n",
      "  Batch 152/210 - Loss: 0.1189\n",
      "  Batch 153/210 - Loss: 0.1013\n",
      "  Batch 154/210 - Loss: 0.1322\n",
      "  Batch 155/210 - Loss: 0.0949\n",
      "  Batch 156/210 - Loss: 0.0879\n",
      "  Batch 157/210 - Loss: 0.0934\n",
      "  Batch 158/210 - Loss: 0.0987\n",
      "  Batch 159/210 - Loss: 0.0975\n",
      "  Batch 160/210 - Loss: 0.0922\n",
      "  Batch 161/210 - Loss: 0.0795\n",
      "  Batch 162/210 - Loss: 0.1011\n",
      "  Batch 163/210 - Loss: 0.1039\n",
      "  Batch 164/210 - Loss: 0.1414\n",
      "  Batch 165/210 - Loss: 0.1113\n",
      "  Batch 166/210 - Loss: 0.0811\n",
      "  Batch 167/210 - Loss: 0.0891\n",
      "  Batch 168/210 - Loss: 0.0827\n",
      "  Batch 169/210 - Loss: 0.0908\n",
      "  Batch 170/210 - Loss: 0.0945\n",
      "  Batch 171/210 - Loss: 0.1100\n",
      "  Batch 172/210 - Loss: 0.0933\n",
      "  Batch 173/210 - Loss: 0.0998\n",
      "  Batch 174/210 - Loss: 0.1071\n",
      "  Batch 175/210 - Loss: 0.1003\n",
      "  Batch 176/210 - Loss: 0.1006\n",
      "  Batch 177/210 - Loss: 0.0971\n",
      "  Batch 178/210 - Loss: 0.0760\n",
      "  Batch 179/210 - Loss: 0.0947\n",
      "  Batch 180/210 - Loss: 0.0855\n",
      "  Batch 181/210 - Loss: 0.1145\n",
      "  Batch 182/210 - Loss: 0.1120\n",
      "  Batch 183/210 - Loss: 0.0965\n",
      "  Batch 184/210 - Loss: 0.0892\n",
      "  Batch 185/210 - Loss: 0.0951\n",
      "  Batch 186/210 - Loss: 0.1149\n",
      "  Batch 187/210 - Loss: 0.0991\n",
      "  Batch 188/210 - Loss: 0.0770\n",
      "  Batch 189/210 - Loss: 0.0972\n",
      "  Batch 190/210 - Loss: 0.1040\n",
      "  Batch 191/210 - Loss: 0.0984\n",
      "  Batch 192/210 - Loss: 0.0816\n",
      "  Batch 193/210 - Loss: 0.0931\n",
      "  Batch 194/210 - Loss: 0.1032\n",
      "  Batch 195/210 - Loss: 0.1198\n",
      "  Batch 196/210 - Loss: 0.0853\n",
      "  Batch 197/210 - Loss: 0.1063\n",
      "  Batch 198/210 - Loss: 0.0975\n",
      "  Batch 199/210 - Loss: 0.0856\n",
      "  Batch 200/210 - Loss: 0.1052\n",
      "  Batch 201/210 - Loss: 0.0900\n",
      "  Batch 202/210 - Loss: 0.0977\n",
      "  Batch 203/210 - Loss: 0.1059\n",
      "  Batch 204/210 - Loss: 0.0752\n",
      "  Batch 205/210 - Loss: 0.1016\n",
      "  Batch 206/210 - Loss: 0.1062\n",
      "  Batch 207/210 - Loss: 0.0892\n",
      "  Batch 208/210 - Loss: 0.0931\n",
      "  Batch 209/210 - Loss: 0.0903\n",
      "  Batch 210/210 - Loss: 0.1020\n",
      "Epoch 9 Completed. Train Loss: 0.1139, Val Loss: 0.0436\n",
      "\n",
      "Epoch 10/13\n",
      "  Batch 1/210 - Loss: 0.0679\n",
      "  Batch 2/210 - Loss: 0.0947\n",
      "  Batch 3/210 - Loss: 0.0737\n",
      "  Batch 4/210 - Loss: 0.1033\n",
      "  Batch 5/210 - Loss: 0.0889\n",
      "  Batch 6/210 - Loss: 0.0914\n",
      "  Batch 7/210 - Loss: 0.0733\n",
      "  Batch 8/210 - Loss: 0.0681\n",
      "  Batch 9/210 - Loss: 0.1012\n",
      "  Batch 10/210 - Loss: 0.0869\n",
      "  Batch 11/210 - Loss: 0.0818\n",
      "  Batch 12/210 - Loss: 0.0775\n",
      "  Batch 13/210 - Loss: 0.0897\n",
      "  Batch 14/210 - Loss: 0.0812\n",
      "  Batch 15/210 - Loss: 0.0775\n",
      "  Batch 16/210 - Loss: 0.0719\n",
      "  Batch 17/210 - Loss: 0.0800\n",
      "  Batch 18/210 - Loss: 0.0621\n",
      "  Batch 19/210 - Loss: 0.1007\n",
      "  Batch 20/210 - Loss: 0.0749\n",
      "  Batch 21/210 - Loss: 0.0757\n",
      "  Batch 22/210 - Loss: 0.0738\n",
      "  Batch 23/210 - Loss: 0.0696\n",
      "  Batch 24/210 - Loss: 0.0688\n",
      "  Batch 25/210 - Loss: 0.0984\n",
      "  Batch 26/210 - Loss: 0.1015\n",
      "  Batch 27/210 - Loss: 0.1088\n",
      "  Batch 28/210 - Loss: 0.0733\n",
      "  Batch 29/210 - Loss: 0.0802\n",
      "  Batch 30/210 - Loss: 0.0734\n",
      "  Batch 31/210 - Loss: 0.0718\n",
      "  Batch 32/210 - Loss: 0.0797\n",
      "  Batch 33/210 - Loss: 0.0782\n",
      "  Batch 34/210 - Loss: 0.0714\n",
      "  Batch 35/210 - Loss: 0.0709\n",
      "  Batch 36/210 - Loss: 0.0950\n",
      "  Batch 37/210 - Loss: 0.0804\n",
      "  Batch 38/210 - Loss: 0.0822\n",
      "  Batch 39/210 - Loss: 0.0601\n",
      "  Batch 40/210 - Loss: 0.0845\n",
      "  Batch 41/210 - Loss: 0.0755\n",
      "  Batch 42/210 - Loss: 0.0873\n",
      "  Batch 43/210 - Loss: 0.0905\n",
      "  Batch 44/210 - Loss: 0.0592\n",
      "  Batch 45/210 - Loss: 0.0781\n",
      "  Batch 46/210 - Loss: 0.0892\n",
      "  Batch 47/210 - Loss: 0.0871\n",
      "  Batch 48/210 - Loss: 0.0801\n",
      "  Batch 49/210 - Loss: 0.0927\n",
      "  Batch 50/210 - Loss: 0.0966\n",
      "  Batch 51/210 - Loss: 0.0621\n",
      "  Batch 52/210 - Loss: 0.0682\n",
      "  Batch 53/210 - Loss: 0.0653\n",
      "  Batch 54/210 - Loss: 0.0954\n",
      "  Batch 55/210 - Loss: 0.0764\n",
      "  Batch 56/210 - Loss: 0.0696\n",
      "  Batch 57/210 - Loss: 0.0829\n",
      "  Batch 58/210 - Loss: 0.0795\n",
      "  Batch 59/210 - Loss: 0.0689\n",
      "  Batch 60/210 - Loss: 0.0800\n",
      "  Batch 61/210 - Loss: 0.0877\n",
      "  Batch 62/210 - Loss: 0.0822\n",
      "  Batch 63/210 - Loss: 0.0841\n",
      "  Batch 64/210 - Loss: 0.0736\n",
      "  Batch 65/210 - Loss: 0.0612\n",
      "  Batch 66/210 - Loss: 0.0781\n",
      "  Batch 67/210 - Loss: 0.0806\n",
      "  Batch 68/210 - Loss: 0.0833\n",
      "  Batch 69/210 - Loss: 0.0810\n",
      "  Batch 70/210 - Loss: 0.0861\n",
      "  Batch 71/210 - Loss: 0.0769\n",
      "  Batch 72/210 - Loss: 0.0832\n",
      "  Batch 73/210 - Loss: 0.0829\n",
      "  Batch 74/210 - Loss: 0.0666\n",
      "  Batch 75/210 - Loss: 0.0812\n",
      "  Batch 76/210 - Loss: 0.0754\n",
      "  Batch 77/210 - Loss: 0.0836\n",
      "  Batch 78/210 - Loss: 0.0679\n",
      "  Batch 79/210 - Loss: 0.0591\n",
      "  Batch 80/210 - Loss: 0.0720\n",
      "  Batch 81/210 - Loss: 0.0870\n",
      "  Batch 82/210 - Loss: 0.0901\n",
      "  Batch 83/210 - Loss: 0.0724\n",
      "  Batch 84/210 - Loss: 0.0870\n",
      "  Batch 85/210 - Loss: 0.0810\n",
      "  Batch 86/210 - Loss: 0.0912\n",
      "  Batch 87/210 - Loss: 0.0631\n",
      "  Batch 88/210 - Loss: 0.0859\n",
      "  Batch 89/210 - Loss: 0.0829\n",
      "  Batch 90/210 - Loss: 0.0575\n",
      "  Batch 91/210 - Loss: 0.0880\n",
      "  Batch 92/210 - Loss: 0.0717\n",
      "  Batch 93/210 - Loss: 0.0744\n",
      "  Batch 94/210 - Loss: 0.1101\n",
      "  Batch 95/210 - Loss: 0.0919\n",
      "  Batch 96/210 - Loss: 0.0744\n",
      "  Batch 97/210 - Loss: 0.0814\n",
      "  Batch 98/210 - Loss: 0.0671\n",
      "  Batch 99/210 - Loss: 0.0765\n",
      "  Batch 100/210 - Loss: 0.0857\n",
      "  Batch 101/210 - Loss: 0.0930\n",
      "  Batch 102/210 - Loss: 0.0751\n",
      "  Batch 103/210 - Loss: 0.0859\n",
      "  Batch 104/210 - Loss: 0.0689\n",
      "  Batch 105/210 - Loss: 0.0657\n",
      "  Batch 106/210 - Loss: 0.0757\n",
      "  Batch 107/210 - Loss: 0.0926\n",
      "  Batch 108/210 - Loss: 0.0989\n",
      "  Batch 109/210 - Loss: 0.0884\n",
      "  Batch 110/210 - Loss: 0.0682\n",
      "  Batch 111/210 - Loss: 0.0678\n",
      "  Batch 112/210 - Loss: 0.0742\n",
      "  Batch 113/210 - Loss: 0.0756\n",
      "  Batch 114/210 - Loss: 0.0909\n",
      "  Batch 115/210 - Loss: 0.0850\n",
      "  Batch 116/210 - Loss: 0.0532\n",
      "  Batch 117/210 - Loss: 0.0705\n",
      "  Batch 118/210 - Loss: 0.0655\n",
      "  Batch 119/210 - Loss: 0.0869\n",
      "  Batch 120/210 - Loss: 0.0646\n",
      "  Batch 121/210 - Loss: 0.0729\n",
      "  Batch 122/210 - Loss: 0.0599\n",
      "  Batch 123/210 - Loss: 0.0656\n",
      "  Batch 124/210 - Loss: 0.0582\n",
      "  Batch 125/210 - Loss: 0.0792\n",
      "  Batch 126/210 - Loss: 0.0760\n",
      "  Batch 127/210 - Loss: 0.0508\n",
      "  Batch 128/210 - Loss: 0.0573\n",
      "  Batch 129/210 - Loss: 0.0936\n",
      "  Batch 130/210 - Loss: 0.0885\n",
      "  Batch 131/210 - Loss: 0.0696\n",
      "  Batch 132/210 - Loss: 0.0808\n",
      "  Batch 133/210 - Loss: 0.0598\n",
      "  Batch 134/210 - Loss: 0.0862\n",
      "  Batch 135/210 - Loss: 0.0640\n",
      "  Batch 136/210 - Loss: 0.0607\n",
      "  Batch 137/210 - Loss: 0.0651\n",
      "  Batch 138/210 - Loss: 0.0838\n",
      "  Batch 139/210 - Loss: 0.0847\n",
      "  Batch 140/210 - Loss: 0.0631\n",
      "  Batch 141/210 - Loss: 0.0819\n",
      "  Batch 142/210 - Loss: 0.0674\n",
      "  Batch 143/210 - Loss: 0.0932\n",
      "  Batch 144/210 - Loss: 0.0604\n",
      "  Batch 145/210 - Loss: 0.0784\n",
      "  Batch 146/210 - Loss: 0.0590\n",
      "  Batch 147/210 - Loss: 0.0510\n",
      "  Batch 148/210 - Loss: 0.0554\n",
      "  Batch 149/210 - Loss: 0.0677\n",
      "  Batch 150/210 - Loss: 0.0661\n",
      "  Batch 151/210 - Loss: 0.0806\n",
      "  Batch 152/210 - Loss: 0.0838\n",
      "  Batch 153/210 - Loss: 0.0791\n",
      "  Batch 154/210 - Loss: 0.0573\n",
      "  Batch 155/210 - Loss: 0.0570\n",
      "  Batch 156/210 - Loss: 0.0647\n",
      "  Batch 157/210 - Loss: 0.0810\n",
      "  Batch 158/210 - Loss: 0.0632\n",
      "  Batch 159/210 - Loss: 0.0872\n",
      "  Batch 160/210 - Loss: 0.0710\n",
      "  Batch 161/210 - Loss: 0.0642\n",
      "  Batch 162/210 - Loss: 0.0859\n",
      "  Batch 163/210 - Loss: 0.0505\n",
      "  Batch 164/210 - Loss: 0.0549\n",
      "  Batch 165/210 - Loss: 0.0552\n",
      "  Batch 166/210 - Loss: 0.0567\n",
      "  Batch 167/210 - Loss: 0.0581\n",
      "  Batch 168/210 - Loss: 0.0566\n",
      "  Batch 169/210 - Loss: 0.0703\n",
      "  Batch 170/210 - Loss: 0.0599\n",
      "  Batch 171/210 - Loss: 0.0702\n",
      "  Batch 172/210 - Loss: 0.0724\n",
      "  Batch 173/210 - Loss: 0.0684\n",
      "  Batch 174/210 - Loss: 0.0682\n",
      "  Batch 175/210 - Loss: 0.0656\n",
      "  Batch 176/210 - Loss: 0.0523\n",
      "  Batch 177/210 - Loss: 0.0762\n",
      "  Batch 178/210 - Loss: 0.0560\n",
      "  Batch 179/210 - Loss: 0.0753\n",
      "  Batch 180/210 - Loss: 0.0659\n",
      "  Batch 181/210 - Loss: 0.0397\n",
      "  Batch 182/210 - Loss: 0.0778\n",
      "  Batch 183/210 - Loss: 0.0934\n",
      "  Batch 184/210 - Loss: 0.0768\n",
      "  Batch 185/210 - Loss: 0.0565\n",
      "  Batch 186/210 - Loss: 0.0754\n",
      "  Batch 187/210 - Loss: 0.0674\n",
      "  Batch 188/210 - Loss: 0.0640\n",
      "  Batch 189/210 - Loss: 0.0719\n",
      "  Batch 190/210 - Loss: 0.0523\n",
      "  Batch 191/210 - Loss: 0.0687\n",
      "  Batch 192/210 - Loss: 0.0535\n",
      "  Batch 193/210 - Loss: 0.0723\n",
      "  Batch 194/210 - Loss: 0.0699\n",
      "  Batch 195/210 - Loss: 0.0662\n",
      "  Batch 196/210 - Loss: 0.0663\n",
      "  Batch 197/210 - Loss: 0.0595\n",
      "  Batch 198/210 - Loss: 0.0648\n",
      "  Batch 199/210 - Loss: 0.0557\n",
      "  Batch 200/210 - Loss: 0.0656\n",
      "  Batch 201/210 - Loss: 0.0621\n",
      "  Batch 202/210 - Loss: 0.0577\n",
      "  Batch 203/210 - Loss: 0.0673\n",
      "  Batch 204/210 - Loss: 0.0735\n",
      "  Batch 205/210 - Loss: 0.0524\n",
      "  Batch 206/210 - Loss: 0.0718\n",
      "  Batch 207/210 - Loss: 0.0576\n",
      "  Batch 208/210 - Loss: 0.0553\n",
      "  Batch 209/210 - Loss: 0.0687\n",
      "  Batch 210/210 - Loss: 0.0608\n",
      "Epoch 10 Completed. Train Loss: 0.0743, Val Loss: 0.0292\n",
      "\n",
      "Epoch 11/13\n",
      "  Batch 1/210 - Loss: 0.0712\n",
      "  Batch 2/210 - Loss: 0.0651\n",
      "  Batch 3/210 - Loss: 0.0593\n",
      "  Batch 4/210 - Loss: 0.0547\n",
      "  Batch 5/210 - Loss: 0.0529\n",
      "  Batch 6/210 - Loss: 0.0518\n",
      "  Batch 7/210 - Loss: 0.0546\n",
      "  Batch 8/210 - Loss: 0.0649\n",
      "  Batch 9/210 - Loss: 0.0602\n",
      "  Batch 10/210 - Loss: 0.0477\n",
      "  Batch 11/210 - Loss: 0.0802\n",
      "  Batch 12/210 - Loss: 0.0594\n",
      "  Batch 13/210 - Loss: 0.0402\n",
      "  Batch 14/210 - Loss: 0.0562\n",
      "  Batch 15/210 - Loss: 0.0568\n",
      "  Batch 16/210 - Loss: 0.0492\n",
      "  Batch 17/210 - Loss: 0.0499\n",
      "  Batch 18/210 - Loss: 0.0623\n",
      "  Batch 19/210 - Loss: 0.0509\n",
      "  Batch 20/210 - Loss: 0.0655\n",
      "  Batch 21/210 - Loss: 0.0471\n",
      "  Batch 22/210 - Loss: 0.0477\n",
      "  Batch 23/210 - Loss: 0.0529\n",
      "  Batch 24/210 - Loss: 0.0513\n",
      "  Batch 25/210 - Loss: 0.0512\n",
      "  Batch 26/210 - Loss: 0.0495\n",
      "  Batch 27/210 - Loss: 0.0601\n",
      "  Batch 28/210 - Loss: 0.0572\n",
      "  Batch 29/210 - Loss: 0.0617\n",
      "  Batch 30/210 - Loss: 0.0803\n",
      "  Batch 31/210 - Loss: 0.0420\n",
      "  Batch 32/210 - Loss: 0.0551\n",
      "  Batch 33/210 - Loss: 0.0757\n",
      "  Batch 34/210 - Loss: 0.0634\n",
      "  Batch 35/210 - Loss: 0.0498\n",
      "  Batch 36/210 - Loss: 0.0602\n",
      "  Batch 37/210 - Loss: 0.0630\n",
      "  Batch 38/210 - Loss: 0.0428\n",
      "  Batch 39/210 - Loss: 0.0462\n",
      "  Batch 40/210 - Loss: 0.0505\n",
      "  Batch 41/210 - Loss: 0.0461\n",
      "  Batch 42/210 - Loss: 0.0556\n",
      "  Batch 43/210 - Loss: 0.0482\n",
      "  Batch 44/210 - Loss: 0.0591\n",
      "  Batch 45/210 - Loss: 0.0458\n",
      "  Batch 46/210 - Loss: 0.0512\n",
      "  Batch 47/210 - Loss: 0.0633\n",
      "  Batch 48/210 - Loss: 0.0699\n",
      "  Batch 49/210 - Loss: 0.0564\n",
      "  Batch 50/210 - Loss: 0.0650\n",
      "  Batch 51/210 - Loss: 0.0514\n",
      "  Batch 52/210 - Loss: 0.0523\n",
      "  Batch 53/210 - Loss: 0.0559\n",
      "  Batch 54/210 - Loss: 0.0428\n",
      "  Batch 55/210 - Loss: 0.0501\n",
      "  Batch 56/210 - Loss: 0.0702\n",
      "  Batch 57/210 - Loss: 0.0702\n",
      "  Batch 58/210 - Loss: 0.0441\n",
      "  Batch 59/210 - Loss: 0.0415\n",
      "  Batch 60/210 - Loss: 0.0635\n",
      "  Batch 61/210 - Loss: 0.0519\n",
      "  Batch 62/210 - Loss: 0.0587\n",
      "  Batch 63/210 - Loss: 0.0571\n",
      "  Batch 64/210 - Loss: 0.0485\n",
      "  Batch 65/210 - Loss: 0.0428\n",
      "  Batch 66/210 - Loss: 0.0566\n",
      "  Batch 67/210 - Loss: 0.0549\n",
      "  Batch 68/210 - Loss: 0.0434\n",
      "  Batch 69/210 - Loss: 0.0655\n",
      "  Batch 70/210 - Loss: 0.0601\n",
      "  Batch 71/210 - Loss: 0.0577\n",
      "  Batch 72/210 - Loss: 0.0627\n",
      "  Batch 73/210 - Loss: 0.0698\n",
      "  Batch 74/210 - Loss: 0.0689\n",
      "  Batch 75/210 - Loss: 0.0454\n",
      "  Batch 76/210 - Loss: 0.0529\n",
      "  Batch 77/210 - Loss: 0.0481\n",
      "  Batch 78/210 - Loss: 0.0472\n",
      "  Batch 79/210 - Loss: 0.0587\n",
      "  Batch 80/210 - Loss: 0.0430\n",
      "  Batch 81/210 - Loss: 0.0584\n",
      "  Batch 82/210 - Loss: 0.0558\n",
      "  Batch 83/210 - Loss: 0.0629\n",
      "  Batch 84/210 - Loss: 0.0608\n",
      "  Batch 85/210 - Loss: 0.0581\n",
      "  Batch 86/210 - Loss: 0.0620\n",
      "  Batch 87/210 - Loss: 0.0465\n",
      "  Batch 88/210 - Loss: 0.0628\n",
      "  Batch 89/210 - Loss: 0.0647\n",
      "  Batch 90/210 - Loss: 0.0423\n",
      "  Batch 91/210 - Loss: 0.0453\n",
      "  Batch 92/210 - Loss: 0.0391\n",
      "  Batch 93/210 - Loss: 0.0769\n",
      "  Batch 94/210 - Loss: 0.0515\n",
      "  Batch 95/210 - Loss: 0.0616\n",
      "  Batch 96/210 - Loss: 0.0520\n",
      "  Batch 97/210 - Loss: 0.0443\n",
      "  Batch 98/210 - Loss: 0.0547\n",
      "  Batch 99/210 - Loss: 0.0684\n",
      "  Batch 100/210 - Loss: 0.0534\n",
      "  Batch 101/210 - Loss: 0.0436\n",
      "  Batch 102/210 - Loss: 0.0411\n",
      "  Batch 103/210 - Loss: 0.0391\n",
      "  Batch 104/210 - Loss: 0.0508\n",
      "  Batch 105/210 - Loss: 0.0604\n",
      "  Batch 106/210 - Loss: 0.0590\n",
      "  Batch 107/210 - Loss: 0.0451\n",
      "  Batch 108/210 - Loss: 0.0622\n",
      "  Batch 109/210 - Loss: 0.0524\n",
      "  Batch 110/210 - Loss: 0.0396\n",
      "  Batch 111/210 - Loss: 0.0551\n",
      "  Batch 112/210 - Loss: 0.0560\n",
      "  Batch 113/210 - Loss: 0.0402\n",
      "  Batch 114/210 - Loss: 0.0528\n",
      "  Batch 115/210 - Loss: 0.0578\n",
      "  Batch 116/210 - Loss: 0.0552\n",
      "  Batch 117/210 - Loss: 0.0507\n",
      "  Batch 118/210 - Loss: 0.0570\n",
      "  Batch 119/210 - Loss: 0.0484\n",
      "  Batch 120/210 - Loss: 0.0413\n",
      "  Batch 121/210 - Loss: 0.0533\n",
      "  Batch 122/210 - Loss: 0.0424\n",
      "  Batch 123/210 - Loss: 0.0514\n",
      "  Batch 124/210 - Loss: 0.0502\n",
      "  Batch 125/210 - Loss: 0.0480\n",
      "  Batch 126/210 - Loss: 0.0584\n",
      "  Batch 127/210 - Loss: 0.0632\n",
      "  Batch 128/210 - Loss: 0.0642\n",
      "  Batch 129/210 - Loss: 0.0492\n",
      "  Batch 130/210 - Loss: 0.0529\n",
      "  Batch 131/210 - Loss: 0.0570\n",
      "  Batch 132/210 - Loss: 0.0413\n",
      "  Batch 133/210 - Loss: 0.0618\n",
      "  Batch 134/210 - Loss: 0.0408\n",
      "  Batch 135/210 - Loss: 0.0474\n",
      "  Batch 136/210 - Loss: 0.0438\n",
      "  Batch 137/210 - Loss: 0.0441\n",
      "  Batch 138/210 - Loss: 0.0621\n",
      "  Batch 139/210 - Loss: 0.0510\n",
      "  Batch 140/210 - Loss: 0.0370\n",
      "  Batch 141/210 - Loss: 0.0662\n",
      "  Batch 142/210 - Loss: 0.0589\n",
      "  Batch 143/210 - Loss: 0.0517\n",
      "  Batch 144/210 - Loss: 0.0448\n",
      "  Batch 145/210 - Loss: 0.0532\n",
      "  Batch 146/210 - Loss: 0.0498\n",
      "  Batch 147/210 - Loss: 0.0435\n",
      "  Batch 148/210 - Loss: 0.0568\n",
      "  Batch 149/210 - Loss: 0.0532\n",
      "  Batch 150/210 - Loss: 0.0529\n",
      "  Batch 151/210 - Loss: 0.0546\n",
      "  Batch 152/210 - Loss: 0.0360\n",
      "  Batch 153/210 - Loss: 0.0446\n",
      "  Batch 154/210 - Loss: 0.0574\n",
      "  Batch 155/210 - Loss: 0.0412\n",
      "  Batch 156/210 - Loss: 0.0555\n",
      "  Batch 157/210 - Loss: 0.0420\n",
      "  Batch 158/210 - Loss: 0.0500\n",
      "  Batch 159/210 - Loss: 0.0464\n",
      "  Batch 160/210 - Loss: 0.0399\n",
      "  Batch 161/210 - Loss: 0.0472\n",
      "  Batch 162/210 - Loss: 0.0640\n",
      "  Batch 163/210 - Loss: 0.0550\n",
      "  Batch 164/210 - Loss: 0.0551\n",
      "  Batch 165/210 - Loss: 0.0452\n",
      "  Batch 166/210 - Loss: 0.0484\n",
      "  Batch 167/210 - Loss: 0.0420\n",
      "  Batch 168/210 - Loss: 0.0495\n",
      "  Batch 169/210 - Loss: 0.0574\n",
      "  Batch 170/210 - Loss: 0.0438\n",
      "  Batch 171/210 - Loss: 0.0597\n",
      "  Batch 172/210 - Loss: 0.0497\n",
      "  Batch 173/210 - Loss: 0.0493\n",
      "  Batch 174/210 - Loss: 0.0498\n",
      "  Batch 175/210 - Loss: 0.0475\n",
      "  Batch 176/210 - Loss: 0.0656\n",
      "  Batch 177/210 - Loss: 0.0479\n",
      "  Batch 178/210 - Loss: 0.0543\n",
      "  Batch 179/210 - Loss: 0.0528\n",
      "  Batch 180/210 - Loss: 0.0402\n",
      "  Batch 181/210 - Loss: 0.0669\n",
      "  Batch 182/210 - Loss: 0.0552\n",
      "  Batch 183/210 - Loss: 0.0394\n",
      "  Batch 184/210 - Loss: 0.0412\n",
      "  Batch 185/210 - Loss: 0.0572\n",
      "  Batch 186/210 - Loss: 0.0465\n",
      "  Batch 187/210 - Loss: 0.0472\n",
      "  Batch 188/210 - Loss: 0.0527\n",
      "  Batch 189/210 - Loss: 0.0433\n",
      "  Batch 190/210 - Loss: 0.0397\n",
      "  Batch 191/210 - Loss: 0.0431\n",
      "  Batch 192/210 - Loss: 0.0336\n",
      "  Batch 193/210 - Loss: 0.0366\n",
      "  Batch 194/210 - Loss: 0.0508\n",
      "  Batch 195/210 - Loss: 0.0352\n",
      "  Batch 196/210 - Loss: 0.0456\n",
      "  Batch 197/210 - Loss: 0.0438\n",
      "  Batch 198/210 - Loss: 0.0590\n",
      "  Batch 199/210 - Loss: 0.0333\n",
      "  Batch 200/210 - Loss: 0.0603\n",
      "  Batch 201/210 - Loss: 0.0447\n",
      "  Batch 202/210 - Loss: 0.0403\n",
      "  Batch 203/210 - Loss: 0.0572\n",
      "  Batch 204/210 - Loss: 0.0622\n",
      "  Batch 205/210 - Loss: 0.0298\n",
      "  Batch 206/210 - Loss: 0.0535\n",
      "  Batch 207/210 - Loss: 0.0426\n",
      "  Batch 208/210 - Loss: 0.0342\n",
      "  Batch 209/210 - Loss: 0.0397\n",
      "  Batch 210/210 - Loss: 0.0380\n",
      "Epoch 11 Completed. Train Loss: 0.0524, Val Loss: 0.0195\n",
      "\n",
      "Epoch 12/13\n",
      "  Batch 1/210 - Loss: 0.0350\n",
      "  Batch 2/210 - Loss: 0.0488\n",
      "  Batch 3/210 - Loss: 0.0458\n",
      "  Batch 4/210 - Loss: 0.0424\n",
      "  Batch 5/210 - Loss: 0.0629\n",
      "  Batch 6/210 - Loss: 0.0463\n",
      "  Batch 7/210 - Loss: 0.0374\n",
      "  Batch 8/210 - Loss: 0.0479\n",
      "  Batch 9/210 - Loss: 0.0426\n",
      "  Batch 10/210 - Loss: 0.0503\n",
      "  Batch 11/210 - Loss: 0.0280\n",
      "  Batch 12/210 - Loss: 0.0468\n",
      "  Batch 13/210 - Loss: 0.0432\n",
      "  Batch 14/210 - Loss: 0.0447\n",
      "  Batch 15/210 - Loss: 0.0418\n",
      "  Batch 16/210 - Loss: 0.0447\n",
      "  Batch 17/210 - Loss: 0.0403\n",
      "  Batch 18/210 - Loss: 0.0409\n",
      "  Batch 19/210 - Loss: 0.0378\n",
      "  Batch 20/210 - Loss: 0.0401\n",
      "  Batch 21/210 - Loss: 0.0498\n",
      "  Batch 22/210 - Loss: 0.0416\n",
      "  Batch 23/210 - Loss: 0.0502\n",
      "  Batch 24/210 - Loss: 0.0423\n",
      "  Batch 25/210 - Loss: 0.0369\n",
      "  Batch 26/210 - Loss: 0.0435\n",
      "  Batch 27/210 - Loss: 0.0475\n",
      "  Batch 28/210 - Loss: 0.0330\n",
      "  Batch 29/210 - Loss: 0.0377\n",
      "  Batch 30/210 - Loss: 0.0356\n",
      "  Batch 31/210 - Loss: 0.0393\n",
      "  Batch 32/210 - Loss: 0.0446\n",
      "  Batch 33/210 - Loss: 0.0408\n",
      "  Batch 34/210 - Loss: 0.0469\n",
      "  Batch 35/210 - Loss: 0.0521\n",
      "  Batch 36/210 - Loss: 0.0374\n",
      "  Batch 37/210 - Loss: 0.0444\n",
      "  Batch 38/210 - Loss: 0.0562\n",
      "  Batch 39/210 - Loss: 0.0431\n",
      "  Batch 40/210 - Loss: 0.0398\n",
      "  Batch 41/210 - Loss: 0.0517\n",
      "  Batch 42/210 - Loss: 0.0522\n",
      "  Batch 43/210 - Loss: 0.0462\n",
      "  Batch 44/210 - Loss: 0.0411\n",
      "  Batch 45/210 - Loss: 0.0394\n",
      "  Batch 46/210 - Loss: 0.0446\n",
      "  Batch 47/210 - Loss: 0.0488\n",
      "  Batch 48/210 - Loss: 0.0404\n",
      "  Batch 49/210 - Loss: 0.0417\n",
      "  Batch 50/210 - Loss: 0.0485\n",
      "  Batch 51/210 - Loss: 0.0438\n",
      "  Batch 52/210 - Loss: 0.0456\n",
      "  Batch 53/210 - Loss: 0.0300\n",
      "  Batch 54/210 - Loss: 0.0333\n",
      "  Batch 55/210 - Loss: 0.0400\n",
      "  Batch 56/210 - Loss: 0.0327\n",
      "  Batch 57/210 - Loss: 0.0370\n",
      "  Batch 58/210 - Loss: 0.0407\n",
      "  Batch 59/210 - Loss: 0.0423\n",
      "  Batch 60/210 - Loss: 0.0391\n",
      "  Batch 61/210 - Loss: 0.0452\n",
      "  Batch 62/210 - Loss: 0.0481\n",
      "  Batch 63/210 - Loss: 0.0400\n",
      "  Batch 64/210 - Loss: 0.0454\n",
      "  Batch 65/210 - Loss: 0.0390\n",
      "  Batch 66/210 - Loss: 0.0322\n",
      "  Batch 67/210 - Loss: 0.0316\n",
      "  Batch 68/210 - Loss: 0.0422\n",
      "  Batch 69/210 - Loss: 0.0369\n",
      "  Batch 70/210 - Loss: 0.0419\n",
      "  Batch 71/210 - Loss: 0.0384\n",
      "  Batch 72/210 - Loss: 0.0414\n",
      "  Batch 73/210 - Loss: 0.0448\n",
      "  Batch 74/210 - Loss: 0.0361\n",
      "  Batch 75/210 - Loss: 0.0349\n",
      "  Batch 76/210 - Loss: 0.0433\n",
      "  Batch 77/210 - Loss: 0.0424\n",
      "  Batch 78/210 - Loss: 0.0418\n",
      "  Batch 79/210 - Loss: 0.0270\n",
      "  Batch 80/210 - Loss: 0.0409\n",
      "  Batch 81/210 - Loss: 0.0380\n",
      "  Batch 82/210 - Loss: 0.0502\n",
      "  Batch 83/210 - Loss: 0.0339\n",
      "  Batch 84/210 - Loss: 0.0350\n",
      "  Batch 85/210 - Loss: 0.0368\n",
      "  Batch 86/210 - Loss: 0.0474\n",
      "  Batch 87/210 - Loss: 0.0484\n",
      "  Batch 88/210 - Loss: 0.0395\n",
      "  Batch 89/210 - Loss: 0.0321\n",
      "  Batch 90/210 - Loss: 0.0383\n",
      "  Batch 91/210 - Loss: 0.0297\n",
      "  Batch 92/210 - Loss: 0.0336\n",
      "  Batch 93/210 - Loss: 0.0226\n",
      "  Batch 94/210 - Loss: 0.0370\n",
      "  Batch 95/210 - Loss: 0.0333\n",
      "  Batch 96/210 - Loss: 0.0370\n",
      "  Batch 97/210 - Loss: 0.0372\n",
      "  Batch 98/210 - Loss: 0.0447\n",
      "  Batch 99/210 - Loss: 0.0386\n",
      "  Batch 100/210 - Loss: 0.0383\n",
      "  Batch 101/210 - Loss: 0.0417\n",
      "  Batch 102/210 - Loss: 0.0302\n",
      "  Batch 103/210 - Loss: 0.0297\n",
      "  Batch 104/210 - Loss: 0.0404\n",
      "  Batch 105/210 - Loss: 0.0516\n",
      "  Batch 106/210 - Loss: 0.0476\n",
      "  Batch 107/210 - Loss: 0.0440\n",
      "  Batch 108/210 - Loss: 0.0377\n",
      "  Batch 109/210 - Loss: 0.0307\n",
      "  Batch 110/210 - Loss: 0.0330\n",
      "  Batch 111/210 - Loss: 0.0417\n",
      "  Batch 112/210 - Loss: 0.0352\n",
      "  Batch 113/210 - Loss: 0.0377\n",
      "  Batch 114/210 - Loss: 0.0448\n",
      "  Batch 115/210 - Loss: 0.0327\n",
      "  Batch 116/210 - Loss: 0.0327\n",
      "  Batch 117/210 - Loss: 0.0391\n",
      "  Batch 118/210 - Loss: 0.0278\n",
      "  Batch 119/210 - Loss: 0.0382\n",
      "  Batch 120/210 - Loss: 0.0248\n",
      "  Batch 121/210 - Loss: 0.0461\n",
      "  Batch 122/210 - Loss: 0.0415\n",
      "  Batch 123/210 - Loss: 0.0284\n",
      "  Batch 124/210 - Loss: 0.0334\n",
      "  Batch 125/210 - Loss: 0.0358\n",
      "  Batch 126/210 - Loss: 0.0478\n",
      "  Batch 127/210 - Loss: 0.0401\n",
      "  Batch 128/210 - Loss: 0.0403\n",
      "  Batch 129/210 - Loss: 0.0402\n",
      "  Batch 130/210 - Loss: 0.0353\n",
      "  Batch 131/210 - Loss: 0.0412\n",
      "  Batch 132/210 - Loss: 0.0425\n",
      "  Batch 133/210 - Loss: 0.0319\n",
      "  Batch 134/210 - Loss: 0.0322\n",
      "  Batch 135/210 - Loss: 0.0268\n",
      "  Batch 136/210 - Loss: 0.0327\n",
      "  Batch 137/210 - Loss: 0.0310\n",
      "  Batch 138/210 - Loss: 0.0426\n",
      "  Batch 139/210 - Loss: 0.0287\n",
      "  Batch 140/210 - Loss: 0.0339\n",
      "  Batch 141/210 - Loss: 0.0383\n",
      "  Batch 142/210 - Loss: 0.0396\n",
      "  Batch 143/210 - Loss: 0.0501\n",
      "  Batch 144/210 - Loss: 0.0326\n",
      "  Batch 145/210 - Loss: 0.0328\n",
      "  Batch 146/210 - Loss: 0.0304\n",
      "  Batch 147/210 - Loss: 0.0451\n",
      "  Batch 148/210 - Loss: 0.0258\n",
      "  Batch 149/210 - Loss: 0.0269\n",
      "  Batch 150/210 - Loss: 0.0280\n",
      "  Batch 151/210 - Loss: 0.0450\n",
      "  Batch 152/210 - Loss: 0.0499\n",
      "  Batch 153/210 - Loss: 0.0333\n",
      "  Batch 154/210 - Loss: 0.0286\n",
      "  Batch 155/210 - Loss: 0.0307\n",
      "  Batch 156/210 - Loss: 0.0403\n",
      "  Batch 157/210 - Loss: 0.0355\n",
      "  Batch 158/210 - Loss: 0.0365\n",
      "  Batch 159/210 - Loss: 0.0649\n",
      "  Batch 160/210 - Loss: 0.0314\n",
      "  Batch 161/210 - Loss: 0.0499\n",
      "  Batch 162/210 - Loss: 0.0279\n",
      "  Batch 163/210 - Loss: 0.0411\n",
      "  Batch 164/210 - Loss: 0.0281\n",
      "  Batch 165/210 - Loss: 0.0519\n",
      "  Batch 166/210 - Loss: 0.0383\n",
      "  Batch 167/210 - Loss: 0.0465\n",
      "  Batch 168/210 - Loss: 0.0419\n",
      "  Batch 169/210 - Loss: 0.0447\n",
      "  Batch 170/210 - Loss: 0.0361\n",
      "  Batch 171/210 - Loss: 0.0396\n",
      "  Batch 172/210 - Loss: 0.0355\n",
      "  Batch 173/210 - Loss: 0.0268\n",
      "  Batch 174/210 - Loss: 0.0440\n",
      "  Batch 175/210 - Loss: 0.0330\n",
      "  Batch 176/210 - Loss: 0.0349\n",
      "  Batch 177/210 - Loss: 0.0408\n",
      "  Batch 178/210 - Loss: 0.0302\n",
      "  Batch 179/210 - Loss: 0.0271\n",
      "  Batch 180/210 - Loss: 0.0347\n",
      "  Batch 181/210 - Loss: 0.0282\n",
      "  Batch 182/210 - Loss: 0.0363\n",
      "  Batch 183/210 - Loss: 0.0325\n",
      "  Batch 184/210 - Loss: 0.0378\n",
      "  Batch 185/210 - Loss: 0.0379\n",
      "  Batch 186/210 - Loss: 0.0253\n",
      "  Batch 187/210 - Loss: 0.0280\n",
      "  Batch 188/210 - Loss: 0.0427\n",
      "  Batch 189/210 - Loss: 0.0450\n",
      "  Batch 190/210 - Loss: 0.0384\n",
      "  Batch 191/210 - Loss: 0.0404\n",
      "  Batch 192/210 - Loss: 0.0530\n",
      "  Batch 193/210 - Loss: 0.0297\n",
      "  Batch 194/210 - Loss: 0.0338\n",
      "  Batch 195/210 - Loss: 0.0416\n",
      "  Batch 196/210 - Loss: 0.0389\n",
      "  Batch 197/210 - Loss: 0.0327\n",
      "  Batch 198/210 - Loss: 0.0294\n",
      "  Batch 199/210 - Loss: 0.0323\n",
      "  Batch 200/210 - Loss: 0.0398\n",
      "  Batch 201/210 - Loss: 0.0247\n",
      "  Batch 202/210 - Loss: 0.0486\n",
      "  Batch 203/210 - Loss: 0.0321\n",
      "  Batch 204/210 - Loss: 0.0269\n",
      "  Batch 205/210 - Loss: 0.0244\n",
      "  Batch 206/210 - Loss: 0.0382\n",
      "  Batch 207/210 - Loss: 0.0389\n",
      "  Batch 208/210 - Loss: 0.0345\n",
      "  Batch 209/210 - Loss: 0.0356\n",
      "  Batch 210/210 - Loss: 0.0373\n",
      "Epoch 12 Completed. Train Loss: 0.0388, Val Loss: 0.0176\n",
      "\n",
      "Epoch 13/13\n",
      "  Batch 1/210 - Loss: 0.0389\n",
      "  Batch 2/210 - Loss: 0.0361\n",
      "  Batch 3/210 - Loss: 0.0267\n",
      "  Batch 4/210 - Loss: 0.0352\n",
      "  Batch 5/210 - Loss: 0.0285\n",
      "  Batch 6/210 - Loss: 0.0252\n",
      "  Batch 7/210 - Loss: 0.0308\n",
      "  Batch 8/210 - Loss: 0.0331\n",
      "  Batch 9/210 - Loss: 0.0359\n",
      "  Batch 10/210 - Loss: 0.0262\n",
      "  Batch 11/210 - Loss: 0.0410\n",
      "  Batch 12/210 - Loss: 0.0349\n",
      "  Batch 13/210 - Loss: 0.0272\n",
      "  Batch 14/210 - Loss: 0.0339\n",
      "  Batch 15/210 - Loss: 0.0319\n",
      "  Batch 16/210 - Loss: 0.0242\n",
      "  Batch 17/210 - Loss: 0.0288\n",
      "  Batch 18/210 - Loss: 0.0256\n",
      "  Batch 19/210 - Loss: 0.0296\n",
      "  Batch 20/210 - Loss: 0.0273\n",
      "  Batch 21/210 - Loss: 0.0280\n",
      "  Batch 22/210 - Loss: 0.0316\n",
      "  Batch 23/210 - Loss: 0.0242\n",
      "  Batch 24/210 - Loss: 0.0307\n",
      "  Batch 25/210 - Loss: 0.0417\n",
      "  Batch 26/210 - Loss: 0.0333\n",
      "  Batch 27/210 - Loss: 0.0307\n",
      "  Batch 28/210 - Loss: 0.0240\n",
      "  Batch 29/210 - Loss: 0.0332\n",
      "  Batch 30/210 - Loss: 0.0390\n",
      "  Batch 31/210 - Loss: 0.0353\n",
      "  Batch 32/210 - Loss: 0.0362\n",
      "  Batch 33/210 - Loss: 0.0229\n",
      "  Batch 34/210 - Loss: 0.0277\n",
      "  Batch 35/210 - Loss: 0.0325\n",
      "  Batch 36/210 - Loss: 0.0417\n",
      "  Batch 37/210 - Loss: 0.0269\n",
      "  Batch 38/210 - Loss: 0.0264\n",
      "  Batch 39/210 - Loss: 0.0416\n",
      "  Batch 40/210 - Loss: 0.0313\n",
      "  Batch 41/210 - Loss: 0.0359\n",
      "  Batch 42/210 - Loss: 0.0256\n",
      "  Batch 43/210 - Loss: 0.0280\n",
      "  Batch 44/210 - Loss: 0.0366\n",
      "  Batch 45/210 - Loss: 0.0358\n",
      "  Batch 46/210 - Loss: 0.0283\n",
      "  Batch 47/210 - Loss: 0.0321\n",
      "  Batch 48/210 - Loss: 0.0275\n",
      "  Batch 49/210 - Loss: 0.0328\n",
      "  Batch 50/210 - Loss: 0.0371\n",
      "  Batch 51/210 - Loss: 0.0316\n",
      "  Batch 52/210 - Loss: 0.0422\n",
      "  Batch 53/210 - Loss: 0.0434\n",
      "  Batch 54/210 - Loss: 0.0340\n",
      "  Batch 55/210 - Loss: 0.0303\n",
      "  Batch 56/210 - Loss: 0.0339\n",
      "  Batch 57/210 - Loss: 0.0359\n",
      "  Batch 58/210 - Loss: 0.0422\n",
      "  Batch 59/210 - Loss: 0.0218\n",
      "  Batch 60/210 - Loss: 0.0367\n",
      "  Batch 61/210 - Loss: 0.0304\n",
      "  Batch 62/210 - Loss: 0.0404\n",
      "  Batch 63/210 - Loss: 0.0280\n",
      "  Batch 64/210 - Loss: 0.0329\n",
      "  Batch 65/210 - Loss: 0.0470\n",
      "  Batch 66/210 - Loss: 0.0350\n",
      "  Batch 67/210 - Loss: 0.0344\n",
      "  Batch 68/210 - Loss: 0.0219\n",
      "  Batch 69/210 - Loss: 0.0315\n",
      "  Batch 70/210 - Loss: 0.0289\n",
      "  Batch 71/210 - Loss: 0.0415\n",
      "  Batch 72/210 - Loss: 0.0328\n",
      "  Batch 73/210 - Loss: 0.0307\n",
      "  Batch 74/210 - Loss: 0.0261\n",
      "  Batch 75/210 - Loss: 0.0353\n",
      "  Batch 76/210 - Loss: 0.0347\n",
      "  Batch 77/210 - Loss: 0.0405\n",
      "  Batch 78/210 - Loss: 0.0256\n",
      "  Batch 79/210 - Loss: 0.0277\n",
      "  Batch 80/210 - Loss: 0.0404\n",
      "  Batch 81/210 - Loss: 0.0291\n",
      "  Batch 82/210 - Loss: 0.0399\n",
      "  Batch 83/210 - Loss: 0.0298\n",
      "  Batch 84/210 - Loss: 0.0279\n",
      "  Batch 85/210 - Loss: 0.0246\n",
      "  Batch 86/210 - Loss: 0.0261\n",
      "  Batch 87/210 - Loss: 0.0332\n",
      "  Batch 88/210 - Loss: 0.0285\n",
      "  Batch 89/210 - Loss: 0.0303\n",
      "  Batch 90/210 - Loss: 0.0312\n",
      "  Batch 91/210 - Loss: 0.0369\n",
      "  Batch 92/210 - Loss: 0.0324\n",
      "  Batch 93/210 - Loss: 0.0445\n",
      "  Batch 94/210 - Loss: 0.0273\n",
      "  Batch 95/210 - Loss: 0.0248\n",
      "  Batch 96/210 - Loss: 0.0223\n",
      "  Batch 97/210 - Loss: 0.0411\n",
      "  Batch 98/210 - Loss: 0.0225\n",
      "  Batch 99/210 - Loss: 0.0257\n",
      "  Batch 100/210 - Loss: 0.0279\n",
      "  Batch 101/210 - Loss: 0.0313\n",
      "  Batch 102/210 - Loss: 0.0313\n",
      "  Batch 103/210 - Loss: 0.0235\n",
      "  Batch 104/210 - Loss: 0.0301\n",
      "  Batch 105/210 - Loss: 0.0245\n",
      "  Batch 106/210 - Loss: 0.0305\n",
      "  Batch 107/210 - Loss: 0.0351\n",
      "  Batch 108/210 - Loss: 0.0356\n",
      "  Batch 109/210 - Loss: 0.0194\n",
      "  Batch 110/210 - Loss: 0.0333\n",
      "  Batch 111/210 - Loss: 0.0238\n",
      "  Batch 112/210 - Loss: 0.0342\n",
      "  Batch 113/210 - Loss: 0.0237\n",
      "  Batch 114/210 - Loss: 0.0232\n",
      "  Batch 115/210 - Loss: 0.0323\n",
      "  Batch 116/210 - Loss: 0.0256\n",
      "  Batch 117/210 - Loss: 0.0309\n",
      "  Batch 118/210 - Loss: 0.0286\n",
      "  Batch 119/210 - Loss: 0.0353\n",
      "  Batch 120/210 - Loss: 0.0260\n",
      "  Batch 121/210 - Loss: 0.0302\n",
      "  Batch 122/210 - Loss: 0.0295\n",
      "  Batch 123/210 - Loss: 0.0311\n",
      "  Batch 124/210 - Loss: 0.0330\n",
      "  Batch 125/210 - Loss: 0.0244\n",
      "  Batch 126/210 - Loss: 0.0370\n",
      "  Batch 127/210 - Loss: 0.0316\n",
      "  Batch 128/210 - Loss: 0.0416\n",
      "  Batch 129/210 - Loss: 0.0339\n",
      "  Batch 130/210 - Loss: 0.0289\n",
      "  Batch 131/210 - Loss: 0.0385\n",
      "  Batch 132/210 - Loss: 0.0204\n",
      "  Batch 133/210 - Loss: 0.0343\n",
      "  Batch 134/210 - Loss: 0.0222\n",
      "  Batch 135/210 - Loss: 0.0300\n",
      "  Batch 136/210 - Loss: 0.0239\n",
      "  Batch 137/210 - Loss: 0.0404\n",
      "  Batch 138/210 - Loss: 0.0225\n",
      "  Batch 139/210 - Loss: 0.0191\n",
      "  Batch 140/210 - Loss: 0.0310\n",
      "  Batch 141/210 - Loss: 0.0441\n",
      "  Batch 142/210 - Loss: 0.0382\n",
      "  Batch 143/210 - Loss: 0.0197\n",
      "  Batch 144/210 - Loss: 0.0294\n",
      "  Batch 145/210 - Loss: 0.0386\n",
      "  Batch 146/210 - Loss: 0.0212\n",
      "  Batch 147/210 - Loss: 0.0329\n",
      "  Batch 148/210 - Loss: 0.0327\n",
      "  Batch 149/210 - Loss: 0.0197\n",
      "  Batch 150/210 - Loss: 0.0254\n",
      "  Batch 151/210 - Loss: 0.0316\n",
      "  Batch 152/210 - Loss: 0.0260\n",
      "  Batch 153/210 - Loss: 0.0404\n",
      "  Batch 154/210 - Loss: 0.0310\n",
      "  Batch 155/210 - Loss: 0.0223\n",
      "  Batch 156/210 - Loss: 0.0327\n",
      "  Batch 157/210 - Loss: 0.0409\n",
      "  Batch 158/210 - Loss: 0.0233\n",
      "  Batch 159/210 - Loss: 0.0343\n",
      "  Batch 160/210 - Loss: 0.0352\n",
      "  Batch 161/210 - Loss: 0.0347\n",
      "  Batch 162/210 - Loss: 0.0228\n",
      "  Batch 163/210 - Loss: 0.0262\n",
      "  Batch 164/210 - Loss: 0.0187\n",
      "  Batch 165/210 - Loss: 0.0322\n",
      "  Batch 166/210 - Loss: 0.0237\n",
      "  Batch 167/210 - Loss: 0.0253\n",
      "  Batch 168/210 - Loss: 0.0307\n",
      "  Batch 169/210 - Loss: 0.0250\n",
      "  Batch 170/210 - Loss: 0.0253\n",
      "  Batch 171/210 - Loss: 0.0408\n",
      "  Batch 172/210 - Loss: 0.0365\n",
      "  Batch 173/210 - Loss: 0.0302\n",
      "  Batch 174/210 - Loss: 0.0383\n",
      "  Batch 175/210 - Loss: 0.0410\n",
      "  Batch 176/210 - Loss: 0.0247\n",
      "  Batch 177/210 - Loss: 0.0326\n",
      "  Batch 178/210 - Loss: 0.0369\n",
      "  Batch 179/210 - Loss: 0.0256\n",
      "  Batch 180/210 - Loss: 0.0226\n",
      "  Batch 181/210 - Loss: 0.0331\n",
      "  Batch 182/210 - Loss: 0.0265\n",
      "  Batch 183/210 - Loss: 0.0271\n",
      "  Batch 184/210 - Loss: 0.0241\n",
      "  Batch 185/210 - Loss: 0.0300\n",
      "  Batch 186/210 - Loss: 0.0296\n",
      "  Batch 187/210 - Loss: 0.0212\n",
      "  Batch 188/210 - Loss: 0.0380\n",
      "  Batch 189/210 - Loss: 0.0295\n",
      "  Batch 190/210 - Loss: 0.0268\n",
      "  Batch 191/210 - Loss: 0.0330\n",
      "  Batch 192/210 - Loss: 0.0281\n",
      "  Batch 193/210 - Loss: 0.0281\n",
      "  Batch 194/210 - Loss: 0.0284\n",
      "  Batch 195/210 - Loss: 0.0321\n",
      "  Batch 196/210 - Loss: 0.0310\n",
      "  Batch 197/210 - Loss: 0.0276\n",
      "  Batch 198/210 - Loss: 0.0294\n",
      "  Batch 199/210 - Loss: 0.0301\n",
      "  Batch 200/210 - Loss: 0.0284\n",
      "  Batch 201/210 - Loss: 0.0249\n",
      "  Batch 202/210 - Loss: 0.0312\n",
      "  Batch 203/210 - Loss: 0.0243\n",
      "  Batch 204/210 - Loss: 0.0215\n",
      "  Batch 205/210 - Loss: 0.0292\n",
      "  Batch 206/210 - Loss: 0.0264\n",
      "  Batch 207/210 - Loss: 0.0279\n",
      "  Batch 208/210 - Loss: 0.0308\n",
      "  Batch 209/210 - Loss: 0.0208\n",
      "  Batch 210/210 - Loss: 0.0256\n",
      "Epoch 13 Completed. Train Loss: 0.0307, Val Loss: 0.0147\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-L': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aamir\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\aamir\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\aamir\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# =================== Dataset ===================\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_lines, target_lines, src_tokenizer, tgt_tokenizer, max_len=100):\n",
    "        self.source_lines = source_lines\n",
    "        self.target_lines = target_lines\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.source_lines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src = self.source_lines[idx][:self.max_len]\n",
    "        tgt = self.target_lines[idx][:self.max_len]\n",
    "        \n",
    "        src_ids = [self.src_tokenizer.piece_to_id(p) for p in src]\n",
    "        tgt_ids = [self.tgt_tokenizer.piece_to_id(p) for p in tgt]\n",
    "        \n",
    "        return torch.tensor(src_ids), torch.tensor(tgt_ids)\n",
    "\n",
    "# =================== Positional Encoding ===================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# =================== Transformer Model ===================\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers,\n",
    "                                          dim_feedforward, dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        output = self.transformer(src_emb, tgt_emb, src_key_padding_mask=src_mask, tgt_key_padding_mask=tgt_mask)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# =================== Utilities ===================\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz))) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "    tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "def compute_bleu(predictions, references):\n",
    "    return corpus_bleu([[ref] for ref in references], predictions)\n",
    "\n",
    "def compute_rouge(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge1, rouge2, rougeL = [], [], []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_sentence = \" \".join(pred)\n",
    "        ref_sentence = \" \".join(ref)\n",
    "        scores = scorer.score(ref_sentence, pred_sentence)\n",
    "        rouge1.append(scores['rouge1'].fmeasure)\n",
    "        rouge2.append(scores['rouge2'].fmeasure)\n",
    "        rougeL.append(scores['rougeL'].fmeasure)\n",
    "    return {\n",
    "        \"rouge-1\": np.mean(rouge1),\n",
    "        \"rouge-2\": np.mean(rouge2),\n",
    "        \"rouge-L\": np.mean(rougeL)\n",
    "    }\n",
    "\n",
    "def plot_training(train_losses, val_losses):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# =================== Training Function ===================\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, num_epochs=30, patience=5):\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt_input)\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            loss = criterion(output, tgt_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            print(f\"  Batch {batch_idx+1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        avg_val_loss = evaluate_model(model, val_loader, criterion)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Completed. Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        scheduler.step(avg_val_loss)  # No 'verbose' here\n",
    "        \n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_transformer.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print('Early stopping triggered!')\n",
    "                break\n",
    "    \n",
    "    plot_training(train_losses, val_losses)\n",
    "\n",
    "def evaluate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            output = model(src, tgt_input)\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            loss = criterion(output, tgt_output)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# =================== Main Execution ===================\n",
    "\n",
    "train_src = tokenized_data['quran']['train.en'] + tokenized_data['bible']['train.en']\n",
    "train_tgt = tokenized_data['quran']['train.ur'] + tokenized_data['bible']['train.ur']\n",
    "\n",
    "val_src = tokenized_data['quran']['dev.en'] + tokenized_data['bible']['dev.en']\n",
    "val_tgt = tokenized_data['quran']['dev.ur'] + tokenized_data['bible']['dev.ur']\n",
    "\n",
    "test_src = tokenized_data['quran']['test.en'] + tokenized_data['bible']['test.en']\n",
    "test_tgt = tokenized_data['quran']['test.ur'] + tokenized_data['bible']['test.ur']\n",
    "\n",
    "train_dataset = TranslationDataset(train_src, train_tgt, tokenizer_en, tokenizer_ur)\n",
    "val_dataset = TranslationDataset(val_src, val_tgt, tokenizer_en, tokenizer_ur)\n",
    "test_dataset = TranslationDataset(test_src, test_tgt, tokenizer_en, tokenizer_ur)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(tokenizer_en)\n",
    "TGT_VOCAB_SIZE = len(tokenizer_ur)\n",
    "\n",
    "model = Transformer(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, d_model=256, nhead=4, num_encoder_layers=3, num_decoder_layers=3).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Train\n",
    "train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, num_epochs=13, patience=5)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_transformer.pth', weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "predictions, references = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for src, tgt in test_loader:\n",
    "        src = src.to(device)\n",
    "        tgt_input = torch.zeros_like(src[:, :1]).fill_(2)  # Assuming 2 = <BOS> token id\n",
    "        outputs = []\n",
    "        \n",
    "        for _ in range(50):\n",
    "            output = model(src, tgt_input)\n",
    "            next_token = output[:, -1, :].argmax(-1, keepdim=True)\n",
    "            tgt_input = torch.cat([tgt_input, next_token], dim=1)\n",
    "            outputs.append(next_token)\n",
    "        \n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        pred_tokens = [tokenizer_ur.id_to_piece(int(tok)) for tok in outputs[0]]\n",
    "        tgt_tokens = [tokenizer_ur.id_to_piece(int(tok)) for tok in tgt[0]]\n",
    "        \n",
    "        predictions.append(pred_tokens)\n",
    "        references.append(tgt_tokens)\n",
    "\n",
    "# Compute and display BLEU & ROUGE\n",
    "bleu = compute_bleu(predictions, references)\n",
    "rouge = compute_rouge(predictions, references)\n",
    "\n",
    "print(f'\\nBLEU Score: {bleu:.4f}')\n",
    "print('ROUGE Scores:', rouge)\n",
    "\n",
    "# Visualization\n",
    "labels = list(rouge.keys())\n",
    "scores = list(rouge.values())\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.bar(labels, scores, color='skyblue')\n",
    "plt.title('ROUGE Scores')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f2e989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70eb84b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Starting...\n",
      "Epoch 1 | Batch 1/94 | Batch Loss: 7.0999\n",
      "Epoch 1 | Batch 2/94 | Batch Loss: 6.9020\n",
      "Epoch 1 | Batch 3/94 | Batch Loss: 6.7452\n",
      "Epoch 1 | Batch 4/94 | Batch Loss: 6.6455\n",
      "Epoch 1 | Batch 5/94 | Batch Loss: 6.5910\n",
      "Epoch 1 | Batch 6/94 | Batch Loss: 6.5154\n",
      "Epoch 1 | Batch 7/94 | Batch Loss: 6.4910\n",
      "Epoch 1 | Batch 8/94 | Batch Loss: 6.4353\n",
      "Epoch 1 | Batch 9/94 | Batch Loss: 6.3976\n",
      "Epoch 1 | Batch 10/94 | Batch Loss: 6.3651\n",
      "Epoch 1 | Batch 11/94 | Batch Loss: 6.3423\n",
      "Epoch 1 | Batch 12/94 | Batch Loss: 6.2756\n",
      "Epoch 1 | Batch 13/94 | Batch Loss: 6.2545\n",
      "Epoch 1 | Batch 14/94 | Batch Loss: 6.2579\n",
      "Epoch 1 | Batch 15/94 | Batch Loss: 6.2252\n",
      "Epoch 1 | Batch 16/94 | Batch Loss: 6.2190\n",
      "Epoch 1 | Batch 17/94 | Batch Loss: 6.2131\n",
      "Epoch 1 | Batch 18/94 | Batch Loss: 6.1826\n",
      "Epoch 1 | Batch 19/94 | Batch Loss: 6.1568\n",
      "Epoch 1 | Batch 20/94 | Batch Loss: 6.1516\n",
      "Epoch 1 | Batch 21/94 | Batch Loss: 6.1184\n",
      "Epoch 1 | Batch 22/94 | Batch Loss: 6.1026\n",
      "Epoch 1 | Batch 23/94 | Batch Loss: 6.0939\n",
      "Epoch 1 | Batch 24/94 | Batch Loss: 6.0747\n",
      "Epoch 1 | Batch 25/94 | Batch Loss: 6.0730\n",
      "Epoch 1 | Batch 26/94 | Batch Loss: 6.0502\n",
      "Epoch 1 | Batch 27/94 | Batch Loss: 6.0499\n",
      "Epoch 1 | Batch 28/94 | Batch Loss: 6.0297\n",
      "Epoch 1 | Batch 29/94 | Batch Loss: 5.9956\n",
      "Epoch 1 | Batch 30/94 | Batch Loss: 5.9828\n",
      "Epoch 1 | Batch 31/94 | Batch Loss: 5.9762\n",
      "Epoch 1 | Batch 32/94 | Batch Loss: 5.9415\n",
      "Epoch 1 | Batch 33/94 | Batch Loss: 5.9239\n",
      "Epoch 1 | Batch 34/94 | Batch Loss: 5.9166\n",
      "Epoch 1 | Batch 35/94 | Batch Loss: 5.9306\n",
      "Epoch 1 | Batch 36/94 | Batch Loss: 5.9111\n",
      "Epoch 1 | Batch 37/94 | Batch Loss: 5.8799\n",
      "Epoch 1 | Batch 38/94 | Batch Loss: 5.8345\n",
      "Epoch 1 | Batch 39/94 | Batch Loss: 5.8649\n",
      "Epoch 1 | Batch 40/94 | Batch Loss: 5.8494\n",
      "Epoch 1 | Batch 41/94 | Batch Loss: 5.8013\n",
      "Epoch 1 | Batch 42/94 | Batch Loss: 5.8060\n",
      "Epoch 1 | Batch 43/94 | Batch Loss: 5.7813\n",
      "Epoch 1 | Batch 44/94 | Batch Loss: 5.7885\n",
      "Epoch 1 | Batch 45/94 | Batch Loss: 5.7713\n",
      "Epoch 1 | Batch 46/94 | Batch Loss: 5.7487\n",
      "Epoch 1 | Batch 47/94 | Batch Loss: 5.7454\n",
      "Epoch 1 | Batch 48/94 | Batch Loss: 5.7708\n",
      "Epoch 1 | Batch 49/94 | Batch Loss: 5.6850\n",
      "Epoch 1 | Batch 50/94 | Batch Loss: 5.7159\n",
      "Epoch 1 | Batch 51/94 | Batch Loss: 5.7066\n",
      "Epoch 1 | Batch 52/94 | Batch Loss: 5.6551\n",
      "Epoch 1 | Batch 53/94 | Batch Loss: 5.6478\n",
      "Epoch 1 | Batch 54/94 | Batch Loss: 5.6256\n",
      "Epoch 1 | Batch 55/94 | Batch Loss: 5.6420\n",
      "Epoch 1 | Batch 56/94 | Batch Loss: 5.6313\n",
      "Epoch 1 | Batch 57/94 | Batch Loss: 5.5424\n",
      "Epoch 1 | Batch 58/94 | Batch Loss: 5.6271\n",
      "Epoch 1 | Batch 59/94 | Batch Loss: 5.5601\n",
      "Epoch 1 | Batch 60/94 | Batch Loss: 5.5834\n",
      "Epoch 1 | Batch 61/94 | Batch Loss: 5.5673\n",
      "Epoch 1 | Batch 62/94 | Batch Loss: 5.5384\n",
      "Epoch 1 | Batch 63/94 | Batch Loss: 5.5020\n",
      "Epoch 1 | Batch 64/94 | Batch Loss: 5.5277\n",
      "Epoch 1 | Batch 65/94 | Batch Loss: 5.4712\n",
      "Epoch 1 | Batch 66/94 | Batch Loss: 5.5009\n",
      "Epoch 1 | Batch 67/94 | Batch Loss: 5.4428\n",
      "Epoch 1 | Batch 68/94 | Batch Loss: 5.4703\n",
      "Epoch 1 | Batch 69/94 | Batch Loss: 5.4629\n",
      "Epoch 1 | Batch 70/94 | Batch Loss: 5.4396\n",
      "Epoch 1 | Batch 71/94 | Batch Loss: 5.3907\n",
      "Epoch 1 | Batch 72/94 | Batch Loss: 5.4044\n",
      "Epoch 1 | Batch 73/94 | Batch Loss: 5.3704\n",
      "Epoch 1 | Batch 74/94 | Batch Loss: 5.3736\n",
      "Epoch 1 | Batch 75/94 | Batch Loss: 5.3858\n",
      "Epoch 1 | Batch 76/94 | Batch Loss: 5.3086\n",
      "Epoch 1 | Batch 77/94 | Batch Loss: 5.3806\n",
      "Epoch 1 | Batch 78/94 | Batch Loss: 5.3455\n",
      "Epoch 1 | Batch 79/94 | Batch Loss: 5.3297\n",
      "Epoch 1 | Batch 80/94 | Batch Loss: 5.2691\n",
      "Epoch 1 | Batch 81/94 | Batch Loss: 5.2903\n",
      "Epoch 1 | Batch 82/94 | Batch Loss: 5.2804\n",
      "Epoch 1 | Batch 83/94 | Batch Loss: 5.2843\n",
      "Epoch 1 | Batch 84/94 | Batch Loss: 5.2988\n",
      "Epoch 1 | Batch 85/94 | Batch Loss: 5.2670\n",
      "Epoch 1 | Batch 86/94 | Batch Loss: 5.1528\n",
      "Epoch 1 | Batch 87/94 | Batch Loss: 5.2228\n",
      "Epoch 1 | Batch 88/94 | Batch Loss: 5.2364\n",
      "Epoch 1 | Batch 89/94 | Batch Loss: 5.2167\n",
      "Epoch 1 | Batch 90/94 | Batch Loss: 5.2691\n",
      "Epoch 1 | Batch 91/94 | Batch Loss: 5.2352\n",
      "Epoch 1 | Batch 92/94 | Batch Loss: 5.2472\n",
      "Epoch 1 | Batch 93/94 | Batch Loss: 5.1590\n",
      "Epoch 1 | Batch 94/94 | Batch Loss: 5.1042\n",
      "Epoch 1 Finished | Average Loss: 5.7835\n",
      "\n",
      "Epoch 2 Starting...\n",
      "Epoch 2 | Batch 1/94 | Batch Loss: 5.1541\n",
      "Epoch 2 | Batch 2/94 | Batch Loss: 5.1289\n",
      "Epoch 2 | Batch 3/94 | Batch Loss: 5.0381\n",
      "Epoch 2 | Batch 4/94 | Batch Loss: 5.1421\n",
      "Epoch 2 | Batch 5/94 | Batch Loss: 5.1222\n",
      "Epoch 2 | Batch 6/94 | Batch Loss: 5.0692\n",
      "Epoch 2 | Batch 7/94 | Batch Loss: 5.0809\n",
      "Epoch 2 | Batch 8/94 | Batch Loss: 5.0913\n",
      "Epoch 2 | Batch 9/94 | Batch Loss: 5.0681\n",
      "Epoch 2 | Batch 10/94 | Batch Loss: 5.0100\n",
      "Epoch 2 | Batch 11/94 | Batch Loss: 5.0677\n",
      "Epoch 2 | Batch 12/94 | Batch Loss: 5.0008\n",
      "Epoch 2 | Batch 13/94 | Batch Loss: 5.0502\n",
      "Epoch 2 | Batch 14/94 | Batch Loss: 4.9098\n",
      "Epoch 2 | Batch 15/94 | Batch Loss: 5.0577\n",
      "Epoch 2 | Batch 16/94 | Batch Loss: 4.9931\n",
      "Epoch 2 | Batch 17/94 | Batch Loss: 4.9488\n",
      "Epoch 2 | Batch 18/94 | Batch Loss: 5.0139\n",
      "Epoch 2 | Batch 19/94 | Batch Loss: 4.9547\n",
      "Epoch 2 | Batch 20/94 | Batch Loss: 4.9421\n",
      "Epoch 2 | Batch 21/94 | Batch Loss: 4.9520\n",
      "Epoch 2 | Batch 22/94 | Batch Loss: 4.9618\n",
      "Epoch 2 | Batch 23/94 | Batch Loss: 4.8711\n",
      "Epoch 2 | Batch 24/94 | Batch Loss: 4.9376\n",
      "Epoch 2 | Batch 25/94 | Batch Loss: 4.8364\n",
      "Epoch 2 | Batch 26/94 | Batch Loss: 4.9129\n",
      "Epoch 2 | Batch 27/94 | Batch Loss: 4.8265\n",
      "Epoch 2 | Batch 28/94 | Batch Loss: 4.8363\n",
      "Epoch 2 | Batch 29/94 | Batch Loss: 4.9138\n",
      "Epoch 2 | Batch 30/94 | Batch Loss: 4.8472\n",
      "Epoch 2 | Batch 31/94 | Batch Loss: 4.8657\n",
      "Epoch 2 | Batch 32/94 | Batch Loss: 4.8834\n",
      "Epoch 2 | Batch 33/94 | Batch Loss: 4.8819\n",
      "Epoch 2 | Batch 34/94 | Batch Loss: 4.8784\n",
      "Epoch 2 | Batch 35/94 | Batch Loss: 4.8162\n",
      "Epoch 2 | Batch 36/94 | Batch Loss: 4.8163\n",
      "Epoch 2 | Batch 37/94 | Batch Loss: 4.8326\n",
      "Epoch 2 | Batch 38/94 | Batch Loss: 4.8746\n",
      "Epoch 2 | Batch 39/94 | Batch Loss: 4.7901\n",
      "Epoch 2 | Batch 40/94 | Batch Loss: 4.7428\n",
      "Epoch 2 | Batch 41/94 | Batch Loss: 4.8381\n",
      "Epoch 2 | Batch 42/94 | Batch Loss: 4.8350\n",
      "Epoch 2 | Batch 43/94 | Batch Loss: 4.7231\n",
      "Epoch 2 | Batch 44/94 | Batch Loss: 4.8491\n",
      "Epoch 2 | Batch 45/94 | Batch Loss: 4.8192\n",
      "Epoch 2 | Batch 46/94 | Batch Loss: 4.7548\n",
      "Epoch 2 | Batch 47/94 | Batch Loss: 4.7321\n",
      "Epoch 2 | Batch 48/94 | Batch Loss: 4.8477\n",
      "Epoch 2 | Batch 49/94 | Batch Loss: 4.7525\n",
      "Epoch 2 | Batch 50/94 | Batch Loss: 4.7613\n",
      "Epoch 2 | Batch 51/94 | Batch Loss: 4.6116\n",
      "Epoch 2 | Batch 52/94 | Batch Loss: 4.7482\n",
      "Epoch 2 | Batch 53/94 | Batch Loss: 4.7070\n",
      "Epoch 2 | Batch 54/94 | Batch Loss: 4.6725\n",
      "Epoch 2 | Batch 55/94 | Batch Loss: 4.6807\n",
      "Epoch 2 | Batch 56/94 | Batch Loss: 4.7275\n",
      "Epoch 2 | Batch 57/94 | Batch Loss: 4.6778\n",
      "Epoch 2 | Batch 58/94 | Batch Loss: 4.7486\n",
      "Epoch 2 | Batch 59/94 | Batch Loss: 4.6909\n",
      "Epoch 2 | Batch 60/94 | Batch Loss: 4.7269\n",
      "Epoch 2 | Batch 61/94 | Batch Loss: 4.6776\n",
      "Epoch 2 | Batch 62/94 | Batch Loss: 4.5498\n",
      "Epoch 2 | Batch 63/94 | Batch Loss: 4.6057\n",
      "Epoch 2 | Batch 64/94 | Batch Loss: 4.5885\n",
      "Epoch 2 | Batch 65/94 | Batch Loss: 4.6819\n",
      "Epoch 2 | Batch 66/94 | Batch Loss: 4.5456\n",
      "Epoch 2 | Batch 67/94 | Batch Loss: 4.5978\n",
      "Epoch 2 | Batch 68/94 | Batch Loss: 4.6781\n",
      "Epoch 2 | Batch 69/94 | Batch Loss: 4.5417\n",
      "Epoch 2 | Batch 70/94 | Batch Loss: 4.5741\n",
      "Epoch 2 | Batch 71/94 | Batch Loss: 4.5185\n",
      "Epoch 2 | Batch 72/94 | Batch Loss: 4.6496\n",
      "Epoch 2 | Batch 73/94 | Batch Loss: 4.6507\n",
      "Epoch 2 | Batch 74/94 | Batch Loss: 4.5680\n",
      "Epoch 2 | Batch 75/94 | Batch Loss: 4.5421\n",
      "Epoch 2 | Batch 76/94 | Batch Loss: 4.5612\n",
      "Epoch 2 | Batch 77/94 | Batch Loss: 4.6377\n",
      "Epoch 2 | Batch 78/94 | Batch Loss: 4.5416\n",
      "Epoch 2 | Batch 79/94 | Batch Loss: 4.5718\n",
      "Epoch 2 | Batch 80/94 | Batch Loss: 4.5434\n",
      "Epoch 2 | Batch 81/94 | Batch Loss: 4.5725\n",
      "Epoch 2 | Batch 82/94 | Batch Loss: 4.5519\n",
      "Epoch 2 | Batch 83/94 | Batch Loss: 4.5067\n",
      "Epoch 2 | Batch 84/94 | Batch Loss: 4.5261\n",
      "Epoch 2 | Batch 85/94 | Batch Loss: 4.5776\n",
      "Epoch 2 | Batch 86/94 | Batch Loss: 4.5318\n",
      "Epoch 2 | Batch 87/94 | Batch Loss: 4.4404\n",
      "Epoch 2 | Batch 88/94 | Batch Loss: 4.4993\n",
      "Epoch 2 | Batch 89/94 | Batch Loss: 4.4610\n",
      "Epoch 2 | Batch 90/94 | Batch Loss: 4.5484\n",
      "Epoch 2 | Batch 91/94 | Batch Loss: 4.4934\n",
      "Epoch 2 | Batch 92/94 | Batch Loss: 4.3053\n",
      "Epoch 2 | Batch 93/94 | Batch Loss: 4.4930\n",
      "Epoch 2 | Batch 94/94 | Batch Loss: 4.4700\n",
      "Epoch 2 Finished | Average Loss: 4.7641\n",
      "\n",
      "Epoch 3 Starting...\n",
      "Epoch 3 | Batch 1/94 | Batch Loss: 4.4236\n",
      "Epoch 3 | Batch 2/94 | Batch Loss: 4.4127\n",
      "Epoch 3 | Batch 3/94 | Batch Loss: 4.4659\n",
      "Epoch 3 | Batch 4/94 | Batch Loss: 4.3339\n",
      "Epoch 3 | Batch 5/94 | Batch Loss: 4.4456\n",
      "Epoch 3 | Batch 6/94 | Batch Loss: 4.4211\n",
      "Epoch 3 | Batch 7/94 | Batch Loss: 4.4161\n",
      "Epoch 3 | Batch 8/94 | Batch Loss: 4.4074\n",
      "Epoch 3 | Batch 9/94 | Batch Loss: 4.4027\n",
      "Epoch 3 | Batch 10/94 | Batch Loss: 4.4861\n",
      "Epoch 3 | Batch 11/94 | Batch Loss: 4.2580\n",
      "Epoch 3 | Batch 12/94 | Batch Loss: 4.3593\n",
      "Epoch 3 | Batch 13/94 | Batch Loss: 4.3593\n",
      "Epoch 3 | Batch 14/94 | Batch Loss: 4.2794\n",
      "Epoch 3 | Batch 15/94 | Batch Loss: 4.3933\n",
      "Epoch 3 | Batch 16/94 | Batch Loss: 4.3852\n",
      "Epoch 3 | Batch 17/94 | Batch Loss: 4.4109\n",
      "Epoch 3 | Batch 18/94 | Batch Loss: 4.3424\n",
      "Epoch 3 | Batch 19/94 | Batch Loss: 4.3843\n",
      "Epoch 3 | Batch 20/94 | Batch Loss: 4.4013\n",
      "Epoch 3 | Batch 21/94 | Batch Loss: 4.3121\n",
      "Epoch 3 | Batch 22/94 | Batch Loss: 4.3488\n",
      "Epoch 3 | Batch 23/94 | Batch Loss: 4.3400\n",
      "Epoch 3 | Batch 24/94 | Batch Loss: 4.3560\n",
      "Epoch 3 | Batch 25/94 | Batch Loss: 4.3346\n",
      "Epoch 3 | Batch 26/94 | Batch Loss: 4.3982\n",
      "Epoch 3 | Batch 27/94 | Batch Loss: 4.2543\n",
      "Epoch 3 | Batch 28/94 | Batch Loss: 4.3011\n",
      "Epoch 3 | Batch 29/94 | Batch Loss: 4.3543\n",
      "Epoch 3 | Batch 30/94 | Batch Loss: 4.2991\n",
      "Epoch 3 | Batch 31/94 | Batch Loss: 4.3316\n",
      "Epoch 3 | Batch 32/94 | Batch Loss: 4.3414\n",
      "Epoch 3 | Batch 33/94 | Batch Loss: 4.2726\n",
      "Epoch 3 | Batch 34/94 | Batch Loss: 4.3471\n",
      "Epoch 3 | Batch 35/94 | Batch Loss: 4.2711\n",
      "Epoch 3 | Batch 36/94 | Batch Loss: 4.2775\n",
      "Epoch 3 | Batch 37/94 | Batch Loss: 4.2592\n",
      "Epoch 3 | Batch 38/94 | Batch Loss: 4.2728\n",
      "Epoch 3 | Batch 39/94 | Batch Loss: 4.3248\n",
      "Epoch 3 | Batch 40/94 | Batch Loss: 4.3275\n",
      "Epoch 3 | Batch 41/94 | Batch Loss: 4.2292\n",
      "Epoch 3 | Batch 42/94 | Batch Loss: 4.2236\n",
      "Epoch 3 | Batch 43/94 | Batch Loss: 4.3127\n",
      "Epoch 3 | Batch 44/94 | Batch Loss: 4.2748\n",
      "Epoch 3 | Batch 45/94 | Batch Loss: 4.2424\n",
      "Epoch 3 | Batch 46/94 | Batch Loss: 4.2799\n",
      "Epoch 3 | Batch 47/94 | Batch Loss: 4.2316\n",
      "Epoch 3 | Batch 48/94 | Batch Loss: 4.1978\n",
      "Epoch 3 | Batch 49/94 | Batch Loss: 4.1864\n",
      "Epoch 3 | Batch 50/94 | Batch Loss: 4.2576\n",
      "Epoch 3 | Batch 51/94 | Batch Loss: 4.2621\n",
      "Epoch 3 | Batch 52/94 | Batch Loss: 4.2675\n",
      "Epoch 3 | Batch 53/94 | Batch Loss: 4.0996\n",
      "Epoch 3 | Batch 54/94 | Batch Loss: 4.1403\n",
      "Epoch 3 | Batch 55/94 | Batch Loss: 4.2466\n",
      "Epoch 3 | Batch 56/94 | Batch Loss: 4.2683\n",
      "Epoch 3 | Batch 57/94 | Batch Loss: 4.2672\n",
      "Epoch 3 | Batch 58/94 | Batch Loss: 4.3094\n",
      "Epoch 3 | Batch 59/94 | Batch Loss: 4.2841\n",
      "Epoch 3 | Batch 60/94 | Batch Loss: 4.1349\n",
      "Epoch 3 | Batch 61/94 | Batch Loss: 4.1595\n",
      "Epoch 3 | Batch 62/94 | Batch Loss: 4.1798\n",
      "Epoch 3 | Batch 63/94 | Batch Loss: 4.1093\n",
      "Epoch 3 | Batch 64/94 | Batch Loss: 4.1228\n",
      "Epoch 3 | Batch 65/94 | Batch Loss: 4.1429\n",
      "Epoch 3 | Batch 66/94 | Batch Loss: 4.1914\n",
      "Epoch 3 | Batch 67/94 | Batch Loss: 4.1473\n",
      "Epoch 3 | Batch 68/94 | Batch Loss: 4.1604\n",
      "Epoch 3 | Batch 69/94 | Batch Loss: 4.2144\n",
      "Epoch 3 | Batch 70/94 | Batch Loss: 4.1951\n",
      "Epoch 3 | Batch 71/94 | Batch Loss: 4.2044\n",
      "Epoch 3 | Batch 72/94 | Batch Loss: 4.1708\n",
      "Epoch 3 | Batch 73/94 | Batch Loss: 4.2017\n",
      "Epoch 3 | Batch 74/94 | Batch Loss: 4.0583\n",
      "Epoch 3 | Batch 75/94 | Batch Loss: 4.1457\n",
      "Epoch 3 | Batch 76/94 | Batch Loss: 4.0541\n",
      "Epoch 3 | Batch 77/94 | Batch Loss: 4.0770\n",
      "Epoch 3 | Batch 78/94 | Batch Loss: 4.1789\n",
      "Epoch 3 | Batch 79/94 | Batch Loss: 4.1712\n",
      "Epoch 3 | Batch 80/94 | Batch Loss: 4.1061\n",
      "Epoch 3 | Batch 81/94 | Batch Loss: 4.1154\n",
      "Epoch 3 | Batch 82/94 | Batch Loss: 4.1815\n",
      "Epoch 3 | Batch 83/94 | Batch Loss: 4.2054\n",
      "Epoch 3 | Batch 84/94 | Batch Loss: 4.0728\n",
      "Epoch 3 | Batch 85/94 | Batch Loss: 4.1493\n",
      "Epoch 3 | Batch 86/94 | Batch Loss: 4.0830\n",
      "Epoch 3 | Batch 87/94 | Batch Loss: 4.1093\n",
      "Epoch 3 | Batch 88/94 | Batch Loss: 4.1483\n",
      "Epoch 3 | Batch 89/94 | Batch Loss: 4.1278\n",
      "Epoch 3 | Batch 90/94 | Batch Loss: 3.9983\n",
      "Epoch 3 | Batch 91/94 | Batch Loss: 4.0702\n",
      "Epoch 3 | Batch 92/94 | Batch Loss: 4.1550\n",
      "Epoch 3 | Batch 93/94 | Batch Loss: 4.0608\n",
      "Epoch 3 | Batch 94/94 | Batch Loss: 4.2059\n",
      "Epoch 3 Finished | Average Loss: 4.2500\n",
      "\n",
      "Epoch 4 Starting...\n",
      "Epoch 4 | Batch 1/94 | Batch Loss: 4.1178\n",
      "Epoch 4 | Batch 2/94 | Batch Loss: 4.0447\n",
      "Epoch 4 | Batch 3/94 | Batch Loss: 4.1046\n",
      "Epoch 4 | Batch 4/94 | Batch Loss: 4.1342\n",
      "Epoch 4 | Batch 5/94 | Batch Loss: 4.0421\n",
      "Epoch 4 | Batch 6/94 | Batch Loss: 4.1205\n",
      "Epoch 4 | Batch 7/94 | Batch Loss: 3.9514\n",
      "Epoch 4 | Batch 8/94 | Batch Loss: 4.0255\n",
      "Epoch 4 | Batch 9/94 | Batch Loss: 4.0607\n",
      "Epoch 4 | Batch 10/94 | Batch Loss: 4.0897\n",
      "Epoch 4 | Batch 11/94 | Batch Loss: 3.8988\n",
      "Epoch 4 | Batch 12/94 | Batch Loss: 4.0764\n",
      "Epoch 4 | Batch 13/94 | Batch Loss: 3.9541\n",
      "Epoch 4 | Batch 14/94 | Batch Loss: 3.9743\n",
      "Epoch 4 | Batch 15/94 | Batch Loss: 4.0086\n",
      "Epoch 4 | Batch 16/94 | Batch Loss: 3.9575\n",
      "Epoch 4 | Batch 17/94 | Batch Loss: 3.9955\n",
      "Epoch 4 | Batch 18/94 | Batch Loss: 4.0258\n",
      "Epoch 4 | Batch 19/94 | Batch Loss: 4.0589\n",
      "Epoch 4 | Batch 20/94 | Batch Loss: 4.0669\n",
      "Epoch 4 | Batch 21/94 | Batch Loss: 3.9923\n",
      "Epoch 4 | Batch 22/94 | Batch Loss: 3.9836\n",
      "Epoch 4 | Batch 23/94 | Batch Loss: 3.9916\n",
      "Epoch 4 | Batch 24/94 | Batch Loss: 3.9818\n",
      "Epoch 4 | Batch 25/94 | Batch Loss: 3.9779\n",
      "Epoch 4 | Batch 26/94 | Batch Loss: 4.0469\n",
      "Epoch 4 | Batch 27/94 | Batch Loss: 3.9742\n",
      "Epoch 4 | Batch 28/94 | Batch Loss: 3.8854\n",
      "Epoch 4 | Batch 29/94 | Batch Loss: 3.9627\n",
      "Epoch 4 | Batch 30/94 | Batch Loss: 3.9089\n",
      "Epoch 4 | Batch 31/94 | Batch Loss: 3.9665\n",
      "Epoch 4 | Batch 32/94 | Batch Loss: 3.8449\n",
      "Epoch 4 | Batch 33/94 | Batch Loss: 3.9623\n",
      "Epoch 4 | Batch 34/94 | Batch Loss: 3.9417\n",
      "Epoch 4 | Batch 35/94 | Batch Loss: 4.0173\n",
      "Epoch 4 | Batch 36/94 | Batch Loss: 3.8808\n",
      "Epoch 4 | Batch 37/94 | Batch Loss: 3.8303\n",
      "Epoch 4 | Batch 38/94 | Batch Loss: 3.8833\n",
      "Epoch 4 | Batch 39/94 | Batch Loss: 3.8994\n",
      "Epoch 4 | Batch 40/94 | Batch Loss: 3.9561\n",
      "Epoch 4 | Batch 41/94 | Batch Loss: 3.8813\n",
      "Epoch 4 | Batch 42/94 | Batch Loss: 3.9539\n",
      "Epoch 4 | Batch 43/94 | Batch Loss: 4.0552\n",
      "Epoch 4 | Batch 44/94 | Batch Loss: 3.9352\n",
      "Epoch 4 | Batch 45/94 | Batch Loss: 3.9994\n",
      "Epoch 4 | Batch 46/94 | Batch Loss: 3.9162\n",
      "Epoch 4 | Batch 47/94 | Batch Loss: 4.0146\n",
      "Epoch 4 | Batch 48/94 | Batch Loss: 3.9815\n",
      "Epoch 4 | Batch 49/94 | Batch Loss: 3.8112\n",
      "Epoch 4 | Batch 50/94 | Batch Loss: 3.8675\n",
      "Epoch 4 | Batch 51/94 | Batch Loss: 3.8718\n",
      "Epoch 4 | Batch 52/94 | Batch Loss: 3.9758\n",
      "Epoch 4 | Batch 53/94 | Batch Loss: 3.7986\n",
      "Epoch 4 | Batch 54/94 | Batch Loss: 3.9591\n",
      "Epoch 4 | Batch 55/94 | Batch Loss: 3.8895\n",
      "Epoch 4 | Batch 56/94 | Batch Loss: 3.8965\n",
      "Epoch 4 | Batch 57/94 | Batch Loss: 3.8992\n",
      "Epoch 4 | Batch 58/94 | Batch Loss: 3.9505\n",
      "Epoch 4 | Batch 59/94 | Batch Loss: 3.9266\n",
      "Epoch 4 | Batch 60/94 | Batch Loss: 3.7750\n",
      "Epoch 4 | Batch 61/94 | Batch Loss: 3.8825\n",
      "Epoch 4 | Batch 62/94 | Batch Loss: 3.9229\n",
      "Epoch 4 | Batch 63/94 | Batch Loss: 3.9450\n",
      "Epoch 4 | Batch 64/94 | Batch Loss: 3.9261\n",
      "Epoch 4 | Batch 65/94 | Batch Loss: 3.8901\n",
      "Epoch 4 | Batch 66/94 | Batch Loss: 3.9099\n",
      "Epoch 4 | Batch 67/94 | Batch Loss: 3.7589\n",
      "Epoch 4 | Batch 68/94 | Batch Loss: 3.9155\n",
      "Epoch 4 | Batch 69/94 | Batch Loss: 3.7820\n",
      "Epoch 4 | Batch 70/94 | Batch Loss: 3.8317\n",
      "Epoch 4 | Batch 71/94 | Batch Loss: 3.8490\n",
      "Epoch 4 | Batch 72/94 | Batch Loss: 3.8440\n",
      "Epoch 4 | Batch 73/94 | Batch Loss: 3.8542\n",
      "Epoch 4 | Batch 74/94 | Batch Loss: 3.8614\n",
      "Epoch 4 | Batch 75/94 | Batch Loss: 3.8668\n",
      "Epoch 4 | Batch 76/94 | Batch Loss: 3.8562\n",
      "Epoch 4 | Batch 77/94 | Batch Loss: 3.8495\n",
      "Epoch 4 | Batch 78/94 | Batch Loss: 4.0009\n",
      "Epoch 4 | Batch 79/94 | Batch Loss: 3.9333\n",
      "Epoch 4 | Batch 80/94 | Batch Loss: 3.8802\n",
      "Epoch 4 | Batch 81/94 | Batch Loss: 3.7694\n",
      "Epoch 4 | Batch 82/94 | Batch Loss: 3.8954\n",
      "Epoch 4 | Batch 83/94 | Batch Loss: 3.8577\n",
      "Epoch 4 | Batch 84/94 | Batch Loss: 3.8695\n",
      "Epoch 4 | Batch 85/94 | Batch Loss: 3.8254\n",
      "Epoch 4 | Batch 86/94 | Batch Loss: 3.8077\n",
      "Epoch 4 | Batch 87/94 | Batch Loss: 3.8532\n",
      "Epoch 4 | Batch 88/94 | Batch Loss: 3.8312\n",
      "Epoch 4 | Batch 89/94 | Batch Loss: 3.7597\n",
      "Epoch 4 | Batch 90/94 | Batch Loss: 3.8100\n",
      "Epoch 4 | Batch 91/94 | Batch Loss: 3.7760\n",
      "Epoch 4 | Batch 92/94 | Batch Loss: 3.7517\n",
      "Epoch 4 | Batch 93/94 | Batch Loss: 3.8284\n",
      "Epoch 4 | Batch 94/94 | Batch Loss: 3.8055\n",
      "Epoch 4 Finished | Average Loss: 3.9251\n",
      "\n",
      "Epoch 5 Starting...\n",
      "Epoch 5 | Batch 1/94 | Batch Loss: 3.8253\n",
      "Epoch 5 | Batch 2/94 | Batch Loss: 3.7268\n",
      "Epoch 5 | Batch 3/94 | Batch Loss: 3.7642\n",
      "Epoch 5 | Batch 4/94 | Batch Loss: 3.6758\n",
      "Epoch 5 | Batch 5/94 | Batch Loss: 3.7750\n",
      "Epoch 5 | Batch 6/94 | Batch Loss: 3.7154\n",
      "Epoch 5 | Batch 7/94 | Batch Loss: 3.8472\n",
      "Epoch 5 | Batch 8/94 | Batch Loss: 3.7153\n",
      "Epoch 5 | Batch 9/94 | Batch Loss: 3.7697\n",
      "Epoch 5 | Batch 10/94 | Batch Loss: 3.8001\n",
      "Epoch 5 | Batch 11/94 | Batch Loss: 3.7844\n",
      "Epoch 5 | Batch 12/94 | Batch Loss: 3.7206\n",
      "Epoch 5 | Batch 13/94 | Batch Loss: 3.7901\n",
      "Epoch 5 | Batch 14/94 | Batch Loss: 3.8894\n",
      "Epoch 5 | Batch 15/94 | Batch Loss: 3.6794\n",
      "Epoch 5 | Batch 16/94 | Batch Loss: 3.8443\n",
      "Epoch 5 | Batch 17/94 | Batch Loss: 3.7336\n",
      "Epoch 5 | Batch 18/94 | Batch Loss: 3.7653\n",
      "Epoch 5 | Batch 19/94 | Batch Loss: 3.7274\n",
      "Epoch 5 | Batch 20/94 | Batch Loss: 3.7456\n",
      "Epoch 5 | Batch 21/94 | Batch Loss: 3.7850\n",
      "Epoch 5 | Batch 22/94 | Batch Loss: 3.7287\n",
      "Epoch 5 | Batch 23/94 | Batch Loss: 3.7635\n",
      "Epoch 5 | Batch 24/94 | Batch Loss: 3.7338\n",
      "Epoch 5 | Batch 25/94 | Batch Loss: 3.7076\n",
      "Epoch 5 | Batch 26/94 | Batch Loss: 3.7885\n",
      "Epoch 5 | Batch 27/94 | Batch Loss: 3.8041\n",
      "Epoch 5 | Batch 28/94 | Batch Loss: 3.8250\n",
      "Epoch 5 | Batch 29/94 | Batch Loss: 3.6568\n",
      "Epoch 5 | Batch 30/94 | Batch Loss: 3.6860\n",
      "Epoch 5 | Batch 31/94 | Batch Loss: 3.7322\n",
      "Epoch 5 | Batch 32/94 | Batch Loss: 3.7104\n",
      "Epoch 5 | Batch 33/94 | Batch Loss: 3.6207\n",
      "Epoch 5 | Batch 34/94 | Batch Loss: 3.7818\n",
      "Epoch 5 | Batch 35/94 | Batch Loss: 3.7174\n",
      "Epoch 5 | Batch 36/94 | Batch Loss: 3.8052\n",
      "Epoch 5 | Batch 37/94 | Batch Loss: 3.6893\n",
      "Epoch 5 | Batch 38/94 | Batch Loss: 3.7386\n",
      "Epoch 5 | Batch 39/94 | Batch Loss: 3.6892\n",
      "Epoch 5 | Batch 40/94 | Batch Loss: 3.6446\n",
      "Epoch 5 | Batch 41/94 | Batch Loss: 3.5892\n",
      "Epoch 5 | Batch 42/94 | Batch Loss: 3.7994\n",
      "Epoch 5 | Batch 43/94 | Batch Loss: 3.6247\n",
      "Epoch 5 | Batch 44/94 | Batch Loss: 3.6921\n",
      "Epoch 5 | Batch 45/94 | Batch Loss: 3.7065\n",
      "Epoch 5 | Batch 46/94 | Batch Loss: 3.8217\n",
      "Epoch 5 | Batch 47/94 | Batch Loss: 3.7294\n",
      "Epoch 5 | Batch 48/94 | Batch Loss: 3.6570\n",
      "Epoch 5 | Batch 49/94 | Batch Loss: 3.7606\n",
      "Epoch 5 | Batch 50/94 | Batch Loss: 3.6685\n",
      "Epoch 5 | Batch 51/94 | Batch Loss: 3.6803\n",
      "Epoch 5 | Batch 52/94 | Batch Loss: 3.6476\n",
      "Epoch 5 | Batch 53/94 | Batch Loss: 3.6286\n",
      "Epoch 5 | Batch 54/94 | Batch Loss: 3.6826\n",
      "Epoch 5 | Batch 55/94 | Batch Loss: 3.7576\n",
      "Epoch 5 | Batch 56/94 | Batch Loss: 3.6703\n",
      "Epoch 5 | Batch 57/94 | Batch Loss: 3.6021\n",
      "Epoch 5 | Batch 58/94 | Batch Loss: 3.7462\n",
      "Epoch 5 | Batch 59/94 | Batch Loss: 3.6889\n",
      "Epoch 5 | Batch 60/94 | Batch Loss: 3.7338\n",
      "Epoch 5 | Batch 61/94 | Batch Loss: 3.6090\n",
      "Epoch 5 | Batch 62/94 | Batch Loss: 3.6834\n",
      "Epoch 5 | Batch 63/94 | Batch Loss: 3.6985\n",
      "Epoch 5 | Batch 64/94 | Batch Loss: 3.7536\n",
      "Epoch 5 | Batch 65/94 | Batch Loss: 3.6622\n",
      "Epoch 5 | Batch 66/94 | Batch Loss: 3.5896\n",
      "Epoch 5 | Batch 67/94 | Batch Loss: 3.6285\n",
      "Epoch 5 | Batch 68/94 | Batch Loss: 3.6382\n",
      "Epoch 5 | Batch 69/94 | Batch Loss: 3.5533\n",
      "Epoch 5 | Batch 70/94 | Batch Loss: 3.6394\n",
      "Epoch 5 | Batch 71/94 | Batch Loss: 3.6332\n",
      "Epoch 5 | Batch 72/94 | Batch Loss: 3.6267\n",
      "Epoch 5 | Batch 73/94 | Batch Loss: 3.6455\n",
      "Epoch 5 | Batch 74/94 | Batch Loss: 3.5961\n",
      "Epoch 5 | Batch 75/94 | Batch Loss: 3.6961\n",
      "Epoch 5 | Batch 76/94 | Batch Loss: 3.6117\n",
      "Epoch 5 | Batch 77/94 | Batch Loss: 3.7118\n",
      "Epoch 5 | Batch 78/94 | Batch Loss: 3.6586\n",
      "Epoch 5 | Batch 79/94 | Batch Loss: 3.5741\n",
      "Epoch 5 | Batch 80/94 | Batch Loss: 3.6398\n",
      "Epoch 5 | Batch 81/94 | Batch Loss: 3.5708\n",
      "Epoch 5 | Batch 82/94 | Batch Loss: 3.6389\n",
      "Epoch 5 | Batch 83/94 | Batch Loss: 3.5635\n",
      "Epoch 5 | Batch 84/94 | Batch Loss: 3.5693\n",
      "Epoch 5 | Batch 85/94 | Batch Loss: 3.6679\n",
      "Epoch 5 | Batch 86/94 | Batch Loss: 3.6285\n",
      "Epoch 5 | Batch 87/94 | Batch Loss: 3.5884\n",
      "Epoch 5 | Batch 88/94 | Batch Loss: 3.7353\n",
      "Epoch 5 | Batch 89/94 | Batch Loss: 3.6086\n",
      "Epoch 5 | Batch 90/94 | Batch Loss: 3.5923\n",
      "Epoch 5 | Batch 91/94 | Batch Loss: 3.4991\n",
      "Epoch 5 | Batch 92/94 | Batch Loss: 3.5833\n",
      "Epoch 5 | Batch 93/94 | Batch Loss: 3.7197\n",
      "Epoch 5 | Batch 94/94 | Batch Loss: 3.5812\n",
      "Epoch 5 Finished | Average Loss: 3.6935\n",
      "\n",
      "Epoch 6 Starting...\n",
      "Epoch 6 | Batch 1/94 | Batch Loss: 3.5643\n",
      "Epoch 6 | Batch 2/94 | Batch Loss: 3.6227\n",
      "Epoch 6 | Batch 3/94 | Batch Loss: 3.5747\n",
      "Epoch 6 | Batch 4/94 | Batch Loss: 3.5820\n",
      "Epoch 6 | Batch 5/94 | Batch Loss: 3.7224\n",
      "Epoch 6 | Batch 6/94 | Batch Loss: 3.4887\n",
      "Epoch 6 | Batch 7/94 | Batch Loss: 3.6539\n",
      "Epoch 6 | Batch 8/94 | Batch Loss: 3.5700\n",
      "Epoch 6 | Batch 9/94 | Batch Loss: 3.5208\n",
      "Epoch 6 | Batch 10/94 | Batch Loss: 3.6470\n",
      "Epoch 6 | Batch 11/94 | Batch Loss: 3.4224\n",
      "Epoch 6 | Batch 12/94 | Batch Loss: 3.5645\n",
      "Epoch 6 | Batch 13/94 | Batch Loss: 3.6456\n",
      "Epoch 6 | Batch 14/94 | Batch Loss: 3.5858\n",
      "Epoch 6 | Batch 15/94 | Batch Loss: 3.6064\n",
      "Epoch 6 | Batch 16/94 | Batch Loss: 3.5825\n",
      "Epoch 6 | Batch 17/94 | Batch Loss: 3.6083\n",
      "Epoch 6 | Batch 18/94 | Batch Loss: 3.5437\n",
      "Epoch 6 | Batch 19/94 | Batch Loss: 3.5536\n",
      "Epoch 6 | Batch 20/94 | Batch Loss: 3.6746\n",
      "Epoch 6 | Batch 21/94 | Batch Loss: 3.5226\n",
      "Epoch 6 | Batch 22/94 | Batch Loss: 3.3940\n",
      "Epoch 6 | Batch 23/94 | Batch Loss: 3.6229\n",
      "Epoch 6 | Batch 24/94 | Batch Loss: 3.4761\n",
      "Epoch 6 | Batch 25/94 | Batch Loss: 3.4694\n",
      "Epoch 6 | Batch 26/94 | Batch Loss: 3.4518\n",
      "Epoch 6 | Batch 27/94 | Batch Loss: 3.5474\n",
      "Epoch 6 | Batch 28/94 | Batch Loss: 3.5770\n",
      "Epoch 6 | Batch 29/94 | Batch Loss: 3.6176\n",
      "Epoch 6 | Batch 30/94 | Batch Loss: 3.5405\n",
      "Epoch 6 | Batch 31/94 | Batch Loss: 3.6334\n",
      "Epoch 6 | Batch 32/94 | Batch Loss: 3.4383\n",
      "Epoch 6 | Batch 33/94 | Batch Loss: 3.6069\n",
      "Epoch 6 | Batch 34/94 | Batch Loss: 3.5896\n",
      "Epoch 6 | Batch 35/94 | Batch Loss: 3.4510\n",
      "Epoch 6 | Batch 36/94 | Batch Loss: 3.5930\n",
      "Epoch 6 | Batch 37/94 | Batch Loss: 3.4671\n",
      "Epoch 6 | Batch 38/94 | Batch Loss: 3.6249\n",
      "Epoch 6 | Batch 39/94 | Batch Loss: 3.4742\n",
      "Epoch 6 | Batch 40/94 | Batch Loss: 3.3989\n",
      "Epoch 6 | Batch 41/94 | Batch Loss: 3.5506\n",
      "Epoch 6 | Batch 42/94 | Batch Loss: 3.4161\n",
      "Epoch 6 | Batch 43/94 | Batch Loss: 3.4652\n",
      "Epoch 6 | Batch 44/94 | Batch Loss: 3.6021\n",
      "Epoch 6 | Batch 45/94 | Batch Loss: 3.6434\n",
      "Epoch 6 | Batch 46/94 | Batch Loss: 3.5061\n",
      "Epoch 6 | Batch 47/94 | Batch Loss: 3.4608\n",
      "Epoch 6 | Batch 48/94 | Batch Loss: 3.5185\n",
      "Epoch 6 | Batch 49/94 | Batch Loss: 3.5592\n",
      "Epoch 6 | Batch 50/94 | Batch Loss: 3.3287\n",
      "Epoch 6 | Batch 51/94 | Batch Loss: 3.5611\n",
      "Epoch 6 | Batch 52/94 | Batch Loss: 3.5242\n",
      "Epoch 6 | Batch 53/94 | Batch Loss: 3.6017\n",
      "Epoch 6 | Batch 54/94 | Batch Loss: 3.4439\n",
      "Epoch 6 | Batch 55/94 | Batch Loss: 3.4370\n",
      "Epoch 6 | Batch 56/94 | Batch Loss: 3.5123\n",
      "Epoch 6 | Batch 57/94 | Batch Loss: 3.3926\n",
      "Epoch 6 | Batch 58/94 | Batch Loss: 3.4479\n",
      "Epoch 6 | Batch 59/94 | Batch Loss: 3.5750\n",
      "Epoch 6 | Batch 60/94 | Batch Loss: 3.5162\n",
      "Epoch 6 | Batch 61/94 | Batch Loss: 3.4709\n",
      "Epoch 6 | Batch 62/94 | Batch Loss: 3.4357\n",
      "Epoch 6 | Batch 63/94 | Batch Loss: 3.5500\n",
      "Epoch 6 | Batch 64/94 | Batch Loss: 3.4775\n",
      "Epoch 6 | Batch 65/94 | Batch Loss: 3.4933\n",
      "Epoch 6 | Batch 66/94 | Batch Loss: 3.4304\n",
      "Epoch 6 | Batch 67/94 | Batch Loss: 3.5364\n",
      "Epoch 6 | Batch 68/94 | Batch Loss: 3.4993\n",
      "Epoch 6 | Batch 69/94 | Batch Loss: 3.4350\n",
      "Epoch 6 | Batch 70/94 | Batch Loss: 3.4495\n",
      "Epoch 6 | Batch 71/94 | Batch Loss: 3.5032\n",
      "Epoch 6 | Batch 72/94 | Batch Loss: 3.5054\n",
      "Epoch 6 | Batch 73/94 | Batch Loss: 3.6373\n",
      "Epoch 6 | Batch 74/94 | Batch Loss: 3.4262\n",
      "Epoch 6 | Batch 75/94 | Batch Loss: 3.4940\n",
      "Epoch 6 | Batch 76/94 | Batch Loss: 3.5075\n",
      "Epoch 6 | Batch 77/94 | Batch Loss: 3.5258\n",
      "Epoch 6 | Batch 78/94 | Batch Loss: 3.5123\n",
      "Epoch 6 | Batch 79/94 | Batch Loss: 3.3393\n",
      "Epoch 6 | Batch 80/94 | Batch Loss: 3.4043\n",
      "Epoch 6 | Batch 81/94 | Batch Loss: 3.6039\n",
      "Epoch 6 | Batch 82/94 | Batch Loss: 3.4715\n",
      "Epoch 6 | Batch 83/94 | Batch Loss: 3.3688\n",
      "Epoch 6 | Batch 84/94 | Batch Loss: 3.5200\n",
      "Epoch 6 | Batch 85/94 | Batch Loss: 3.4207\n",
      "Epoch 6 | Batch 86/94 | Batch Loss: 3.5756\n",
      "Epoch 6 | Batch 87/94 | Batch Loss: 3.3008\n",
      "Epoch 6 | Batch 88/94 | Batch Loss: 3.3727\n",
      "Epoch 6 | Batch 89/94 | Batch Loss: 3.3672\n",
      "Epoch 6 | Batch 90/94 | Batch Loss: 3.3712\n",
      "Epoch 6 | Batch 91/94 | Batch Loss: 3.4471\n",
      "Epoch 6 | Batch 92/94 | Batch Loss: 3.5226\n",
      "Epoch 6 | Batch 93/94 | Batch Loss: 3.4625\n",
      "Epoch 6 | Batch 94/94 | Batch Loss: 3.4106\n",
      "Epoch 6 Finished | Average Loss: 3.5121\n",
      "\n",
      "Epoch 7 Starting...\n",
      "Epoch 7 | Batch 1/94 | Batch Loss: 3.5473\n",
      "Epoch 7 | Batch 2/94 | Batch Loss: 3.4165\n",
      "Epoch 7 | Batch 3/94 | Batch Loss: 3.3711\n",
      "Epoch 7 | Batch 4/94 | Batch Loss: 3.4593\n",
      "Epoch 7 | Batch 5/94 | Batch Loss: 3.3018\n",
      "Epoch 7 | Batch 6/94 | Batch Loss: 3.3992\n",
      "Epoch 7 | Batch 7/94 | Batch Loss: 3.3768\n",
      "Epoch 7 | Batch 8/94 | Batch Loss: 3.3251\n",
      "Epoch 7 | Batch 9/94 | Batch Loss: 3.3777\n",
      "Epoch 7 | Batch 10/94 | Batch Loss: 3.4525\n",
      "Epoch 7 | Batch 11/94 | Batch Loss: 3.3885\n",
      "Epoch 7 | Batch 12/94 | Batch Loss: 3.4596\n",
      "Epoch 7 | Batch 13/94 | Batch Loss: 3.3917\n",
      "Epoch 7 | Batch 14/94 | Batch Loss: 3.4032\n",
      "Epoch 7 | Batch 15/94 | Batch Loss: 3.3612\n",
      "Epoch 7 | Batch 16/94 | Batch Loss: 3.3692\n",
      "Epoch 7 | Batch 17/94 | Batch Loss: 3.4951\n",
      "Epoch 7 | Batch 18/94 | Batch Loss: 3.4393\n",
      "Epoch 7 | Batch 19/94 | Batch Loss: 3.3929\n",
      "Epoch 7 | Batch 20/94 | Batch Loss: 3.2900\n",
      "Epoch 7 | Batch 21/94 | Batch Loss: 3.4483\n",
      "Epoch 7 | Batch 22/94 | Batch Loss: 3.4693\n",
      "Epoch 7 | Batch 23/94 | Batch Loss: 3.4155\n",
      "Epoch 7 | Batch 24/94 | Batch Loss: 3.3843\n",
      "Epoch 7 | Batch 25/94 | Batch Loss: 3.3837\n",
      "Epoch 7 | Batch 26/94 | Batch Loss: 3.3708\n",
      "Epoch 7 | Batch 27/94 | Batch Loss: 3.3522\n",
      "Epoch 7 | Batch 28/94 | Batch Loss: 3.3589\n",
      "Epoch 7 | Batch 29/94 | Batch Loss: 3.3182\n",
      "Epoch 7 | Batch 30/94 | Batch Loss: 3.2926\n",
      "Epoch 7 | Batch 31/94 | Batch Loss: 3.4226\n",
      "Epoch 7 | Batch 32/94 | Batch Loss: 3.4756\n",
      "Epoch 7 | Batch 33/94 | Batch Loss: 3.3561\n",
      "Epoch 7 | Batch 34/94 | Batch Loss: 3.4006\n",
      "Epoch 7 | Batch 35/94 | Batch Loss: 3.3263\n",
      "Epoch 7 | Batch 36/94 | Batch Loss: 3.4051\n",
      "Epoch 7 | Batch 37/94 | Batch Loss: 3.2893\n",
      "Epoch 7 | Batch 38/94 | Batch Loss: 3.2991\n",
      "Epoch 7 | Batch 39/94 | Batch Loss: 3.4067\n",
      "Epoch 7 | Batch 40/94 | Batch Loss: 3.3018\n",
      "Epoch 7 | Batch 41/94 | Batch Loss: 3.2111\n",
      "Epoch 7 | Batch 42/94 | Batch Loss: 3.3652\n",
      "Epoch 7 | Batch 43/94 | Batch Loss: 3.2871\n",
      "Epoch 7 | Batch 44/94 | Batch Loss: 3.2731\n",
      "Epoch 7 | Batch 45/94 | Batch Loss: 3.4558\n",
      "Epoch 7 | Batch 46/94 | Batch Loss: 3.2512\n",
      "Epoch 7 | Batch 47/94 | Batch Loss: 3.4353\n",
      "Epoch 7 | Batch 48/94 | Batch Loss: 3.3055\n",
      "Epoch 7 | Batch 49/94 | Batch Loss: 3.3352\n",
      "Epoch 7 | Batch 50/94 | Batch Loss: 3.3262\n",
      "Epoch 7 | Batch 51/94 | Batch Loss: 3.4617\n",
      "Epoch 7 | Batch 52/94 | Batch Loss: 3.3986\n",
      "Epoch 7 | Batch 53/94 | Batch Loss: 3.3323\n",
      "Epoch 7 | Batch 54/94 | Batch Loss: 3.3881\n",
      "Epoch 7 | Batch 55/94 | Batch Loss: 3.3736\n",
      "Epoch 7 | Batch 56/94 | Batch Loss: 3.4106\n",
      "Epoch 7 | Batch 57/94 | Batch Loss: 3.3869\n",
      "Epoch 7 | Batch 58/94 | Batch Loss: 3.4904\n",
      "Epoch 7 | Batch 59/94 | Batch Loss: 3.5058\n",
      "Epoch 7 | Batch 60/94 | Batch Loss: 3.3763\n",
      "Epoch 7 | Batch 61/94 | Batch Loss: 3.3984\n",
      "Epoch 7 | Batch 62/94 | Batch Loss: 3.2600\n",
      "Epoch 7 | Batch 63/94 | Batch Loss: 3.3802\n",
      "Epoch 7 | Batch 64/94 | Batch Loss: 3.2515\n",
      "Epoch 7 | Batch 65/94 | Batch Loss: 3.3953\n",
      "Epoch 7 | Batch 66/94 | Batch Loss: 3.2968\n",
      "Epoch 7 | Batch 67/94 | Batch Loss: 3.4387\n",
      "Epoch 7 | Batch 68/94 | Batch Loss: 3.2410\n",
      "Epoch 7 | Batch 69/94 | Batch Loss: 3.3167\n",
      "Epoch 7 | Batch 70/94 | Batch Loss: 3.3389\n",
      "Epoch 7 | Batch 71/94 | Batch Loss: 3.3646\n",
      "Epoch 7 | Batch 72/94 | Batch Loss: 3.3342\n",
      "Epoch 7 | Batch 73/94 | Batch Loss: 3.2662\n",
      "Epoch 7 | Batch 74/94 | Batch Loss: 3.4038\n",
      "Epoch 7 | Batch 75/94 | Batch Loss: 3.3063\n",
      "Epoch 7 | Batch 76/94 | Batch Loss: 3.4296\n",
      "Epoch 7 | Batch 77/94 | Batch Loss: 3.1881\n",
      "Epoch 7 | Batch 78/94 | Batch Loss: 3.3212\n",
      "Epoch 7 | Batch 79/94 | Batch Loss: 3.3385\n",
      "Epoch 7 | Batch 80/94 | Batch Loss: 3.3095\n",
      "Epoch 7 | Batch 81/94 | Batch Loss: 3.2791\n",
      "Epoch 7 | Batch 82/94 | Batch Loss: 3.3065\n",
      "Epoch 7 | Batch 83/94 | Batch Loss: 3.2953\n",
      "Epoch 7 | Batch 84/94 | Batch Loss: 3.3810\n",
      "Epoch 7 | Batch 85/94 | Batch Loss: 3.4151\n",
      "Epoch 7 | Batch 86/94 | Batch Loss: 3.4675\n",
      "Epoch 7 | Batch 87/94 | Batch Loss: 3.3819\n",
      "Epoch 7 | Batch 88/94 | Batch Loss: 3.1405\n",
      "Epoch 7 | Batch 89/94 | Batch Loss: 3.2984\n",
      "Epoch 7 | Batch 90/94 | Batch Loss: 3.2909\n",
      "Epoch 7 | Batch 91/94 | Batch Loss: 3.2556\n",
      "Epoch 7 | Batch 92/94 | Batch Loss: 3.3052\n",
      "Epoch 7 | Batch 93/94 | Batch Loss: 3.3190\n",
      "Epoch 7 | Batch 94/94 | Batch Loss: 3.3748\n",
      "Epoch 7 Finished | Average Loss: 3.3612\n",
      "\n",
      "Epoch 8 Starting...\n",
      "Epoch 8 | Batch 1/94 | Batch Loss: 3.2240\n",
      "Epoch 8 | Batch 2/94 | Batch Loss: 3.1492\n",
      "Epoch 8 | Batch 3/94 | Batch Loss: 3.3764\n",
      "Epoch 8 | Batch 4/94 | Batch Loss: 3.3414\n",
      "Epoch 8 | Batch 5/94 | Batch Loss: 3.2993\n",
      "Epoch 8 | Batch 6/94 | Batch Loss: 3.2696\n",
      "Epoch 8 | Batch 7/94 | Batch Loss: 3.2775\n",
      "Epoch 8 | Batch 8/94 | Batch Loss: 3.2200\n",
      "Epoch 8 | Batch 9/94 | Batch Loss: 3.1924\n",
      "Epoch 8 | Batch 10/94 | Batch Loss: 3.1406\n",
      "Epoch 8 | Batch 11/94 | Batch Loss: 3.1721\n",
      "Epoch 8 | Batch 12/94 | Batch Loss: 3.2772\n",
      "Epoch 8 | Batch 13/94 | Batch Loss: 3.1812\n",
      "Epoch 8 | Batch 14/94 | Batch Loss: 3.3318\n",
      "Epoch 8 | Batch 15/94 | Batch Loss: 3.3403\n",
      "Epoch 8 | Batch 16/94 | Batch Loss: 3.1695\n",
      "Epoch 8 | Batch 17/94 | Batch Loss: 3.2687\n",
      "Epoch 8 | Batch 18/94 | Batch Loss: 3.2230\n",
      "Epoch 8 | Batch 19/94 | Batch Loss: 3.2913\n",
      "Epoch 8 | Batch 20/94 | Batch Loss: 3.1977\n",
      "Epoch 8 | Batch 21/94 | Batch Loss: 3.3633\n",
      "Epoch 8 | Batch 22/94 | Batch Loss: 3.2427\n",
      "Epoch 8 | Batch 23/94 | Batch Loss: 3.3214\n",
      "Epoch 8 | Batch 24/94 | Batch Loss: 3.2073\n",
      "Epoch 8 | Batch 25/94 | Batch Loss: 3.2700\n",
      "Epoch 8 | Batch 26/94 | Batch Loss: 3.3034\n",
      "Epoch 8 | Batch 27/94 | Batch Loss: 3.3245\n",
      "Epoch 8 | Batch 28/94 | Batch Loss: 3.1640\n",
      "Epoch 8 | Batch 29/94 | Batch Loss: 3.1817\n",
      "Epoch 8 | Batch 30/94 | Batch Loss: 3.2267\n",
      "Epoch 8 | Batch 31/94 | Batch Loss: 3.3526\n",
      "Epoch 8 | Batch 32/94 | Batch Loss: 3.1830\n",
      "Epoch 8 | Batch 33/94 | Batch Loss: 3.2161\n",
      "Epoch 8 | Batch 34/94 | Batch Loss: 3.2274\n",
      "Epoch 8 | Batch 35/94 | Batch Loss: 3.1859\n",
      "Epoch 8 | Batch 36/94 | Batch Loss: 3.2713\n",
      "Epoch 8 | Batch 37/94 | Batch Loss: 3.2579\n",
      "Epoch 8 | Batch 38/94 | Batch Loss: 3.2339\n",
      "Epoch 8 | Batch 39/94 | Batch Loss: 3.1968\n",
      "Epoch 8 | Batch 40/94 | Batch Loss: 3.2499\n",
      "Epoch 8 | Batch 41/94 | Batch Loss: 3.2884\n",
      "Epoch 8 | Batch 42/94 | Batch Loss: 3.1790\n",
      "Epoch 8 | Batch 43/94 | Batch Loss: 3.1674\n",
      "Epoch 8 | Batch 44/94 | Batch Loss: 3.1874\n",
      "Epoch 8 | Batch 45/94 | Batch Loss: 3.2956\n",
      "Epoch 8 | Batch 46/94 | Batch Loss: 3.2391\n",
      "Epoch 8 | Batch 47/94 | Batch Loss: 3.1680\n",
      "Epoch 8 | Batch 48/94 | Batch Loss: 3.1964\n",
      "Epoch 8 | Batch 49/94 | Batch Loss: 3.1480\n",
      "Epoch 8 | Batch 50/94 | Batch Loss: 3.2193\n",
      "Epoch 8 | Batch 51/94 | Batch Loss: 3.2147\n",
      "Epoch 8 | Batch 52/94 | Batch Loss: 3.1461\n",
      "Epoch 8 | Batch 53/94 | Batch Loss: 3.2718\n",
      "Epoch 8 | Batch 54/94 | Batch Loss: 3.1821\n",
      "Epoch 8 | Batch 55/94 | Batch Loss: 3.2955\n",
      "Epoch 8 | Batch 56/94 | Batch Loss: 3.1687\n",
      "Epoch 8 | Batch 57/94 | Batch Loss: 3.2000\n",
      "Epoch 8 | Batch 58/94 | Batch Loss: 3.2318\n",
      "Epoch 8 | Batch 59/94 | Batch Loss: 3.1621\n",
      "Epoch 8 | Batch 60/94 | Batch Loss: 3.1739\n",
      "Epoch 8 | Batch 61/94 | Batch Loss: 3.1923\n",
      "Epoch 8 | Batch 62/94 | Batch Loss: 3.3106\n",
      "Epoch 8 | Batch 63/94 | Batch Loss: 3.3094\n",
      "Epoch 8 | Batch 64/94 | Batch Loss: 3.1060\n",
      "Epoch 8 | Batch 65/94 | Batch Loss: 3.1597\n",
      "Epoch 8 | Batch 66/94 | Batch Loss: 3.1239\n",
      "Epoch 8 | Batch 67/94 | Batch Loss: 3.2694\n",
      "Epoch 8 | Batch 68/94 | Batch Loss: 3.3137\n",
      "Epoch 8 | Batch 69/94 | Batch Loss: 3.1604\n",
      "Epoch 8 | Batch 70/94 | Batch Loss: 2.9891\n",
      "Epoch 8 | Batch 71/94 | Batch Loss: 3.3430\n",
      "Epoch 8 | Batch 72/94 | Batch Loss: 3.2812\n",
      "Epoch 8 | Batch 73/94 | Batch Loss: 3.2909\n",
      "Epoch 8 | Batch 74/94 | Batch Loss: 3.1244\n",
      "Epoch 8 | Batch 75/94 | Batch Loss: 3.3106\n",
      "Epoch 8 | Batch 76/94 | Batch Loss: 3.2615\n",
      "Epoch 8 | Batch 77/94 | Batch Loss: 3.1952\n",
      "Epoch 8 | Batch 78/94 | Batch Loss: 3.1877\n",
      "Epoch 8 | Batch 79/94 | Batch Loss: 3.2552\n",
      "Epoch 8 | Batch 80/94 | Batch Loss: 3.2623\n",
      "Epoch 8 | Batch 81/94 | Batch Loss: 3.2262\n",
      "Epoch 8 | Batch 82/94 | Batch Loss: 3.2108\n",
      "Epoch 8 | Batch 83/94 | Batch Loss: 3.2555\n",
      "Epoch 8 | Batch 84/94 | Batch Loss: 3.0520\n",
      "Epoch 8 | Batch 85/94 | Batch Loss: 3.2606\n",
      "Epoch 8 | Batch 86/94 | Batch Loss: 3.1929\n",
      "Epoch 8 | Batch 87/94 | Batch Loss: 3.1991\n",
      "Epoch 8 | Batch 88/94 | Batch Loss: 3.2546\n",
      "Epoch 8 | Batch 89/94 | Batch Loss: 3.1545\n",
      "Epoch 8 | Batch 90/94 | Batch Loss: 3.1857\n",
      "Epoch 8 | Batch 91/94 | Batch Loss: 3.1782\n",
      "Epoch 8 | Batch 92/94 | Batch Loss: 3.1263\n",
      "Epoch 8 | Batch 93/94 | Batch Loss: 3.0539\n",
      "Epoch 8 | Batch 94/94 | Batch Loss: 3.1118\n",
      "Epoch 8 Finished | Average Loss: 3.2231\n",
      "\n",
      "Epoch 9 Starting...\n",
      "Epoch 9 | Batch 1/94 | Batch Loss: 3.0804\n",
      "Epoch 9 | Batch 2/94 | Batch Loss: 3.0962\n",
      "Epoch 9 | Batch 3/94 | Batch Loss: 3.1779\n",
      "Epoch 9 | Batch 4/94 | Batch Loss: 3.2255\n",
      "Epoch 9 | Batch 5/94 | Batch Loss: 3.1993\n",
      "Epoch 9 | Batch 6/94 | Batch Loss: 3.2412\n",
      "Epoch 9 | Batch 7/94 | Batch Loss: 3.1453\n",
      "Epoch 9 | Batch 8/94 | Batch Loss: 3.1949\n",
      "Epoch 9 | Batch 9/94 | Batch Loss: 3.1205\n",
      "Epoch 9 | Batch 10/94 | Batch Loss: 3.2386\n",
      "Epoch 9 | Batch 11/94 | Batch Loss: 3.1411\n",
      "Epoch 9 | Batch 12/94 | Batch Loss: 3.0882\n",
      "Epoch 9 | Batch 13/94 | Batch Loss: 3.0183\n",
      "Epoch 9 | Batch 14/94 | Batch Loss: 2.9853\n",
      "Epoch 9 | Batch 15/94 | Batch Loss: 3.0599\n",
      "Epoch 9 | Batch 16/94 | Batch Loss: 3.0166\n",
      "Epoch 9 | Batch 17/94 | Batch Loss: 3.2224\n",
      "Epoch 9 | Batch 18/94 | Batch Loss: 3.2196\n",
      "Epoch 9 | Batch 19/94 | Batch Loss: 3.2259\n",
      "Epoch 9 | Batch 20/94 | Batch Loss: 3.1114\n",
      "Epoch 9 | Batch 21/94 | Batch Loss: 3.1033\n",
      "Epoch 9 | Batch 22/94 | Batch Loss: 3.1809\n",
      "Epoch 9 | Batch 23/94 | Batch Loss: 3.1547\n",
      "Epoch 9 | Batch 24/94 | Batch Loss: 3.0142\n",
      "Epoch 9 | Batch 25/94 | Batch Loss: 3.0454\n",
      "Epoch 9 | Batch 26/94 | Batch Loss: 3.1720\n",
      "Epoch 9 | Batch 27/94 | Batch Loss: 3.0917\n",
      "Epoch 9 | Batch 28/94 | Batch Loss: 3.1420\n",
      "Epoch 9 | Batch 29/94 | Batch Loss: 3.1628\n",
      "Epoch 9 | Batch 30/94 | Batch Loss: 3.1591\n",
      "Epoch 9 | Batch 31/94 | Batch Loss: 3.0404\n",
      "Epoch 9 | Batch 32/94 | Batch Loss: 3.1027\n",
      "Epoch 9 | Batch 33/94 | Batch Loss: 3.1238\n",
      "Epoch 9 | Batch 34/94 | Batch Loss: 3.0556\n",
      "Epoch 9 | Batch 35/94 | Batch Loss: 3.1731\n",
      "Epoch 9 | Batch 36/94 | Batch Loss: 2.9851\n",
      "Epoch 9 | Batch 37/94 | Batch Loss: 3.0812\n",
      "Epoch 9 | Batch 38/94 | Batch Loss: 3.0448\n",
      "Epoch 9 | Batch 39/94 | Batch Loss: 3.2758\n",
      "Epoch 9 | Batch 40/94 | Batch Loss: 3.1774\n",
      "Epoch 9 | Batch 41/94 | Batch Loss: 3.0520\n",
      "Epoch 9 | Batch 42/94 | Batch Loss: 3.0381\n",
      "Epoch 9 | Batch 43/94 | Batch Loss: 3.0958\n",
      "Epoch 9 | Batch 44/94 | Batch Loss: 2.9958\n",
      "Epoch 9 | Batch 45/94 | Batch Loss: 3.1213\n",
      "Epoch 9 | Batch 46/94 | Batch Loss: 3.0525\n",
      "Epoch 9 | Batch 47/94 | Batch Loss: 3.0367\n",
      "Epoch 9 | Batch 48/94 | Batch Loss: 3.1355\n",
      "Epoch 9 | Batch 49/94 | Batch Loss: 3.1075\n",
      "Epoch 9 | Batch 50/94 | Batch Loss: 2.9883\n",
      "Epoch 9 | Batch 51/94 | Batch Loss: 3.1044\n",
      "Epoch 9 | Batch 52/94 | Batch Loss: 3.0719\n",
      "Epoch 9 | Batch 53/94 | Batch Loss: 2.9981\n",
      "Epoch 9 | Batch 54/94 | Batch Loss: 3.0553\n",
      "Epoch 9 | Batch 55/94 | Batch Loss: 3.1580\n",
      "Epoch 9 | Batch 56/94 | Batch Loss: 3.0743\n",
      "Epoch 9 | Batch 57/94 | Batch Loss: 3.0831\n",
      "Epoch 9 | Batch 58/94 | Batch Loss: 3.1552\n",
      "Epoch 9 | Batch 59/94 | Batch Loss: 3.1484\n",
      "Epoch 9 | Batch 60/94 | Batch Loss: 2.9148\n",
      "Epoch 9 | Batch 61/94 | Batch Loss: 3.1824\n",
      "Epoch 9 | Batch 62/94 | Batch Loss: 3.1458\n",
      "Epoch 9 | Batch 63/94 | Batch Loss: 2.9751\n",
      "Epoch 9 | Batch 64/94 | Batch Loss: 3.1453\n",
      "Epoch 9 | Batch 65/94 | Batch Loss: 2.9554\n",
      "Epoch 9 | Batch 66/94 | Batch Loss: 3.2076\n",
      "Epoch 9 | Batch 67/94 | Batch Loss: 3.0305\n",
      "Epoch 9 | Batch 68/94 | Batch Loss: 2.9852\n",
      "Epoch 9 | Batch 69/94 | Batch Loss: 3.2010\n",
      "Epoch 9 | Batch 70/94 | Batch Loss: 3.0106\n",
      "Epoch 9 | Batch 71/94 | Batch Loss: 3.0902\n",
      "Epoch 9 | Batch 72/94 | Batch Loss: 3.0072\n",
      "Epoch 9 | Batch 73/94 | Batch Loss: 3.1054\n",
      "Epoch 9 | Batch 74/94 | Batch Loss: 2.9873\n",
      "Epoch 9 | Batch 75/94 | Batch Loss: 3.0592\n",
      "Epoch 9 | Batch 76/94 | Batch Loss: 3.1618\n",
      "Epoch 9 | Batch 77/94 | Batch Loss: 3.0126\n",
      "Epoch 9 | Batch 78/94 | Batch Loss: 3.1362\n",
      "Epoch 9 | Batch 79/94 | Batch Loss: 3.0589\n",
      "Epoch 9 | Batch 80/94 | Batch Loss: 3.1446\n",
      "Epoch 9 | Batch 81/94 | Batch Loss: 2.9826\n",
      "Epoch 9 | Batch 82/94 | Batch Loss: 2.9905\n",
      "Epoch 9 | Batch 83/94 | Batch Loss: 3.1200\n",
      "Epoch 9 | Batch 84/94 | Batch Loss: 3.0585\n",
      "Epoch 9 | Batch 85/94 | Batch Loss: 3.0973\n",
      "Epoch 9 | Batch 86/94 | Batch Loss: 3.0186\n",
      "Epoch 9 | Batch 87/94 | Batch Loss: 3.1578\n",
      "Epoch 9 | Batch 88/94 | Batch Loss: 3.0554\n",
      "Epoch 9 | Batch 89/94 | Batch Loss: 2.9680\n",
      "Epoch 9 | Batch 90/94 | Batch Loss: 2.8961\n",
      "Epoch 9 | Batch 91/94 | Batch Loss: 3.0941\n",
      "Epoch 9 | Batch 92/94 | Batch Loss: 3.0246\n",
      "Epoch 9 | Batch 93/94 | Batch Loss: 3.1557\n",
      "Epoch 9 | Batch 94/94 | Batch Loss: 2.9557\n",
      "Epoch 9 Finished | Average Loss: 3.0921\n",
      "\n",
      "Epoch 10 Starting...\n",
      "Epoch 10 | Batch 1/94 | Batch Loss: 2.9522\n",
      "Epoch 10 | Batch 2/94 | Batch Loss: 3.0230\n",
      "Epoch 10 | Batch 3/94 | Batch Loss: 3.0950\n",
      "Epoch 10 | Batch 4/94 | Batch Loss: 2.9958\n",
      "Epoch 10 | Batch 5/94 | Batch Loss: 3.0829\n",
      "Epoch 10 | Batch 6/94 | Batch Loss: 2.9697\n",
      "Epoch 10 | Batch 7/94 | Batch Loss: 2.9697\n",
      "Epoch 10 | Batch 8/94 | Batch Loss: 3.0366\n",
      "Epoch 10 | Batch 9/94 | Batch Loss: 2.9535\n",
      "Epoch 10 | Batch 10/94 | Batch Loss: 3.0093\n",
      "Epoch 10 | Batch 11/94 | Batch Loss: 2.9734\n",
      "Epoch 10 | Batch 12/94 | Batch Loss: 3.0672\n",
      "Epoch 10 | Batch 13/94 | Batch Loss: 2.9543\n",
      "Epoch 10 | Batch 14/94 | Batch Loss: 3.0797\n",
      "Epoch 10 | Batch 15/94 | Batch Loss: 3.0365\n",
      "Epoch 10 | Batch 16/94 | Batch Loss: 3.0360\n",
      "Epoch 10 | Batch 17/94 | Batch Loss: 2.9088\n",
      "Epoch 10 | Batch 18/94 | Batch Loss: 2.8091\n",
      "Epoch 10 | Batch 19/94 | Batch Loss: 3.0597\n",
      "Epoch 10 | Batch 20/94 | Batch Loss: 2.9626\n",
      "Epoch 10 | Batch 21/94 | Batch Loss: 3.0794\n",
      "Epoch 10 | Batch 22/94 | Batch Loss: 2.9724\n",
      "Epoch 10 | Batch 23/94 | Batch Loss: 3.0631\n",
      "Epoch 10 | Batch 24/94 | Batch Loss: 2.9289\n",
      "Epoch 10 | Batch 25/94 | Batch Loss: 2.9457\n",
      "Epoch 10 | Batch 26/94 | Batch Loss: 3.0271\n",
      "Epoch 10 | Batch 27/94 | Batch Loss: 2.9140\n",
      "Epoch 10 | Batch 28/94 | Batch Loss: 3.0101\n",
      "Epoch 10 | Batch 29/94 | Batch Loss: 3.0345\n",
      "Epoch 10 | Batch 30/94 | Batch Loss: 3.0458\n",
      "Epoch 10 | Batch 31/94 | Batch Loss: 3.0290\n",
      "Epoch 10 | Batch 32/94 | Batch Loss: 3.0040\n",
      "Epoch 10 | Batch 33/94 | Batch Loss: 3.0331\n",
      "Epoch 10 | Batch 34/94 | Batch Loss: 2.8874\n",
      "Epoch 10 | Batch 35/94 | Batch Loss: 3.0171\n",
      "Epoch 10 | Batch 36/94 | Batch Loss: 2.9952\n",
      "Epoch 10 | Batch 37/94 | Batch Loss: 3.0489\n",
      "Epoch 10 | Batch 38/94 | Batch Loss: 2.9780\n",
      "Epoch 10 | Batch 39/94 | Batch Loss: 3.1876\n",
      "Epoch 10 | Batch 40/94 | Batch Loss: 2.8238\n",
      "Epoch 10 | Batch 41/94 | Batch Loss: 3.1181\n",
      "Epoch 10 | Batch 42/94 | Batch Loss: 2.8909\n",
      "Epoch 10 | Batch 43/94 | Batch Loss: 2.9914\n",
      "Epoch 10 | Batch 44/94 | Batch Loss: 2.9171\n",
      "Epoch 10 | Batch 45/94 | Batch Loss: 3.1524\n",
      "Epoch 10 | Batch 46/94 | Batch Loss: 2.9762\n",
      "Epoch 10 | Batch 47/94 | Batch Loss: 2.9487\n",
      "Epoch 10 | Batch 48/94 | Batch Loss: 3.0145\n",
      "Epoch 10 | Batch 49/94 | Batch Loss: 2.9495\n",
      "Epoch 10 | Batch 50/94 | Batch Loss: 2.9550\n",
      "Epoch 10 | Batch 51/94 | Batch Loss: 3.0227\n",
      "Epoch 10 | Batch 52/94 | Batch Loss: 3.0378\n",
      "Epoch 10 | Batch 53/94 | Batch Loss: 2.8885\n",
      "Epoch 10 | Batch 54/94 | Batch Loss: 2.9966\n",
      "Epoch 10 | Batch 55/94 | Batch Loss: 3.0189\n",
      "Epoch 10 | Batch 56/94 | Batch Loss: 2.8075\n",
      "Epoch 10 | Batch 57/94 | Batch Loss: 2.9310\n",
      "Epoch 10 | Batch 58/94 | Batch Loss: 2.8985\n",
      "Epoch 10 | Batch 59/94 | Batch Loss: 2.9351\n",
      "Epoch 10 | Batch 60/94 | Batch Loss: 2.9907\n",
      "Epoch 10 | Batch 61/94 | Batch Loss: 2.9289\n",
      "Epoch 10 | Batch 62/94 | Batch Loss: 2.9381\n",
      "Epoch 10 | Batch 63/94 | Batch Loss: 2.8854\n",
      "Epoch 10 | Batch 64/94 | Batch Loss: 2.8287\n",
      "Epoch 10 | Batch 65/94 | Batch Loss: 2.9932\n",
      "Epoch 10 | Batch 66/94 | Batch Loss: 2.9003\n",
      "Epoch 10 | Batch 67/94 | Batch Loss: 2.8692\n",
      "Epoch 10 | Batch 68/94 | Batch Loss: 2.9400\n",
      "Epoch 10 | Batch 69/94 | Batch Loss: 2.8438\n",
      "Epoch 10 | Batch 70/94 | Batch Loss: 3.0247\n",
      "Epoch 10 | Batch 71/94 | Batch Loss: 2.9575\n",
      "Epoch 10 | Batch 72/94 | Batch Loss: 2.8301\n",
      "Epoch 10 | Batch 73/94 | Batch Loss: 2.9452\n",
      "Epoch 10 | Batch 74/94 | Batch Loss: 2.9010\n",
      "Epoch 10 | Batch 75/94 | Batch Loss: 2.9777\n",
      "Epoch 10 | Batch 76/94 | Batch Loss: 2.9415\n",
      "Epoch 10 | Batch 77/94 | Batch Loss: 2.8968\n",
      "Epoch 10 | Batch 78/94 | Batch Loss: 2.8674\n",
      "Epoch 10 | Batch 79/94 | Batch Loss: 2.9700\n",
      "Epoch 10 | Batch 80/94 | Batch Loss: 2.8679\n",
      "Epoch 10 | Batch 81/94 | Batch Loss: 2.7391\n",
      "Epoch 10 | Batch 82/94 | Batch Loss: 2.6653\n",
      "Epoch 10 | Batch 83/94 | Batch Loss: 2.9996\n",
      "Epoch 10 | Batch 84/94 | Batch Loss: 2.8122\n",
      "Epoch 10 | Batch 85/94 | Batch Loss: 2.8956\n",
      "Epoch 10 | Batch 86/94 | Batch Loss: 2.8351\n",
      "Epoch 10 | Batch 87/94 | Batch Loss: 3.0266\n",
      "Epoch 10 | Batch 88/94 | Batch Loss: 2.9536\n",
      "Epoch 10 | Batch 89/94 | Batch Loss: 3.0288\n",
      "Epoch 10 | Batch 90/94 | Batch Loss: 2.8990\n",
      "Epoch 10 | Batch 91/94 | Batch Loss: 2.8967\n",
      "Epoch 10 | Batch 92/94 | Batch Loss: 2.8774\n",
      "Epoch 10 | Batch 93/94 | Batch Loss: 2.8527\n",
      "Epoch 10 | Batch 94/94 | Batch Loss: 2.9754\n",
      "Epoch 10 Finished | Average Loss: 2.9596\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aamir\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU Score: 1.86\n",
      "ROUGE-1: 0.00\n",
      "ROUGE-2: 0.00\n",
      "ROUGE-L: 0.00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# ========== 1. Load Tokenizers ==========\n",
    "\n",
    "tokenizer_en = spm.SentencePieceProcessor(model_file='bpe_en.model')\n",
    "tokenizer_ur = spm.SentencePieceProcessor(model_file='bpe_ur.model')\n",
    "\n",
    "# ========== 2. Dataset ==========\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_lines, tgt_lines, src_tokenizer, tgt_tokenizer, max_len=128):\n",
    "        self.src_lines = src_lines\n",
    "        self.tgt_lines = tgt_lines\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = [self.src_tokenizer.bos_id()] + self.src_tokenizer.encode(self.src_lines[idx], out_type=int) + [self.src_tokenizer.eos_id()]\n",
    "        tgt = [self.tgt_tokenizer.bos_id()] + self.tgt_tokenizer.encode(self.tgt_lines[idx], out_type=int) + [self.tgt_tokenizer.eos_id()]\n",
    "        \n",
    "        src = src[:self.max_len] + [0]*(self.max_len - len(src)) if len(src) < self.max_len else src[:self.max_len]\n",
    "        tgt = tgt[:self.max_len] + [0]*(self.max_len - len(tgt)) if len(tgt) < self.max_len else tgt[:self.max_len]\n",
    "        \n",
    "        return torch.tensor(src), torch.tensor(tgt)\n",
    "\n",
    "# ========== 3. Transformer Model ==========\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, nhead=8, num_layers=4, dim_feedforward=512, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.rand(5000, d_model), requires_grad=True)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,  # important\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.src_tok_emb(src) + self.positional_encoding[:src.size(1)]\n",
    "        tgt_emb = self.tgt_tok_emb(tgt) + self.positional_encoding[:tgt.size(1)]\n",
    "        \n",
    "        src_padding_mask = (src == 0)\n",
    "        tgt_padding_mask = (tgt == 0)\n",
    "        \n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n",
    "\n",
    "        # ⚡ Fix here: Make types same\n",
    "        tgt_mask = tgt_mask.masked_fill(tgt_mask == float('-inf'), True).masked_fill(tgt_mask == 0, False)\n",
    "\n",
    "        output = self.transformer(\n",
    "            src_emb, tgt_emb,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "        )\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# ========== 4. Prepare Dataset ==========\n",
    "\n",
    "train_src = data['quran']['train.en']\n",
    "train_tgt = data['quran']['train.ur']\n",
    "test_src = data['quran']['test.en']\n",
    "test_tgt = data['quran']['test.ur']\n",
    "\n",
    "train_dataset = TranslationDataset(train_src, train_tgt, tokenizer_en, tokenizer_ur)\n",
    "test_dataset = TranslationDataset(test_src, test_tgt, tokenizer_en, tokenizer_ur)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# ========== 5. Train ==========\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TransformerModel(\n",
    "    tokenizer_en.get_piece_size(),\n",
    "    tokenizer_ur.get_piece_size()\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    print(f\"\\nEpoch {epoch+1} Starting...\")\n",
    "    for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input)\n",
    "        output = output.reshape(-1, output.shape[-1])\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "        loss = criterion(output, tgt_output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 🛠 Print batch loss\n",
    "        print(f\"Epoch {epoch+1} | Batch {batch_idx+1}/{len(train_loader)} | Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1} Finished | Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ========== 6. Visualize Loss ==========\n",
    "\n",
    "plt.plot(train_losses, marker='o')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# ========== 7. Evaluate BLEU and ROUGE ==========\n",
    "\n",
    "model.eval()\n",
    "refs = []\n",
    "hyps = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for src, tgt in test_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        output = model(src, tgt_input)\n",
    "        output = output.argmax(dim=-1)\n",
    "\n",
    "        for i in range(output.size(0)):\n",
    "            pred_tokens = output[i].cpu().numpy().tolist()\n",
    "            true_tokens = tgt[i, 1:].cpu().numpy().tolist()\n",
    "\n",
    "            pred_sentence = tokenizer_ur.decode([tok for tok in pred_tokens if tok > 0])\n",
    "            true_sentence = tokenizer_ur.decode([tok for tok in true_tokens if tok > 0])\n",
    "\n",
    "            hyps.append(pred_sentence.split())\n",
    "            refs.append([true_sentence.split()])\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "bleu_score = corpus_bleu(refs, hyps, smoothing_function=smoothie)\n",
    "print(f\"\\nBLEU Score: {bleu_score*100:.2f}\")\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge1, rouge2, rougel = [], [], []\n",
    "\n",
    "for r, h in zip(refs, hyps):\n",
    "    scores = scorer.score(' '.join(r[0]), ' '.join(h))\n",
    "    rouge1.append(scores['rouge1'].fmeasure)\n",
    "    rouge2.append(scores['rouge2'].fmeasure)\n",
    "    rougel.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "print(f\"ROUGE-1: {np.mean(rouge1)*100:.2f}\")\n",
    "print(f\"ROUGE-2: {np.mean(rouge2)*100:.2f}\")\n",
    "print(f\"ROUGE-L: {np.mean(rougel)*100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67c1c18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Starting...\n",
      "Epoch 1 | Batch 1/94 | Batch Loss: 7.0619\n",
      "Epoch 1 | Batch 2/94 | Batch Loss: 6.8491\n",
      "Epoch 1 | Batch 3/94 | Batch Loss: 6.7195\n",
      "Epoch 1 | Batch 4/94 | Batch Loss: 6.6177\n",
      "Epoch 1 | Batch 5/94 | Batch Loss: 6.5322\n",
      "Epoch 1 | Batch 6/94 | Batch Loss: 6.5054\n",
      "Epoch 1 | Batch 7/94 | Batch Loss: 6.4460\n",
      "Epoch 1 | Batch 8/94 | Batch Loss: 6.4221\n",
      "Epoch 1 | Batch 9/94 | Batch Loss: 6.3628\n",
      "Epoch 1 | Batch 10/94 | Batch Loss: 6.3137\n",
      "Epoch 1 | Batch 11/94 | Batch Loss: 6.2934\n",
      "Epoch 1 | Batch 12/94 | Batch Loss: 6.2870\n",
      "Epoch 1 | Batch 13/94 | Batch Loss: 6.2682\n",
      "Epoch 1 | Batch 14/94 | Batch Loss: 6.2553\n",
      "Epoch 1 | Batch 15/94 | Batch Loss: 6.2202\n",
      "Epoch 1 | Batch 16/94 | Batch Loss: 6.1890\n",
      "Epoch 1 | Batch 17/94 | Batch Loss: 6.1680\n",
      "Epoch 1 | Batch 18/94 | Batch Loss: 6.1364\n",
      "Epoch 1 | Batch 19/94 | Batch Loss: 6.1321\n",
      "Epoch 1 | Batch 20/94 | Batch Loss: 6.1353\n",
      "Epoch 1 | Batch 21/94 | Batch Loss: 6.1155\n",
      "Epoch 1 | Batch 22/94 | Batch Loss: 6.0914\n",
      "Epoch 1 | Batch 23/94 | Batch Loss: 6.0750\n",
      "Epoch 1 | Batch 24/94 | Batch Loss: 6.0869\n",
      "Epoch 1 | Batch 25/94 | Batch Loss: 6.0066\n",
      "Epoch 1 | Batch 26/94 | Batch Loss: 6.0452\n",
      "Epoch 1 | Batch 27/94 | Batch Loss: 6.0253\n",
      "Epoch 1 | Batch 28/94 | Batch Loss: 6.0063\n",
      "Epoch 1 | Batch 29/94 | Batch Loss: 5.9671\n",
      "Epoch 1 | Batch 30/94 | Batch Loss: 5.9394\n",
      "Epoch 1 | Batch 31/94 | Batch Loss: 5.9501\n",
      "Epoch 1 | Batch 32/94 | Batch Loss: 5.9037\n",
      "Epoch 1 | Batch 33/94 | Batch Loss: 5.9248\n",
      "Epoch 1 | Batch 34/94 | Batch Loss: 5.8940\n",
      "Epoch 1 | Batch 35/94 | Batch Loss: 5.8732\n",
      "Epoch 1 | Batch 36/94 | Batch Loss: 5.8721\n",
      "Epoch 1 | Batch 37/94 | Batch Loss: 5.8724\n",
      "Epoch 1 | Batch 38/94 | Batch Loss: 5.8549\n",
      "Epoch 1 | Batch 39/94 | Batch Loss: 5.8300\n",
      "Epoch 1 | Batch 40/94 | Batch Loss: 5.8426\n",
      "Epoch 1 | Batch 41/94 | Batch Loss: 5.8120\n",
      "Epoch 1 | Batch 42/94 | Batch Loss: 5.8015\n",
      "Epoch 1 | Batch 43/94 | Batch Loss: 5.8070\n",
      "Epoch 1 | Batch 44/94 | Batch Loss: 5.7568\n",
      "Epoch 1 | Batch 45/94 | Batch Loss: 5.8025\n",
      "Epoch 1 | Batch 46/94 | Batch Loss: 5.7810\n",
      "Epoch 1 | Batch 47/94 | Batch Loss: 5.7789\n",
      "Epoch 1 | Batch 48/94 | Batch Loss: 5.7479\n",
      "Epoch 1 | Batch 49/94 | Batch Loss: 5.7447\n",
      "Epoch 1 | Batch 50/94 | Batch Loss: 5.7435\n",
      "Epoch 1 | Batch 51/94 | Batch Loss: 5.7200\n",
      "Epoch 1 | Batch 52/94 | Batch Loss: 5.7381\n",
      "Epoch 1 | Batch 53/94 | Batch Loss: 5.6926\n",
      "Epoch 1 | Batch 54/94 | Batch Loss: 5.6906\n",
      "Epoch 1 | Batch 55/94 | Batch Loss: 5.6744\n",
      "Epoch 1 | Batch 56/94 | Batch Loss: 5.6933\n",
      "Epoch 1 | Batch 57/94 | Batch Loss: 5.6485\n",
      "Epoch 1 | Batch 58/94 | Batch Loss: 5.6671\n",
      "Epoch 1 | Batch 59/94 | Batch Loss: 5.6605\n",
      "Epoch 1 | Batch 60/94 | Batch Loss: 5.6676\n",
      "Epoch 1 | Batch 61/94 | Batch Loss: 5.6494\n",
      "Epoch 1 | Batch 62/94 | Batch Loss: 5.6325\n",
      "Epoch 1 | Batch 63/94 | Batch Loss: 5.6115\n",
      "Epoch 1 | Batch 64/94 | Batch Loss: 5.6318\n",
      "Epoch 1 | Batch 65/94 | Batch Loss: 5.5821\n",
      "Epoch 1 | Batch 66/94 | Batch Loss: 5.5831\n",
      "Epoch 1 | Batch 67/94 | Batch Loss: 5.5355\n",
      "Epoch 1 | Batch 68/94 | Batch Loss: 5.5595\n",
      "Epoch 1 | Batch 69/94 | Batch Loss: 5.5536\n",
      "Epoch 1 | Batch 70/94 | Batch Loss: 5.5838\n",
      "Epoch 1 | Batch 71/94 | Batch Loss: 5.5343\n",
      "Epoch 1 | Batch 72/94 | Batch Loss: 5.5054\n",
      "Epoch 1 | Batch 73/94 | Batch Loss: 5.5042\n",
      "Epoch 1 | Batch 74/94 | Batch Loss: 5.5477\n",
      "Epoch 1 | Batch 75/94 | Batch Loss: 5.5377\n",
      "Epoch 1 | Batch 76/94 | Batch Loss: 5.5091\n",
      "Epoch 1 | Batch 77/94 | Batch Loss: 5.4981\n",
      "Epoch 1 | Batch 78/94 | Batch Loss: 5.4893\n",
      "Epoch 1 | Batch 79/94 | Batch Loss: 5.4586\n",
      "Epoch 1 | Batch 80/94 | Batch Loss: 5.5111\n",
      "Epoch 1 | Batch 81/94 | Batch Loss: 5.4406\n",
      "Epoch 1 | Batch 82/94 | Batch Loss: 5.4335\n",
      "Epoch 1 | Batch 83/94 | Batch Loss: 5.4379\n",
      "Epoch 1 | Batch 84/94 | Batch Loss: 5.4436\n",
      "Epoch 1 | Batch 85/94 | Batch Loss: 5.4272\n",
      "Epoch 1 | Batch 86/94 | Batch Loss: 5.4110\n",
      "Epoch 1 | Batch 87/94 | Batch Loss: 5.3924\n",
      "Epoch 1 | Batch 88/94 | Batch Loss: 5.3874\n",
      "Epoch 1 | Batch 89/94 | Batch Loss: 5.3624\n",
      "Epoch 1 | Batch 90/94 | Batch Loss: 5.3812\n",
      "Epoch 1 | Batch 91/94 | Batch Loss: 5.4567\n",
      "Epoch 1 | Batch 92/94 | Batch Loss: 5.3778\n",
      "Epoch 1 | Batch 93/94 | Batch Loss: 5.3383\n",
      "Epoch 1 | Batch 94/94 | Batch Loss: 5.3604\n",
      "Epoch 1 Finished | Average Loss: 5.8339\n",
      "\n",
      "Epoch 2 Starting...\n",
      "Epoch 2 | Batch 1/94 | Batch Loss: 5.3697\n",
      "Epoch 2 | Batch 2/94 | Batch Loss: 5.3598\n",
      "Epoch 2 | Batch 3/94 | Batch Loss: 5.3033\n",
      "Epoch 2 | Batch 4/94 | Batch Loss: 5.3675\n",
      "Epoch 2 | Batch 5/94 | Batch Loss: 5.2999\n",
      "Epoch 2 | Batch 6/94 | Batch Loss: 5.3290\n",
      "Epoch 2 | Batch 7/94 | Batch Loss: 5.2409\n",
      "Epoch 2 | Batch 8/94 | Batch Loss: 5.2586\n",
      "Epoch 2 | Batch 9/94 | Batch Loss: 5.3057\n",
      "Epoch 2 | Batch 10/94 | Batch Loss: 5.2937\n",
      "Epoch 2 | Batch 11/94 | Batch Loss: 5.2827\n",
      "Epoch 2 | Batch 12/94 | Batch Loss: 5.2266\n",
      "Epoch 2 | Batch 13/94 | Batch Loss: 5.3114\n",
      "Epoch 2 | Batch 14/94 | Batch Loss: 5.2951\n",
      "Epoch 2 | Batch 15/94 | Batch Loss: 5.2562\n",
      "Epoch 2 | Batch 16/94 | Batch Loss: 5.2526\n",
      "Epoch 2 | Batch 17/94 | Batch Loss: 5.2284\n",
      "Epoch 2 | Batch 18/94 | Batch Loss: 5.2375\n",
      "Epoch 2 | Batch 19/94 | Batch Loss: 5.2170\n",
      "Epoch 2 | Batch 20/94 | Batch Loss: 5.2084\n",
      "Epoch 2 | Batch 21/94 | Batch Loss: 5.1951\n",
      "Epoch 2 | Batch 22/94 | Batch Loss: 5.2480\n",
      "Epoch 2 | Batch 23/94 | Batch Loss: 5.2386\n",
      "Epoch 2 | Batch 24/94 | Batch Loss: 5.1138\n",
      "Epoch 2 | Batch 25/94 | Batch Loss: 5.1997\n",
      "Epoch 2 | Batch 26/94 | Batch Loss: 5.1661\n",
      "Epoch 2 | Batch 27/94 | Batch Loss: 5.2139\n",
      "Epoch 2 | Batch 28/94 | Batch Loss: 5.1975\n",
      "Epoch 2 | Batch 29/94 | Batch Loss: 5.1186\n",
      "Epoch 2 | Batch 30/94 | Batch Loss: 5.1603\n",
      "Epoch 2 | Batch 31/94 | Batch Loss: 5.2248\n",
      "Epoch 2 | Batch 32/94 | Batch Loss: 5.1863\n",
      "Epoch 2 | Batch 33/94 | Batch Loss: 5.1471\n",
      "Epoch 2 | Batch 34/94 | Batch Loss: 5.1532\n",
      "Epoch 2 | Batch 35/94 | Batch Loss: 5.1375\n",
      "Epoch 2 | Batch 36/94 | Batch Loss: 5.1027\n",
      "Epoch 2 | Batch 37/94 | Batch Loss: 5.1064\n",
      "Epoch 2 | Batch 38/94 | Batch Loss: 5.1359\n",
      "Epoch 2 | Batch 39/94 | Batch Loss: 5.0679\n",
      "Epoch 2 | Batch 40/94 | Batch Loss: 5.1424\n",
      "Epoch 2 | Batch 41/94 | Batch Loss: 5.0575\n",
      "Epoch 2 | Batch 42/94 | Batch Loss: 5.0514\n",
      "Epoch 2 | Batch 43/94 | Batch Loss: 5.1474\n",
      "Epoch 2 | Batch 44/94 | Batch Loss: 5.0816\n",
      "Epoch 2 | Batch 45/94 | Batch Loss: 5.1165\n",
      "Epoch 2 | Batch 46/94 | Batch Loss: 5.1228\n",
      "Epoch 2 | Batch 47/94 | Batch Loss: 5.0473\n",
      "Epoch 2 | Batch 48/94 | Batch Loss: 5.1042\n",
      "Epoch 2 | Batch 49/94 | Batch Loss: 5.0848\n",
      "Epoch 2 | Batch 50/94 | Batch Loss: 5.0084\n",
      "Epoch 2 | Batch 51/94 | Batch Loss: 5.0608\n",
      "Epoch 2 | Batch 52/94 | Batch Loss: 4.9704\n",
      "Epoch 2 | Batch 53/94 | Batch Loss: 5.1061\n",
      "Epoch 2 | Batch 54/94 | Batch Loss: 5.0574\n",
      "Epoch 2 | Batch 55/94 | Batch Loss: 5.0349\n",
      "Epoch 2 | Batch 56/94 | Batch Loss: 5.0160\n",
      "Epoch 2 | Batch 57/94 | Batch Loss: 5.0357\n",
      "Epoch 2 | Batch 58/94 | Batch Loss: 4.9862\n",
      "Epoch 2 | Batch 59/94 | Batch Loss: 4.9699\n",
      "Epoch 2 | Batch 60/94 | Batch Loss: 4.9350\n",
      "Epoch 2 | Batch 61/94 | Batch Loss: 5.0020\n",
      "Epoch 2 | Batch 62/94 | Batch Loss: 4.9926\n",
      "Epoch 2 | Batch 63/94 | Batch Loss: 5.0076\n",
      "Epoch 2 | Batch 64/94 | Batch Loss: 4.9608\n",
      "Epoch 2 | Batch 65/94 | Batch Loss: 4.9189\n",
      "Epoch 2 | Batch 66/94 | Batch Loss: 4.9899\n",
      "Epoch 2 | Batch 67/94 | Batch Loss: 5.0015\n",
      "Epoch 2 | Batch 68/94 | Batch Loss: 4.8650\n",
      "Epoch 2 | Batch 69/94 | Batch Loss: 4.8908\n",
      "Epoch 2 | Batch 70/94 | Batch Loss: 4.9946\n",
      "Epoch 2 | Batch 71/94 | Batch Loss: 4.9456\n",
      "Epoch 2 | Batch 72/94 | Batch Loss: 4.9859\n",
      "Epoch 2 | Batch 73/94 | Batch Loss: 4.8961\n",
      "Epoch 2 | Batch 74/94 | Batch Loss: 4.9468\n",
      "Epoch 2 | Batch 75/94 | Batch Loss: 4.9211\n",
      "Epoch 2 | Batch 76/94 | Batch Loss: 4.8688\n",
      "Epoch 2 | Batch 77/94 | Batch Loss: 4.9021\n",
      "Epoch 2 | Batch 78/94 | Batch Loss: 4.8927\n",
      "Epoch 2 | Batch 79/94 | Batch Loss: 4.9331\n",
      "Epoch 2 | Batch 80/94 | Batch Loss: 4.8948\n",
      "Epoch 2 | Batch 81/94 | Batch Loss: 4.8923\n",
      "Epoch 2 | Batch 82/94 | Batch Loss: 4.9273\n",
      "Epoch 2 | Batch 83/94 | Batch Loss: 4.8515\n",
      "Epoch 2 | Batch 84/94 | Batch Loss: 4.8449\n",
      "Epoch 2 | Batch 85/94 | Batch Loss: 4.8722\n",
      "Epoch 2 | Batch 86/94 | Batch Loss: 4.9061\n",
      "Epoch 2 | Batch 87/94 | Batch Loss: 4.9364\n",
      "Epoch 2 | Batch 88/94 | Batch Loss: 4.8408\n",
      "Epoch 2 | Batch 89/94 | Batch Loss: 4.9184\n",
      "Epoch 2 | Batch 90/94 | Batch Loss: 4.8342\n",
      "Epoch 2 | Batch 91/94 | Batch Loss: 4.8788\n",
      "Epoch 2 | Batch 92/94 | Batch Loss: 4.7953\n",
      "Epoch 2 | Batch 93/94 | Batch Loss: 4.8508\n",
      "Epoch 2 | Batch 94/94 | Batch Loss: 4.8271\n",
      "Epoch 2 Finished | Average Loss: 5.0732\n",
      "\n",
      "Epoch 3 Starting...\n",
      "Epoch 3 | Batch 1/94 | Batch Loss: 4.8160\n",
      "Epoch 3 | Batch 2/94 | Batch Loss: 4.7912\n",
      "Epoch 3 | Batch 3/94 | Batch Loss: 4.7663\n",
      "Epoch 3 | Batch 4/94 | Batch Loss: 4.8532\n",
      "Epoch 3 | Batch 5/94 | Batch Loss: 4.7418\n",
      "Epoch 3 | Batch 6/94 | Batch Loss: 4.6949\n",
      "Epoch 3 | Batch 7/94 | Batch Loss: 4.7949\n",
      "Epoch 3 | Batch 8/94 | Batch Loss: 4.7746\n",
      "Epoch 3 | Batch 9/94 | Batch Loss: 4.7875\n",
      "Epoch 3 | Batch 10/94 | Batch Loss: 4.6945\n",
      "Epoch 3 | Batch 11/94 | Batch Loss: 4.7330\n",
      "Epoch 3 | Batch 12/94 | Batch Loss: 4.7496\n",
      "Epoch 3 | Batch 13/94 | Batch Loss: 4.7368\n",
      "Epoch 3 | Batch 14/94 | Batch Loss: 4.8072\n",
      "Epoch 3 | Batch 15/94 | Batch Loss: 4.7414\n",
      "Epoch 3 | Batch 16/94 | Batch Loss: 4.8402\n",
      "Epoch 3 | Batch 17/94 | Batch Loss: 4.6839\n",
      "Epoch 3 | Batch 18/94 | Batch Loss: 4.7373\n",
      "Epoch 3 | Batch 19/94 | Batch Loss: 4.6958\n",
      "Epoch 3 | Batch 20/94 | Batch Loss: 4.6783\n",
      "Epoch 3 | Batch 21/94 | Batch Loss: 4.6567\n",
      "Epoch 3 | Batch 22/94 | Batch Loss: 4.6970\n",
      "Epoch 3 | Batch 23/94 | Batch Loss: 4.7026\n",
      "Epoch 3 | Batch 24/94 | Batch Loss: 4.7745\n",
      "Epoch 3 | Batch 25/94 | Batch Loss: 4.7168\n",
      "Epoch 3 | Batch 26/94 | Batch Loss: 4.6815\n",
      "Epoch 3 | Batch 27/94 | Batch Loss: 4.6641\n",
      "Epoch 3 | Batch 28/94 | Batch Loss: 4.6739\n",
      "Epoch 3 | Batch 29/94 | Batch Loss: 4.6590\n",
      "Epoch 3 | Batch 30/94 | Batch Loss: 4.6630\n",
      "Epoch 3 | Batch 31/94 | Batch Loss: 4.6171\n",
      "Epoch 3 | Batch 32/94 | Batch Loss: 4.7068\n",
      "Epoch 3 | Batch 33/94 | Batch Loss: 4.6288\n",
      "Epoch 3 | Batch 34/94 | Batch Loss: 4.7172\n",
      "Epoch 3 | Batch 35/94 | Batch Loss: 4.6395\n",
      "Epoch 3 | Batch 36/94 | Batch Loss: 4.6804\n",
      "Epoch 3 | Batch 37/94 | Batch Loss: 4.5898\n",
      "Epoch 3 | Batch 38/94 | Batch Loss: 4.5962\n",
      "Epoch 3 | Batch 39/94 | Batch Loss: 4.6476\n",
      "Epoch 3 | Batch 40/94 | Batch Loss: 4.6624\n",
      "Epoch 3 | Batch 41/94 | Batch Loss: 4.6046\n",
      "Epoch 3 | Batch 42/94 | Batch Loss: 4.6841\n",
      "Epoch 3 | Batch 43/94 | Batch Loss: 4.5389\n",
      "Epoch 3 | Batch 44/94 | Batch Loss: 4.6472\n",
      "Epoch 3 | Batch 45/94 | Batch Loss: 4.6351\n",
      "Epoch 3 | Batch 46/94 | Batch Loss: 4.5993\n",
      "Epoch 3 | Batch 47/94 | Batch Loss: 4.5761\n",
      "Epoch 3 | Batch 48/94 | Batch Loss: 4.5475\n",
      "Epoch 3 | Batch 49/94 | Batch Loss: 4.6022\n",
      "Epoch 3 | Batch 50/94 | Batch Loss: 4.5433\n",
      "Epoch 3 | Batch 51/94 | Batch Loss: 4.5840\n",
      "Epoch 3 | Batch 52/94 | Batch Loss: 4.5927\n",
      "Epoch 3 | Batch 53/94 | Batch Loss: 4.5006\n",
      "Epoch 3 | Batch 54/94 | Batch Loss: 4.6318\n",
      "Epoch 3 | Batch 55/94 | Batch Loss: 4.6137\n",
      "Epoch 3 | Batch 56/94 | Batch Loss: 4.5605\n",
      "Epoch 3 | Batch 57/94 | Batch Loss: 4.5172\n",
      "Epoch 3 | Batch 58/94 | Batch Loss: 4.5382\n",
      "Epoch 3 | Batch 59/94 | Batch Loss: 4.6671\n",
      "Epoch 3 | Batch 60/94 | Batch Loss: 4.6060\n",
      "Epoch 3 | Batch 61/94 | Batch Loss: 4.6035\n",
      "Epoch 3 | Batch 62/94 | Batch Loss: 4.5325\n",
      "Epoch 3 | Batch 63/94 | Batch Loss: 4.6060\n",
      "Epoch 3 | Batch 64/94 | Batch Loss: 4.4701\n",
      "Epoch 3 | Batch 65/94 | Batch Loss: 4.5436\n",
      "Epoch 3 | Batch 66/94 | Batch Loss: 4.5827\n",
      "Epoch 3 | Batch 67/94 | Batch Loss: 4.5175\n",
      "Epoch 3 | Batch 68/94 | Batch Loss: 4.4829\n",
      "Epoch 3 | Batch 69/94 | Batch Loss: 4.4661\n",
      "Epoch 3 | Batch 70/94 | Batch Loss: 4.5512\n",
      "Epoch 3 | Batch 71/94 | Batch Loss: 4.5349\n",
      "Epoch 3 | Batch 72/94 | Batch Loss: 4.4892\n",
      "Epoch 3 | Batch 73/94 | Batch Loss: 4.5175\n",
      "Epoch 3 | Batch 74/94 | Batch Loss: 4.5193\n",
      "Epoch 3 | Batch 75/94 | Batch Loss: 4.5151\n",
      "Epoch 3 | Batch 76/94 | Batch Loss: 4.5136\n",
      "Epoch 3 | Batch 77/94 | Batch Loss: 4.4986\n",
      "Epoch 3 | Batch 78/94 | Batch Loss: 4.5763\n",
      "Epoch 3 | Batch 79/94 | Batch Loss: 4.4959\n",
      "Epoch 3 | Batch 80/94 | Batch Loss: 4.4977\n",
      "Epoch 3 | Batch 81/94 | Batch Loss: 4.4782\n",
      "Epoch 3 | Batch 82/94 | Batch Loss: 4.4604\n",
      "Epoch 3 | Batch 83/94 | Batch Loss: 4.5157\n",
      "Epoch 3 | Batch 84/94 | Batch Loss: 4.5647\n",
      "Epoch 3 | Batch 85/94 | Batch Loss: 4.4335\n",
      "Epoch 3 | Batch 86/94 | Batch Loss: 4.4451\n",
      "Epoch 3 | Batch 87/94 | Batch Loss: 4.3941\n",
      "Epoch 3 | Batch 88/94 | Batch Loss: 4.5318\n",
      "Epoch 3 | Batch 89/94 | Batch Loss: 4.3856\n",
      "Epoch 3 | Batch 90/94 | Batch Loss: 4.4334\n",
      "Epoch 3 | Batch 91/94 | Batch Loss: 4.4426\n",
      "Epoch 3 | Batch 92/94 | Batch Loss: 4.3889\n",
      "Epoch 3 | Batch 93/94 | Batch Loss: 4.4777\n",
      "Epoch 3 | Batch 94/94 | Batch Loss: 4.4278\n",
      "Epoch 3 Finished | Average Loss: 4.6083\n",
      "\n",
      "Epoch 4 Starting...\n",
      "Epoch 4 | Batch 1/94 | Batch Loss: 4.3992\n",
      "Epoch 4 | Batch 2/94 | Batch Loss: 4.4227\n",
      "Epoch 4 | Batch 3/94 | Batch Loss: 4.4212\n",
      "Epoch 4 | Batch 4/94 | Batch Loss: 4.4103\n",
      "Epoch 4 | Batch 5/94 | Batch Loss: 4.3859\n",
      "Epoch 4 | Batch 6/94 | Batch Loss: 4.3735\n",
      "Epoch 4 | Batch 7/94 | Batch Loss: 4.3910\n",
      "Epoch 4 | Batch 8/94 | Batch Loss: 4.3559\n",
      "Epoch 4 | Batch 9/94 | Batch Loss: 4.3514\n",
      "Epoch 4 | Batch 10/94 | Batch Loss: 4.3614\n",
      "Epoch 4 | Batch 11/94 | Batch Loss: 4.5033\n",
      "Epoch 4 | Batch 12/94 | Batch Loss: 4.4514\n",
      "Epoch 4 | Batch 13/94 | Batch Loss: 4.4063\n",
      "Epoch 4 | Batch 14/94 | Batch Loss: 4.4141\n",
      "Epoch 4 | Batch 15/94 | Batch Loss: 4.3486\n",
      "Epoch 4 | Batch 16/94 | Batch Loss: 4.3711\n",
      "Epoch 4 | Batch 17/94 | Batch Loss: 4.4557\n",
      "Epoch 4 | Batch 18/94 | Batch Loss: 4.4025\n",
      "Epoch 4 | Batch 19/94 | Batch Loss: 4.3150\n",
      "Epoch 4 | Batch 20/94 | Batch Loss: 4.4280\n",
      "Epoch 4 | Batch 21/94 | Batch Loss: 4.4348\n",
      "Epoch 4 | Batch 22/94 | Batch Loss: 4.4034\n",
      "Epoch 4 | Batch 23/94 | Batch Loss: 4.3513\n",
      "Epoch 4 | Batch 24/94 | Batch Loss: 4.3215\n",
      "Epoch 4 | Batch 25/94 | Batch Loss: 4.4158\n",
      "Epoch 4 | Batch 26/94 | Batch Loss: 4.3338\n",
      "Epoch 4 | Batch 27/94 | Batch Loss: 4.3796\n",
      "Epoch 4 | Batch 28/94 | Batch Loss: 4.3771\n",
      "Epoch 4 | Batch 29/94 | Batch Loss: 4.3270\n",
      "Epoch 4 | Batch 30/94 | Batch Loss: 4.3825\n",
      "Epoch 4 | Batch 31/94 | Batch Loss: 4.3126\n",
      "Epoch 4 | Batch 32/94 | Batch Loss: 4.3172\n",
      "Epoch 4 | Batch 33/94 | Batch Loss: 4.3917\n",
      "Epoch 4 | Batch 34/94 | Batch Loss: 4.3411\n",
      "Epoch 4 | Batch 35/94 | Batch Loss: 4.4439\n",
      "Epoch 4 | Batch 36/94 | Batch Loss: 4.4272\n",
      "Epoch 4 | Batch 37/94 | Batch Loss: 4.2690\n",
      "Epoch 4 | Batch 38/94 | Batch Loss: 4.4206\n",
      "Epoch 4 | Batch 39/94 | Batch Loss: 4.3696\n",
      "Epoch 4 | Batch 40/94 | Batch Loss: 4.2891\n",
      "Epoch 4 | Batch 41/94 | Batch Loss: 4.3050\n",
      "Epoch 4 | Batch 42/94 | Batch Loss: 4.2596\n",
      "Epoch 4 | Batch 43/94 | Batch Loss: 4.3279\n",
      "Epoch 4 | Batch 44/94 | Batch Loss: 4.3293\n",
      "Epoch 4 | Batch 45/94 | Batch Loss: 4.2540\n",
      "Epoch 4 | Batch 46/94 | Batch Loss: 4.3190\n",
      "Epoch 4 | Batch 47/94 | Batch Loss: 4.2494\n",
      "Epoch 4 | Batch 48/94 | Batch Loss: 4.4564\n",
      "Epoch 4 | Batch 49/94 | Batch Loss: 4.3201\n",
      "Epoch 4 | Batch 50/94 | Batch Loss: 4.3149\n",
      "Epoch 4 | Batch 51/94 | Batch Loss: 4.3568\n",
      "Epoch 4 | Batch 52/94 | Batch Loss: 4.2996\n",
      "Epoch 4 | Batch 53/94 | Batch Loss: 4.2357\n",
      "Epoch 4 | Batch 54/94 | Batch Loss: 4.3764\n",
      "Epoch 4 | Batch 55/94 | Batch Loss: 4.4387\n",
      "Epoch 4 | Batch 56/94 | Batch Loss: 4.4062\n",
      "Epoch 4 | Batch 57/94 | Batch Loss: 4.2054\n",
      "Epoch 4 | Batch 58/94 | Batch Loss: 4.3414\n",
      "Epoch 4 | Batch 59/94 | Batch Loss: 4.2159\n",
      "Epoch 4 | Batch 60/94 | Batch Loss: 4.2283\n",
      "Epoch 4 | Batch 61/94 | Batch Loss: 4.3204\n",
      "Epoch 4 | Batch 62/94 | Batch Loss: 4.2771\n",
      "Epoch 4 | Batch 63/94 | Batch Loss: 4.2741\n",
      "Epoch 4 | Batch 64/94 | Batch Loss: 4.2487\n",
      "Epoch 4 | Batch 65/94 | Batch Loss: 4.2484\n",
      "Epoch 4 | Batch 66/94 | Batch Loss: 4.3213\n",
      "Epoch 4 | Batch 67/94 | Batch Loss: 4.2852\n",
      "Epoch 4 | Batch 68/94 | Batch Loss: 4.2705\n",
      "Epoch 4 | Batch 69/94 | Batch Loss: 4.3476\n",
      "Epoch 4 | Batch 70/94 | Batch Loss: 4.2964\n",
      "Epoch 4 | Batch 71/94 | Batch Loss: 4.0945\n",
      "Epoch 4 | Batch 72/94 | Batch Loss: 4.2711\n",
      "Epoch 4 | Batch 73/94 | Batch Loss: 4.3510\n",
      "Epoch 4 | Batch 74/94 | Batch Loss: 4.2528\n",
      "Epoch 4 | Batch 75/94 | Batch Loss: 4.1601\n",
      "Epoch 4 | Batch 76/94 | Batch Loss: 4.2801\n",
      "Epoch 4 | Batch 77/94 | Batch Loss: 4.2039\n",
      "Epoch 4 | Batch 78/94 | Batch Loss: 4.3193\n",
      "Epoch 4 | Batch 79/94 | Batch Loss: 4.2077\n",
      "Epoch 4 | Batch 80/94 | Batch Loss: 4.1887\n",
      "Epoch 4 | Batch 81/94 | Batch Loss: 4.1797\n",
      "Epoch 4 | Batch 82/94 | Batch Loss: 4.2584\n",
      "Epoch 4 | Batch 83/94 | Batch Loss: 4.2954\n",
      "Epoch 4 | Batch 84/94 | Batch Loss: 4.3503\n",
      "Epoch 4 | Batch 85/94 | Batch Loss: 4.2081\n",
      "Epoch 4 | Batch 86/94 | Batch Loss: 4.3481\n",
      "Epoch 4 | Batch 87/94 | Batch Loss: 4.1922\n",
      "Epoch 4 | Batch 88/94 | Batch Loss: 4.3814\n",
      "Epoch 4 | Batch 89/94 | Batch Loss: 4.1697\n",
      "Epoch 4 | Batch 90/94 | Batch Loss: 4.2026\n",
      "Epoch 4 | Batch 91/94 | Batch Loss: 4.2710\n",
      "Epoch 4 | Batch 92/94 | Batch Loss: 4.3637\n",
      "Epoch 4 | Batch 93/94 | Batch Loss: 4.1379\n",
      "Epoch 4 | Batch 94/94 | Batch Loss: 4.2410\n",
      "Epoch 4 Finished | Average Loss: 4.3227\n",
      "\n",
      "Epoch 5 Starting...\n",
      "Epoch 5 | Batch 1/94 | Batch Loss: 4.2761\n",
      "Epoch 5 | Batch 2/94 | Batch Loss: 4.1877\n",
      "Epoch 5 | Batch 3/94 | Batch Loss: 4.2890\n",
      "Epoch 5 | Batch 4/94 | Batch Loss: 4.0882\n",
      "Epoch 5 | Batch 5/94 | Batch Loss: 4.1923\n",
      "Epoch 5 | Batch 6/94 | Batch Loss: 4.1981\n",
      "Epoch 5 | Batch 7/94 | Batch Loss: 4.2005\n",
      "Epoch 5 | Batch 8/94 | Batch Loss: 4.1948\n",
      "Epoch 5 | Batch 9/94 | Batch Loss: 4.2887\n",
      "Epoch 5 | Batch 10/94 | Batch Loss: 4.2553\n",
      "Epoch 5 | Batch 11/94 | Batch Loss: 4.2606\n",
      "Epoch 5 | Batch 12/94 | Batch Loss: 4.2880\n",
      "Epoch 5 | Batch 13/94 | Batch Loss: 4.1337\n",
      "Epoch 5 | Batch 14/94 | Batch Loss: 4.2017\n",
      "Epoch 5 | Batch 15/94 | Batch Loss: 4.1867\n",
      "Epoch 5 | Batch 16/94 | Batch Loss: 4.2119\n",
      "Epoch 5 | Batch 17/94 | Batch Loss: 4.1742\n",
      "Epoch 5 | Batch 18/94 | Batch Loss: 4.0580\n",
      "Epoch 5 | Batch 19/94 | Batch Loss: 4.1513\n",
      "Epoch 5 | Batch 20/94 | Batch Loss: 4.1751\n",
      "Epoch 5 | Batch 21/94 | Batch Loss: 4.1667\n",
      "Epoch 5 | Batch 22/94 | Batch Loss: 4.1920\n",
      "Epoch 5 | Batch 23/94 | Batch Loss: 4.1867\n",
      "Epoch 5 | Batch 24/94 | Batch Loss: 4.2414\n",
      "Epoch 5 | Batch 25/94 | Batch Loss: 4.1682\n",
      "Epoch 5 | Batch 26/94 | Batch Loss: 4.1219\n",
      "Epoch 5 | Batch 27/94 | Batch Loss: 4.1437\n",
      "Epoch 5 | Batch 28/94 | Batch Loss: 4.1394\n",
      "Epoch 5 | Batch 29/94 | Batch Loss: 4.1584\n",
      "Epoch 5 | Batch 30/94 | Batch Loss: 4.1214\n",
      "Epoch 5 | Batch 31/94 | Batch Loss: 4.1065\n",
      "Epoch 5 | Batch 32/94 | Batch Loss: 4.1552\n",
      "Epoch 5 | Batch 33/94 | Batch Loss: 4.2354\n",
      "Epoch 5 | Batch 34/94 | Batch Loss: 4.2106\n",
      "Epoch 5 | Batch 35/94 | Batch Loss: 4.1272\n",
      "Epoch 5 | Batch 36/94 | Batch Loss: 4.2008\n",
      "Epoch 5 | Batch 37/94 | Batch Loss: 4.1604\n",
      "Epoch 5 | Batch 38/94 | Batch Loss: 4.1095\n",
      "Epoch 5 | Batch 39/94 | Batch Loss: 4.1453\n",
      "Epoch 5 | Batch 40/94 | Batch Loss: 4.1968\n",
      "Epoch 5 | Batch 41/94 | Batch Loss: 4.1324\n",
      "Epoch 5 | Batch 42/94 | Batch Loss: 4.0690\n",
      "Epoch 5 | Batch 43/94 | Batch Loss: 4.0432\n",
      "Epoch 5 | Batch 44/94 | Batch Loss: 4.1482\n",
      "Epoch 5 | Batch 45/94 | Batch Loss: 4.1052\n",
      "Epoch 5 | Batch 46/94 | Batch Loss: 4.1866\n",
      "Epoch 5 | Batch 47/94 | Batch Loss: 4.1330\n",
      "Epoch 5 | Batch 48/94 | Batch Loss: 4.0724\n",
      "Epoch 5 | Batch 49/94 | Batch Loss: 4.1405\n",
      "Epoch 5 | Batch 50/94 | Batch Loss: 4.1343\n",
      "Epoch 5 | Batch 51/94 | Batch Loss: 4.1592\n",
      "Epoch 5 | Batch 52/94 | Batch Loss: 4.1792\n",
      "Epoch 5 | Batch 53/94 | Batch Loss: 4.2442\n",
      "Epoch 5 | Batch 54/94 | Batch Loss: 4.1365\n",
      "Epoch 5 | Batch 55/94 | Batch Loss: 4.0467\n",
      "Epoch 5 | Batch 56/94 | Batch Loss: 4.1096\n",
      "Epoch 5 | Batch 57/94 | Batch Loss: 4.1552\n",
      "Epoch 5 | Batch 58/94 | Batch Loss: 4.1752\n",
      "Epoch 5 | Batch 59/94 | Batch Loss: 4.1276\n",
      "Epoch 5 | Batch 60/94 | Batch Loss: 4.0578\n",
      "Epoch 5 | Batch 61/94 | Batch Loss: 4.1619\n",
      "Epoch 5 | Batch 62/94 | Batch Loss: 4.0775\n",
      "Epoch 5 | Batch 63/94 | Batch Loss: 4.1502\n",
      "Epoch 5 | Batch 64/94 | Batch Loss: 4.0141\n",
      "Epoch 5 | Batch 65/94 | Batch Loss: 4.1162\n",
      "Epoch 5 | Batch 66/94 | Batch Loss: 4.1860\n",
      "Epoch 5 | Batch 67/94 | Batch Loss: 4.0790\n",
      "Epoch 5 | Batch 68/94 | Batch Loss: 4.1016\n",
      "Epoch 5 | Batch 69/94 | Batch Loss: 4.1191\n",
      "Epoch 5 | Batch 70/94 | Batch Loss: 4.1244\n",
      "Epoch 5 | Batch 71/94 | Batch Loss: 4.0528\n",
      "Epoch 5 | Batch 72/94 | Batch Loss: 4.0811\n",
      "Epoch 5 | Batch 73/94 | Batch Loss: 4.0590\n",
      "Epoch 5 | Batch 74/94 | Batch Loss: 4.0823\n",
      "Epoch 5 | Batch 75/94 | Batch Loss: 4.1353\n",
      "Epoch 5 | Batch 76/94 | Batch Loss: 4.0874\n",
      "Epoch 5 | Batch 77/94 | Batch Loss: 4.0819\n",
      "Epoch 5 | Batch 78/94 | Batch Loss: 3.9880\n",
      "Epoch 5 | Batch 79/94 | Batch Loss: 4.0823\n",
      "Epoch 5 | Batch 80/94 | Batch Loss: 4.1354\n",
      "Epoch 5 | Batch 81/94 | Batch Loss: 4.0396\n",
      "Epoch 5 | Batch 82/94 | Batch Loss: 4.1628\n",
      "Epoch 5 | Batch 83/94 | Batch Loss: 4.0802\n",
      "Epoch 5 | Batch 84/94 | Batch Loss: 4.0634\n",
      "Epoch 5 | Batch 85/94 | Batch Loss: 4.0416\n",
      "Epoch 5 | Batch 86/94 | Batch Loss: 4.0542\n",
      "Epoch 5 | Batch 87/94 | Batch Loss: 4.0980\n",
      "Epoch 5 | Batch 88/94 | Batch Loss: 4.0566\n",
      "Epoch 5 | Batch 89/94 | Batch Loss: 4.0166\n",
      "Epoch 5 | Batch 90/94 | Batch Loss: 4.0442\n",
      "Epoch 5 | Batch 91/94 | Batch Loss: 4.1919\n",
      "Epoch 5 | Batch 92/94 | Batch Loss: 3.9817\n",
      "Epoch 5 | Batch 93/94 | Batch Loss: 4.0491\n",
      "Epoch 5 | Batch 94/94 | Batch Loss: 4.0726\n",
      "Epoch 5 Finished | Average Loss: 4.1354\n",
      "\n",
      "Epoch 6 Starting...\n",
      "Epoch 6 | Batch 1/94 | Batch Loss: 4.0290\n",
      "Epoch 6 | Batch 2/94 | Batch Loss: 4.1054\n",
      "Epoch 6 | Batch 3/94 | Batch Loss: 4.0112\n",
      "Epoch 6 | Batch 4/94 | Batch Loss: 3.9883\n",
      "Epoch 6 | Batch 5/94 | Batch Loss: 3.9772\n",
      "Epoch 6 | Batch 6/94 | Batch Loss: 4.1183\n",
      "Epoch 6 | Batch 7/94 | Batch Loss: 4.0301\n",
      "Epoch 6 | Batch 8/94 | Batch Loss: 4.0846\n",
      "Epoch 6 | Batch 9/94 | Batch Loss: 4.0660\n",
      "Epoch 6 | Batch 10/94 | Batch Loss: 4.0681\n",
      "Epoch 6 | Batch 11/94 | Batch Loss: 3.9454\n",
      "Epoch 6 | Batch 12/94 | Batch Loss: 4.0904\n",
      "Epoch 6 | Batch 13/94 | Batch Loss: 4.0319\n",
      "Epoch 6 | Batch 14/94 | Batch Loss: 4.0252\n",
      "Epoch 6 | Batch 15/94 | Batch Loss: 4.0933\n",
      "Epoch 6 | Batch 16/94 | Batch Loss: 4.0220\n",
      "Epoch 6 | Batch 17/94 | Batch Loss: 4.0031\n",
      "Epoch 6 | Batch 18/94 | Batch Loss: 4.0065\n",
      "Epoch 6 | Batch 19/94 | Batch Loss: 4.1045\n",
      "Epoch 6 | Batch 20/94 | Batch Loss: 3.9949\n",
      "Epoch 6 | Batch 21/94 | Batch Loss: 3.9953\n",
      "Epoch 6 | Batch 22/94 | Batch Loss: 4.0558\n",
      "Epoch 6 | Batch 23/94 | Batch Loss: 4.0335\n",
      "Epoch 6 | Batch 24/94 | Batch Loss: 4.1013\n",
      "Epoch 6 | Batch 25/94 | Batch Loss: 4.0612\n",
      "Epoch 6 | Batch 26/94 | Batch Loss: 3.9706\n",
      "Epoch 6 | Batch 27/94 | Batch Loss: 3.9994\n",
      "Epoch 6 | Batch 28/94 | Batch Loss: 3.9418\n",
      "Epoch 6 | Batch 29/94 | Batch Loss: 4.0986\n",
      "Epoch 6 | Batch 30/94 | Batch Loss: 4.0371\n",
      "Epoch 6 | Batch 31/94 | Batch Loss: 4.1345\n",
      "Epoch 6 | Batch 32/94 | Batch Loss: 4.0096\n",
      "Epoch 6 | Batch 33/94 | Batch Loss: 3.8658\n",
      "Epoch 6 | Batch 34/94 | Batch Loss: 4.0616\n",
      "Epoch 6 | Batch 35/94 | Batch Loss: 3.9472\n",
      "Epoch 6 | Batch 36/94 | Batch Loss: 3.9326\n",
      "Epoch 6 | Batch 37/94 | Batch Loss: 4.0138\n",
      "Epoch 6 | Batch 38/94 | Batch Loss: 3.9500\n",
      "Epoch 6 | Batch 39/94 | Batch Loss: 4.0765\n",
      "Epoch 6 | Batch 40/94 | Batch Loss: 3.9990\n",
      "Epoch 6 | Batch 41/94 | Batch Loss: 3.9909\n",
      "Epoch 6 | Batch 42/94 | Batch Loss: 3.9894\n",
      "Epoch 6 | Batch 43/94 | Batch Loss: 4.0086\n",
      "Epoch 6 | Batch 44/94 | Batch Loss: 4.0278\n",
      "Epoch 6 | Batch 45/94 | Batch Loss: 4.0330\n",
      "Epoch 6 | Batch 46/94 | Batch Loss: 3.9623\n",
      "Epoch 6 | Batch 47/94 | Batch Loss: 3.9811\n",
      "Epoch 6 | Batch 48/94 | Batch Loss: 4.1591\n",
      "Epoch 6 | Batch 49/94 | Batch Loss: 4.0633\n",
      "Epoch 6 | Batch 50/94 | Batch Loss: 4.0109\n",
      "Epoch 6 | Batch 51/94 | Batch Loss: 4.0059\n",
      "Epoch 6 | Batch 52/94 | Batch Loss: 4.0138\n",
      "Epoch 6 | Batch 53/94 | Batch Loss: 3.9891\n",
      "Epoch 6 | Batch 54/94 | Batch Loss: 3.8957\n",
      "Epoch 6 | Batch 55/94 | Batch Loss: 3.9762\n",
      "Epoch 6 | Batch 56/94 | Batch Loss: 4.0103\n",
      "Epoch 6 | Batch 57/94 | Batch Loss: 4.0270\n",
      "Epoch 6 | Batch 58/94 | Batch Loss: 3.9190\n",
      "Epoch 6 | Batch 59/94 | Batch Loss: 3.9080\n",
      "Epoch 6 | Batch 60/94 | Batch Loss: 4.0142\n",
      "Epoch 6 | Batch 61/94 | Batch Loss: 3.9292\n",
      "Epoch 6 | Batch 62/94 | Batch Loss: 4.0733\n",
      "Epoch 6 | Batch 63/94 | Batch Loss: 3.8913\n",
      "Epoch 6 | Batch 64/94 | Batch Loss: 3.9578\n",
      "Epoch 6 | Batch 65/94 | Batch Loss: 4.0215\n",
      "Epoch 6 | Batch 66/94 | Batch Loss: 3.9858\n",
      "Epoch 6 | Batch 67/94 | Batch Loss: 4.0005\n",
      "Epoch 6 | Batch 68/94 | Batch Loss: 3.9332\n",
      "Epoch 6 | Batch 69/94 | Batch Loss: 4.0019\n",
      "Epoch 6 | Batch 70/94 | Batch Loss: 4.0330\n",
      "Epoch 6 | Batch 71/94 | Batch Loss: 3.9044\n",
      "Epoch 6 | Batch 72/94 | Batch Loss: 3.9387\n",
      "Epoch 6 | Batch 73/94 | Batch Loss: 4.0126\n",
      "Epoch 6 | Batch 74/94 | Batch Loss: 3.8263\n",
      "Epoch 6 | Batch 75/94 | Batch Loss: 3.9710\n",
      "Epoch 6 | Batch 76/94 | Batch Loss: 4.0011\n",
      "Epoch 6 | Batch 77/94 | Batch Loss: 3.9243\n",
      "Epoch 6 | Batch 78/94 | Batch Loss: 4.0373\n",
      "Epoch 6 | Batch 79/94 | Batch Loss: 3.9352\n",
      "Epoch 6 | Batch 80/94 | Batch Loss: 4.0698\n",
      "Epoch 6 | Batch 81/94 | Batch Loss: 3.9828\n",
      "Epoch 6 | Batch 82/94 | Batch Loss: 3.9901\n",
      "Epoch 6 | Batch 83/94 | Batch Loss: 3.9458\n",
      "Epoch 6 | Batch 84/94 | Batch Loss: 3.9618\n",
      "Epoch 6 | Batch 85/94 | Batch Loss: 3.8938\n",
      "Epoch 6 | Batch 86/94 | Batch Loss: 3.8907\n",
      "Epoch 6 | Batch 87/94 | Batch Loss: 3.8499\n",
      "Epoch 6 | Batch 88/94 | Batch Loss: 3.9114\n",
      "Epoch 6 | Batch 89/94 | Batch Loss: 3.9393\n",
      "Epoch 6 | Batch 90/94 | Batch Loss: 3.9134\n",
      "Epoch 6 | Batch 91/94 | Batch Loss: 3.8562\n",
      "Epoch 6 | Batch 92/94 | Batch Loss: 3.9791\n",
      "Epoch 6 | Batch 93/94 | Batch Loss: 3.9440\n",
      "Epoch 6 | Batch 94/94 | Batch Loss: 3.9064\n",
      "Epoch 6 Finished | Average Loss: 3.9945\n",
      "\n",
      "Epoch 7 Starting...\n",
      "Epoch 7 | Batch 1/94 | Batch Loss: 3.8457\n",
      "Epoch 7 | Batch 2/94 | Batch Loss: 3.9393\n",
      "Epoch 7 | Batch 3/94 | Batch Loss: 3.8602\n",
      "Epoch 7 | Batch 4/94 | Batch Loss: 3.9849\n",
      "Epoch 7 | Batch 5/94 | Batch Loss: 3.9142\n",
      "Epoch 7 | Batch 6/94 | Batch Loss: 3.9693\n",
      "Epoch 7 | Batch 7/94 | Batch Loss: 3.9411\n",
      "Epoch 7 | Batch 8/94 | Batch Loss: 3.9588\n",
      "Epoch 7 | Batch 9/94 | Batch Loss: 3.8368\n",
      "Epoch 7 | Batch 10/94 | Batch Loss: 3.8108\n",
      "Epoch 7 | Batch 11/94 | Batch Loss: 3.8897\n",
      "Epoch 7 | Batch 12/94 | Batch Loss: 3.8657\n",
      "Epoch 7 | Batch 13/94 | Batch Loss: 3.9496\n",
      "Epoch 7 | Batch 14/94 | Batch Loss: 3.8761\n",
      "Epoch 7 | Batch 15/94 | Batch Loss: 3.8835\n",
      "Epoch 7 | Batch 16/94 | Batch Loss: 3.9656\n",
      "Epoch 7 | Batch 17/94 | Batch Loss: 3.8714\n",
      "Epoch 7 | Batch 18/94 | Batch Loss: 3.9328\n",
      "Epoch 7 | Batch 19/94 | Batch Loss: 3.9787\n",
      "Epoch 7 | Batch 20/94 | Batch Loss: 3.8328\n",
      "Epoch 7 | Batch 21/94 | Batch Loss: 3.8916\n",
      "Epoch 7 | Batch 22/94 | Batch Loss: 4.0052\n",
      "Epoch 7 | Batch 23/94 | Batch Loss: 3.9950\n",
      "Epoch 7 | Batch 24/94 | Batch Loss: 3.9353\n",
      "Epoch 7 | Batch 25/94 | Batch Loss: 3.8950\n",
      "Epoch 7 | Batch 26/94 | Batch Loss: 3.8728\n",
      "Epoch 7 | Batch 27/94 | Batch Loss: 3.9177\n",
      "Epoch 7 | Batch 28/94 | Batch Loss: 3.9113\n",
      "Epoch 7 | Batch 29/94 | Batch Loss: 3.9464\n",
      "Epoch 7 | Batch 30/94 | Batch Loss: 3.8687\n",
      "Epoch 7 | Batch 31/94 | Batch Loss: 3.8663\n",
      "Epoch 7 | Batch 32/94 | Batch Loss: 3.7633\n",
      "Epoch 7 | Batch 33/94 | Batch Loss: 3.8041\n",
      "Epoch 7 | Batch 34/94 | Batch Loss: 3.8411\n",
      "Epoch 7 | Batch 35/94 | Batch Loss: 3.8836\n",
      "Epoch 7 | Batch 36/94 | Batch Loss: 3.9073\n",
      "Epoch 7 | Batch 37/94 | Batch Loss: 3.9140\n",
      "Epoch 7 | Batch 38/94 | Batch Loss: 3.8335\n",
      "Epoch 7 | Batch 39/94 | Batch Loss: 3.8929\n",
      "Epoch 7 | Batch 40/94 | Batch Loss: 3.7849\n",
      "Epoch 7 | Batch 41/94 | Batch Loss: 3.8448\n",
      "Epoch 7 | Batch 42/94 | Batch Loss: 3.9543\n",
      "Epoch 7 | Batch 43/94 | Batch Loss: 3.9415\n",
      "Epoch 7 | Batch 44/94 | Batch Loss: 3.8760\n",
      "Epoch 7 | Batch 45/94 | Batch Loss: 3.7791\n",
      "Epoch 7 | Batch 46/94 | Batch Loss: 3.8695\n",
      "Epoch 7 | Batch 47/94 | Batch Loss: 3.8024\n",
      "Epoch 7 | Batch 48/94 | Batch Loss: 3.8847\n",
      "Epoch 7 | Batch 49/94 | Batch Loss: 3.9356\n",
      "Epoch 7 | Batch 50/94 | Batch Loss: 3.9062\n",
      "Epoch 7 | Batch 51/94 | Batch Loss: 3.8062\n",
      "Epoch 7 | Batch 52/94 | Batch Loss: 3.9405\n",
      "Epoch 7 | Batch 53/94 | Batch Loss: 3.8856\n",
      "Epoch 7 | Batch 54/94 | Batch Loss: 3.7854\n",
      "Epoch 7 | Batch 55/94 | Batch Loss: 3.8971\n",
      "Epoch 7 | Batch 56/94 | Batch Loss: 3.9193\n",
      "Epoch 7 | Batch 57/94 | Batch Loss: 3.8551\n",
      "Epoch 7 | Batch 58/94 | Batch Loss: 3.8967\n",
      "Epoch 7 | Batch 59/94 | Batch Loss: 3.8828\n",
      "Epoch 7 | Batch 60/94 | Batch Loss: 3.9210\n",
      "Epoch 7 | Batch 61/94 | Batch Loss: 3.9301\n",
      "Epoch 7 | Batch 62/94 | Batch Loss: 3.8433\n",
      "Epoch 7 | Batch 63/94 | Batch Loss: 3.9788\n",
      "Epoch 7 | Batch 64/94 | Batch Loss: 3.9793\n",
      "Epoch 7 | Batch 65/94 | Batch Loss: 3.8938\n",
      "Epoch 7 | Batch 66/94 | Batch Loss: 3.9000\n",
      "Epoch 7 | Batch 67/94 | Batch Loss: 3.8261\n",
      "Epoch 7 | Batch 68/94 | Batch Loss: 3.8503\n",
      "Epoch 7 | Batch 69/94 | Batch Loss: 3.8521\n",
      "Epoch 7 | Batch 70/94 | Batch Loss: 3.9374\n",
      "Epoch 7 | Batch 71/94 | Batch Loss: 3.8856\n",
      "Epoch 7 | Batch 72/94 | Batch Loss: 3.7941\n",
      "Epoch 7 | Batch 73/94 | Batch Loss: 3.8861\n",
      "Epoch 7 | Batch 74/94 | Batch Loss: 3.8386\n",
      "Epoch 7 | Batch 75/94 | Batch Loss: 3.8096\n",
      "Epoch 7 | Batch 76/94 | Batch Loss: 3.7872\n",
      "Epoch 7 | Batch 77/94 | Batch Loss: 3.8748\n",
      "Epoch 7 | Batch 78/94 | Batch Loss: 3.9254\n",
      "Epoch 7 | Batch 79/94 | Batch Loss: 3.8331\n",
      "Epoch 7 | Batch 80/94 | Batch Loss: 3.9158\n",
      "Epoch 7 | Batch 81/94 | Batch Loss: 3.8639\n",
      "Epoch 7 | Batch 82/94 | Batch Loss: 3.8416\n",
      "Epoch 7 | Batch 83/94 | Batch Loss: 3.7311\n",
      "Epoch 7 | Batch 84/94 | Batch Loss: 3.8074\n",
      "Epoch 7 | Batch 85/94 | Batch Loss: 3.8339\n",
      "Epoch 7 | Batch 86/94 | Batch Loss: 3.8741\n",
      "Epoch 7 | Batch 87/94 | Batch Loss: 3.9549\n",
      "Epoch 7 | Batch 88/94 | Batch Loss: 3.9166\n",
      "Epoch 7 | Batch 89/94 | Batch Loss: 3.8721\n",
      "Epoch 7 | Batch 90/94 | Batch Loss: 3.7851\n",
      "Epoch 7 | Batch 91/94 | Batch Loss: 3.9269\n",
      "Epoch 7 | Batch 92/94 | Batch Loss: 3.8239\n",
      "Epoch 7 | Batch 93/94 | Batch Loss: 3.8673\n",
      "Epoch 7 | Batch 94/94 | Batch Loss: 3.8376\n",
      "Epoch 7 Finished | Average Loss: 3.8819\n",
      "\n",
      "Epoch 8 Starting...\n",
      "Epoch 8 | Batch 1/94 | Batch Loss: 3.7960\n",
      "Epoch 8 | Batch 2/94 | Batch Loss: 3.8626\n",
      "Epoch 8 | Batch 3/94 | Batch Loss: 3.8926\n",
      "Epoch 8 | Batch 4/94 | Batch Loss: 3.8201\n",
      "Epoch 8 | Batch 5/94 | Batch Loss: 3.8195\n",
      "Epoch 8 | Batch 6/94 | Batch Loss: 3.9193\n",
      "Epoch 8 | Batch 7/94 | Batch Loss: 3.8544\n",
      "Epoch 8 | Batch 8/94 | Batch Loss: 3.7788\n",
      "Epoch 8 | Batch 9/94 | Batch Loss: 3.8250\n",
      "Epoch 8 | Batch 10/94 | Batch Loss: 3.8606\n",
      "Epoch 8 | Batch 11/94 | Batch Loss: 3.8353\n",
      "Epoch 8 | Batch 12/94 | Batch Loss: 3.7722\n",
      "Epoch 8 | Batch 13/94 | Batch Loss: 3.7345\n",
      "Epoch 8 | Batch 14/94 | Batch Loss: 3.7437\n",
      "Epoch 8 | Batch 15/94 | Batch Loss: 3.8148\n",
      "Epoch 8 | Batch 16/94 | Batch Loss: 3.8991\n",
      "Epoch 8 | Batch 17/94 | Batch Loss: 3.8780\n",
      "Epoch 8 | Batch 18/94 | Batch Loss: 3.8404\n",
      "Epoch 8 | Batch 19/94 | Batch Loss: 3.8216\n",
      "Epoch 8 | Batch 20/94 | Batch Loss: 3.8277\n",
      "Epoch 8 | Batch 21/94 | Batch Loss: 3.8596\n",
      "Epoch 8 | Batch 22/94 | Batch Loss: 3.7865\n",
      "Epoch 8 | Batch 23/94 | Batch Loss: 3.7750\n",
      "Epoch 8 | Batch 24/94 | Batch Loss: 3.8558\n",
      "Epoch 8 | Batch 25/94 | Batch Loss: 3.7540\n",
      "Epoch 8 | Batch 26/94 | Batch Loss: 3.8311\n",
      "Epoch 8 | Batch 27/94 | Batch Loss: 3.7564\n",
      "Epoch 8 | Batch 28/94 | Batch Loss: 3.8076\n",
      "Epoch 8 | Batch 29/94 | Batch Loss: 3.7179\n",
      "Epoch 8 | Batch 30/94 | Batch Loss: 3.7309\n",
      "Epoch 8 | Batch 31/94 | Batch Loss: 3.9434\n",
      "Epoch 8 | Batch 32/94 | Batch Loss: 3.8217\n",
      "Epoch 8 | Batch 33/94 | Batch Loss: 3.7797\n",
      "Epoch 8 | Batch 34/94 | Batch Loss: 3.9291\n",
      "Epoch 8 | Batch 35/94 | Batch Loss: 3.7381\n",
      "Epoch 8 | Batch 36/94 | Batch Loss: 3.7447\n",
      "Epoch 8 | Batch 37/94 | Batch Loss: 3.7762\n",
      "Epoch 8 | Batch 38/94 | Batch Loss: 3.7988\n",
      "Epoch 8 | Batch 39/94 | Batch Loss: 3.8620\n",
      "Epoch 8 | Batch 40/94 | Batch Loss: 3.7563\n",
      "Epoch 8 | Batch 41/94 | Batch Loss: 3.6610\n",
      "Epoch 8 | Batch 42/94 | Batch Loss: 3.7202\n",
      "Epoch 8 | Batch 43/94 | Batch Loss: 3.7490\n",
      "Epoch 8 | Batch 44/94 | Batch Loss: 3.7736\n",
      "Epoch 8 | Batch 45/94 | Batch Loss: 3.7903\n",
      "Epoch 8 | Batch 46/94 | Batch Loss: 3.8762\n",
      "Epoch 8 | Batch 47/94 | Batch Loss: 3.8699\n",
      "Epoch 8 | Batch 48/94 | Batch Loss: 3.8093\n",
      "Epoch 8 | Batch 49/94 | Batch Loss: 3.6686\n",
      "Epoch 8 | Batch 50/94 | Batch Loss: 3.7248\n",
      "Epoch 8 | Batch 51/94 | Batch Loss: 3.8700\n",
      "Epoch 8 | Batch 52/94 | Batch Loss: 3.7728\n",
      "Epoch 8 | Batch 53/94 | Batch Loss: 3.7182\n",
      "Epoch 8 | Batch 54/94 | Batch Loss: 3.7609\n",
      "Epoch 8 | Batch 55/94 | Batch Loss: 3.7725\n",
      "Epoch 8 | Batch 56/94 | Batch Loss: 3.7665\n",
      "Epoch 8 | Batch 57/94 | Batch Loss: 3.7803\n",
      "Epoch 8 | Batch 58/94 | Batch Loss: 3.6986\n",
      "Epoch 8 | Batch 59/94 | Batch Loss: 3.6855\n",
      "Epoch 8 | Batch 60/94 | Batch Loss: 3.7425\n",
      "Epoch 8 | Batch 61/94 | Batch Loss: 3.7587\n",
      "Epoch 8 | Batch 62/94 | Batch Loss: 3.8312\n",
      "Epoch 8 | Batch 63/94 | Batch Loss: 3.7901\n",
      "Epoch 8 | Batch 64/94 | Batch Loss: 3.7622\n",
      "Epoch 8 | Batch 65/94 | Batch Loss: 3.8077\n",
      "Epoch 8 | Batch 66/94 | Batch Loss: 3.8407\n",
      "Epoch 8 | Batch 67/94 | Batch Loss: 3.7402\n",
      "Epoch 8 | Batch 68/94 | Batch Loss: 3.7037\n",
      "Epoch 8 | Batch 69/94 | Batch Loss: 3.7824\n",
      "Epoch 8 | Batch 70/94 | Batch Loss: 3.8032\n",
      "Epoch 8 | Batch 71/94 | Batch Loss: 3.8319\n",
      "Epoch 8 | Batch 72/94 | Batch Loss: 3.8207\n",
      "Epoch 8 | Batch 73/94 | Batch Loss: 3.7569\n",
      "Epoch 8 | Batch 74/94 | Batch Loss: 3.8581\n",
      "Epoch 8 | Batch 75/94 | Batch Loss: 3.6641\n",
      "Epoch 8 | Batch 76/94 | Batch Loss: 3.6864\n",
      "Epoch 8 | Batch 77/94 | Batch Loss: 3.7108\n",
      "Epoch 8 | Batch 78/94 | Batch Loss: 3.7665\n",
      "Epoch 8 | Batch 79/94 | Batch Loss: 3.7617\n",
      "Epoch 8 | Batch 80/94 | Batch Loss: 3.6810\n",
      "Epoch 8 | Batch 81/94 | Batch Loss: 3.7136\n",
      "Epoch 8 | Batch 82/94 | Batch Loss: 3.8091\n",
      "Epoch 8 | Batch 83/94 | Batch Loss: 3.7678\n",
      "Epoch 8 | Batch 84/94 | Batch Loss: 3.6881\n",
      "Epoch 8 | Batch 85/94 | Batch Loss: 3.6598\n",
      "Epoch 8 | Batch 86/94 | Batch Loss: 3.7190\n",
      "Epoch 8 | Batch 87/94 | Batch Loss: 3.8334\n",
      "Epoch 8 | Batch 88/94 | Batch Loss: 3.7575\n",
      "Epoch 8 | Batch 89/94 | Batch Loss: 3.9060\n",
      "Epoch 8 | Batch 90/94 | Batch Loss: 3.6823\n",
      "Epoch 8 | Batch 91/94 | Batch Loss: 3.8352\n",
      "Epoch 8 | Batch 92/94 | Batch Loss: 3.7262\n",
      "Epoch 8 | Batch 93/94 | Batch Loss: 3.7271\n",
      "Epoch 8 | Batch 94/94 | Batch Loss: 3.6711\n",
      "Epoch 8 Finished | Average Loss: 3.7839\n",
      "\n",
      "Epoch 9 Starting...\n",
      "Epoch 9 | Batch 1/94 | Batch Loss: 3.8038\n",
      "Epoch 9 | Batch 2/94 | Batch Loss: 3.8371\n",
      "Epoch 9 | Batch 3/94 | Batch Loss: 3.7631\n",
      "Epoch 9 | Batch 4/94 | Batch Loss: 3.8315\n",
      "Epoch 9 | Batch 5/94 | Batch Loss: 3.6813\n",
      "Epoch 9 | Batch 6/94 | Batch Loss: 3.6617\n",
      "Epoch 9 | Batch 7/94 | Batch Loss: 3.5808\n",
      "Epoch 9 | Batch 8/94 | Batch Loss: 3.6681\n",
      "Epoch 9 | Batch 9/94 | Batch Loss: 3.6546\n",
      "Epoch 9 | Batch 10/94 | Batch Loss: 3.7375\n",
      "Epoch 9 | Batch 11/94 | Batch Loss: 3.7573\n",
      "Epoch 9 | Batch 12/94 | Batch Loss: 3.7901\n",
      "Epoch 9 | Batch 13/94 | Batch Loss: 3.7209\n",
      "Epoch 9 | Batch 14/94 | Batch Loss: 3.6718\n",
      "Epoch 9 | Batch 15/94 | Batch Loss: 3.7303\n",
      "Epoch 9 | Batch 16/94 | Batch Loss: 3.6994\n",
      "Epoch 9 | Batch 17/94 | Batch Loss: 3.7773\n",
      "Epoch 9 | Batch 18/94 | Batch Loss: 3.7667\n",
      "Epoch 9 | Batch 19/94 | Batch Loss: 3.6878\n",
      "Epoch 9 | Batch 20/94 | Batch Loss: 3.6855\n",
      "Epoch 9 | Batch 21/94 | Batch Loss: 3.7275\n",
      "Epoch 9 | Batch 22/94 | Batch Loss: 3.7071\n",
      "Epoch 9 | Batch 23/94 | Batch Loss: 3.6930\n",
      "Epoch 9 | Batch 24/94 | Batch Loss: 3.7100\n",
      "Epoch 9 | Batch 25/94 | Batch Loss: 3.7310\n",
      "Epoch 9 | Batch 26/94 | Batch Loss: 3.7603\n",
      "Epoch 9 | Batch 27/94 | Batch Loss: 3.7628\n",
      "Epoch 9 | Batch 28/94 | Batch Loss: 3.6538\n",
      "Epoch 9 | Batch 29/94 | Batch Loss: 3.6795\n",
      "Epoch 9 | Batch 30/94 | Batch Loss: 3.5876\n",
      "Epoch 9 | Batch 31/94 | Batch Loss: 3.7295\n",
      "Epoch 9 | Batch 32/94 | Batch Loss: 3.6193\n",
      "Epoch 9 | Batch 33/94 | Batch Loss: 3.7208\n",
      "Epoch 9 | Batch 34/94 | Batch Loss: 3.6805\n",
      "Epoch 9 | Batch 35/94 | Batch Loss: 3.6708\n",
      "Epoch 9 | Batch 36/94 | Batch Loss: 3.7115\n",
      "Epoch 9 | Batch 37/94 | Batch Loss: 3.6717\n",
      "Epoch 9 | Batch 38/94 | Batch Loss: 3.7400\n",
      "Epoch 9 | Batch 39/94 | Batch Loss: 3.7227\n",
      "Epoch 9 | Batch 40/94 | Batch Loss: 3.6267\n",
      "Epoch 9 | Batch 41/94 | Batch Loss: 3.6091\n",
      "Epoch 9 | Batch 42/94 | Batch Loss: 3.7265\n",
      "Epoch 9 | Batch 43/94 | Batch Loss: 3.6308\n",
      "Epoch 9 | Batch 44/94 | Batch Loss: 3.7037\n",
      "Epoch 9 | Batch 45/94 | Batch Loss: 3.7683\n",
      "Epoch 9 | Batch 46/94 | Batch Loss: 3.7459\n",
      "Epoch 9 | Batch 47/94 | Batch Loss: 3.7000\n",
      "Epoch 9 | Batch 48/94 | Batch Loss: 3.6975\n",
      "Epoch 9 | Batch 49/94 | Batch Loss: 3.7319\n",
      "Epoch 9 | Batch 50/94 | Batch Loss: 3.7033\n",
      "Epoch 9 | Batch 51/94 | Batch Loss: 3.7155\n",
      "Epoch 9 | Batch 52/94 | Batch Loss: 3.6497\n",
      "Epoch 9 | Batch 53/94 | Batch Loss: 3.7019\n",
      "Epoch 9 | Batch 54/94 | Batch Loss: 3.7115\n",
      "Epoch 9 | Batch 55/94 | Batch Loss: 3.6784\n",
      "Epoch 9 | Batch 56/94 | Batch Loss: 3.7135\n",
      "Epoch 9 | Batch 57/94 | Batch Loss: 3.7320\n",
      "Epoch 9 | Batch 58/94 | Batch Loss: 3.7242\n",
      "Epoch 9 | Batch 59/94 | Batch Loss: 3.7748\n",
      "Epoch 9 | Batch 60/94 | Batch Loss: 3.6645\n",
      "Epoch 9 | Batch 61/94 | Batch Loss: 3.6607\n",
      "Epoch 9 | Batch 62/94 | Batch Loss: 3.7222\n",
      "Epoch 9 | Batch 63/94 | Batch Loss: 3.6382\n",
      "Epoch 9 | Batch 64/94 | Batch Loss: 3.6017\n",
      "Epoch 9 | Batch 65/94 | Batch Loss: 3.7707\n",
      "Epoch 9 | Batch 66/94 | Batch Loss: 3.7561\n",
      "Epoch 9 | Batch 67/94 | Batch Loss: 3.7494\n",
      "Epoch 9 | Batch 68/94 | Batch Loss: 3.7942\n",
      "Epoch 9 | Batch 69/94 | Batch Loss: 3.7029\n",
      "Epoch 9 | Batch 70/94 | Batch Loss: 3.6666\n",
      "Epoch 9 | Batch 71/94 | Batch Loss: 3.7523\n",
      "Epoch 9 | Batch 72/94 | Batch Loss: 3.7311\n",
      "Epoch 9 | Batch 73/94 | Batch Loss: 3.6772\n",
      "Epoch 9 | Batch 74/94 | Batch Loss: 3.7562\n",
      "Epoch 9 | Batch 75/94 | Batch Loss: 3.6428\n",
      "Epoch 9 | Batch 76/94 | Batch Loss: 3.6844\n",
      "Epoch 9 | Batch 77/94 | Batch Loss: 3.7397\n",
      "Epoch 9 | Batch 78/94 | Batch Loss: 3.6789\n",
      "Epoch 9 | Batch 79/94 | Batch Loss: 3.7087\n",
      "Epoch 9 | Batch 80/94 | Batch Loss: 3.6530\n",
      "Epoch 9 | Batch 81/94 | Batch Loss: 3.6537\n",
      "Epoch 9 | Batch 82/94 | Batch Loss: 3.6556\n",
      "Epoch 9 | Batch 83/94 | Batch Loss: 3.6617\n",
      "Epoch 9 | Batch 84/94 | Batch Loss: 3.7139\n",
      "Epoch 9 | Batch 85/94 | Batch Loss: 3.7514\n",
      "Epoch 9 | Batch 86/94 | Batch Loss: 3.6572\n",
      "Epoch 9 | Batch 87/94 | Batch Loss: 3.5782\n",
      "Epoch 9 | Batch 88/94 | Batch Loss: 3.5485\n",
      "Epoch 9 | Batch 89/94 | Batch Loss: 3.6918\n",
      "Epoch 9 | Batch 90/94 | Batch Loss: 3.6600\n",
      "Epoch 9 | Batch 91/94 | Batch Loss: 3.7148\n",
      "Epoch 9 | Batch 92/94 | Batch Loss: 3.7084\n",
      "Epoch 9 | Batch 93/94 | Batch Loss: 3.6990\n",
      "Epoch 9 | Batch 94/94 | Batch Loss: 3.6777\n",
      "Epoch 9 Finished | Average Loss: 3.7016\n",
      "\n",
      "Epoch 10 Starting...\n",
      "Epoch 10 | Batch 1/94 | Batch Loss: 3.7183\n",
      "Epoch 10 | Batch 2/94 | Batch Loss: 3.5452\n",
      "Epoch 10 | Batch 3/94 | Batch Loss: 3.6665\n",
      "Epoch 10 | Batch 4/94 | Batch Loss: 3.6330\n",
      "Epoch 10 | Batch 5/94 | Batch Loss: 3.7658\n",
      "Epoch 10 | Batch 6/94 | Batch Loss: 3.6284\n",
      "Epoch 10 | Batch 7/94 | Batch Loss: 3.6342\n",
      "Epoch 10 | Batch 8/94 | Batch Loss: 3.5487\n",
      "Epoch 10 | Batch 9/94 | Batch Loss: 3.6143\n",
      "Epoch 10 | Batch 10/94 | Batch Loss: 3.7014\n",
      "Epoch 10 | Batch 11/94 | Batch Loss: 3.5853\n",
      "Epoch 10 | Batch 12/94 | Batch Loss: 3.5351\n",
      "Epoch 10 | Batch 13/94 | Batch Loss: 3.6429\n",
      "Epoch 10 | Batch 14/94 | Batch Loss: 3.6734\n",
      "Epoch 10 | Batch 15/94 | Batch Loss: 3.7330\n",
      "Epoch 10 | Batch 16/94 | Batch Loss: 3.5243\n",
      "Epoch 10 | Batch 17/94 | Batch Loss: 3.6627\n",
      "Epoch 10 | Batch 18/94 | Batch Loss: 3.6846\n",
      "Epoch 10 | Batch 19/94 | Batch Loss: 3.5882\n",
      "Epoch 10 | Batch 20/94 | Batch Loss: 3.7054\n",
      "Epoch 10 | Batch 21/94 | Batch Loss: 3.5961\n",
      "Epoch 10 | Batch 22/94 | Batch Loss: 3.6798\n",
      "Epoch 10 | Batch 23/94 | Batch Loss: 3.6997\n",
      "Epoch 10 | Batch 24/94 | Batch Loss: 3.6759\n",
      "Epoch 10 | Batch 25/94 | Batch Loss: 3.6074\n",
      "Epoch 10 | Batch 26/94 | Batch Loss: 3.5770\n",
      "Epoch 10 | Batch 27/94 | Batch Loss: 3.6663\n",
      "Epoch 10 | Batch 28/94 | Batch Loss: 3.6084\n",
      "Epoch 10 | Batch 29/94 | Batch Loss: 3.6938\n",
      "Epoch 10 | Batch 30/94 | Batch Loss: 3.6650\n",
      "Epoch 10 | Batch 31/94 | Batch Loss: 3.6086\n",
      "Epoch 10 | Batch 32/94 | Batch Loss: 3.5960\n",
      "Epoch 10 | Batch 33/94 | Batch Loss: 3.6496\n",
      "Epoch 10 | Batch 34/94 | Batch Loss: 3.6667\n",
      "Epoch 10 | Batch 35/94 | Batch Loss: 3.5420\n",
      "Epoch 10 | Batch 36/94 | Batch Loss: 3.7010\n",
      "Epoch 10 | Batch 37/94 | Batch Loss: 3.6676\n",
      "Epoch 10 | Batch 38/94 | Batch Loss: 3.5982\n",
      "Epoch 10 | Batch 39/94 | Batch Loss: 3.6017\n",
      "Epoch 10 | Batch 40/94 | Batch Loss: 3.6311\n",
      "Epoch 10 | Batch 41/94 | Batch Loss: 3.7031\n",
      "Epoch 10 | Batch 42/94 | Batch Loss: 3.6574\n",
      "Epoch 10 | Batch 43/94 | Batch Loss: 3.6998\n",
      "Epoch 10 | Batch 44/94 | Batch Loss: 3.5968\n",
      "Epoch 10 | Batch 45/94 | Batch Loss: 3.6247\n",
      "Epoch 10 | Batch 46/94 | Batch Loss: 3.6426\n",
      "Epoch 10 | Batch 47/94 | Batch Loss: 3.5587\n",
      "Epoch 10 | Batch 48/94 | Batch Loss: 3.6290\n",
      "Epoch 10 | Batch 49/94 | Batch Loss: 3.6457\n",
      "Epoch 10 | Batch 50/94 | Batch Loss: 3.7228\n",
      "Epoch 10 | Batch 51/94 | Batch Loss: 3.7722\n",
      "Epoch 10 | Batch 52/94 | Batch Loss: 3.4693\n",
      "Epoch 10 | Batch 53/94 | Batch Loss: 3.6635\n",
      "Epoch 10 | Batch 54/94 | Batch Loss: 3.6000\n",
      "Epoch 10 | Batch 55/94 | Batch Loss: 3.7049\n",
      "Epoch 10 | Batch 56/94 | Batch Loss: 3.5420\n",
      "Epoch 10 | Batch 57/94 | Batch Loss: 3.6653\n",
      "Epoch 10 | Batch 58/94 | Batch Loss: 3.6109\n",
      "Epoch 10 | Batch 59/94 | Batch Loss: 3.6591\n",
      "Epoch 10 | Batch 60/94 | Batch Loss: 3.6408\n",
      "Epoch 10 | Batch 61/94 | Batch Loss: 3.6841\n",
      "Epoch 10 | Batch 62/94 | Batch Loss: 3.6164\n",
      "Epoch 10 | Batch 63/94 | Batch Loss: 3.6911\n",
      "Epoch 10 | Batch 64/94 | Batch Loss: 3.5733\n",
      "Epoch 10 | Batch 65/94 | Batch Loss: 3.6077\n",
      "Epoch 10 | Batch 66/94 | Batch Loss: 3.5640\n",
      "Epoch 10 | Batch 67/94 | Batch Loss: 3.5653\n",
      "Epoch 10 | Batch 68/94 | Batch Loss: 3.5192\n",
      "Epoch 10 | Batch 69/94 | Batch Loss: 3.5802\n",
      "Epoch 10 | Batch 70/94 | Batch Loss: 3.5854\n",
      "Epoch 10 | Batch 71/94 | Batch Loss: 3.6695\n",
      "Epoch 10 | Batch 72/94 | Batch Loss: 3.6103\n",
      "Epoch 10 | Batch 73/94 | Batch Loss: 3.6296\n",
      "Epoch 10 | Batch 74/94 | Batch Loss: 3.5771\n",
      "Epoch 10 | Batch 75/94 | Batch Loss: 3.6080\n",
      "Epoch 10 | Batch 76/94 | Batch Loss: 3.6232\n",
      "Epoch 10 | Batch 77/94 | Batch Loss: 3.5911\n",
      "Epoch 10 | Batch 78/94 | Batch Loss: 3.6497\n",
      "Epoch 10 | Batch 79/94 | Batch Loss: 3.5534\n",
      "Epoch 10 | Batch 80/94 | Batch Loss: 3.5220\n",
      "Epoch 10 | Batch 81/94 | Batch Loss: 3.6579\n",
      "Epoch 10 | Batch 82/94 | Batch Loss: 3.7142\n",
      "Epoch 10 | Batch 83/94 | Batch Loss: 3.5529\n",
      "Epoch 10 | Batch 84/94 | Batch Loss: 3.6344\n",
      "Epoch 10 | Batch 85/94 | Batch Loss: 3.5671\n",
      "Epoch 10 | Batch 86/94 | Batch Loss: 3.5973\n",
      "Epoch 10 | Batch 87/94 | Batch Loss: 3.6172\n",
      "Epoch 10 | Batch 88/94 | Batch Loss: 3.6829\n",
      "Epoch 10 | Batch 89/94 | Batch Loss: 3.5703\n",
      "Epoch 10 | Batch 90/94 | Batch Loss: 3.7030\n",
      "Epoch 10 | Batch 91/94 | Batch Loss: 3.4675\n",
      "Epoch 10 | Batch 92/94 | Batch Loss: 3.6167\n",
      "Epoch 10 | Batch 93/94 | Batch Loss: 3.7435\n",
      "Epoch 10 | Batch 94/94 | Batch Loss: 3.5337\n",
      "Epoch 10 Finished | Average Loss: 3.6278\n",
      "\n",
      "Epoch 11 Starting...\n",
      "Epoch 11 | Batch 1/94 | Batch Loss: 3.6047\n",
      "Epoch 11 | Batch 2/94 | Batch Loss: 3.6377\n",
      "Epoch 11 | Batch 3/94 | Batch Loss: 3.5801\n",
      "Epoch 11 | Batch 4/94 | Batch Loss: 3.5385\n",
      "Epoch 11 | Batch 5/94 | Batch Loss: 3.5345\n",
      "Epoch 11 | Batch 6/94 | Batch Loss: 3.5981\n",
      "Epoch 11 | Batch 7/94 | Batch Loss: 3.5981\n",
      "Epoch 11 | Batch 8/94 | Batch Loss: 3.5529\n",
      "Epoch 11 | Batch 9/94 | Batch Loss: 3.5935\n",
      "Epoch 11 | Batch 10/94 | Batch Loss: 3.5099\n",
      "Epoch 11 | Batch 11/94 | Batch Loss: 3.4956\n",
      "Epoch 11 | Batch 12/94 | Batch Loss: 3.6119\n",
      "Epoch 11 | Batch 13/94 | Batch Loss: 3.6033\n",
      "Epoch 11 | Batch 14/94 | Batch Loss: 3.5332\n",
      "Epoch 11 | Batch 15/94 | Batch Loss: 3.6147\n",
      "Epoch 11 | Batch 16/94 | Batch Loss: 3.6365\n",
      "Epoch 11 | Batch 17/94 | Batch Loss: 3.6732\n",
      "Epoch 11 | Batch 18/94 | Batch Loss: 3.6678\n",
      "Epoch 11 | Batch 19/94 | Batch Loss: 3.4615\n",
      "Epoch 11 | Batch 20/94 | Batch Loss: 3.4911\n",
      "Epoch 11 | Batch 21/94 | Batch Loss: 3.5120\n",
      "Epoch 11 | Batch 22/94 | Batch Loss: 3.5886\n",
      "Epoch 11 | Batch 23/94 | Batch Loss: 3.5795\n",
      "Epoch 11 | Batch 24/94 | Batch Loss: 3.5880\n",
      "Epoch 11 | Batch 25/94 | Batch Loss: 3.4624\n",
      "Epoch 11 | Batch 26/94 | Batch Loss: 3.6719\n",
      "Epoch 11 | Batch 27/94 | Batch Loss: 3.5062\n",
      "Epoch 11 | Batch 28/94 | Batch Loss: 3.5476\n",
      "Epoch 11 | Batch 29/94 | Batch Loss: 3.5442\n",
      "Epoch 11 | Batch 30/94 | Batch Loss: 3.6517\n",
      "Epoch 11 | Batch 31/94 | Batch Loss: 3.7082\n",
      "Epoch 11 | Batch 32/94 | Batch Loss: 3.5823\n",
      "Epoch 11 | Batch 33/94 | Batch Loss: 3.5313\n",
      "Epoch 11 | Batch 34/94 | Batch Loss: 3.6729\n",
      "Epoch 11 | Batch 35/94 | Batch Loss: 3.6461\n",
      "Epoch 11 | Batch 36/94 | Batch Loss: 3.6011\n",
      "Epoch 11 | Batch 37/94 | Batch Loss: 3.5134\n",
      "Epoch 11 | Batch 38/94 | Batch Loss: 3.6328\n",
      "Epoch 11 | Batch 39/94 | Batch Loss: 3.5064\n",
      "Epoch 11 | Batch 40/94 | Batch Loss: 3.5627\n",
      "Epoch 11 | Batch 41/94 | Batch Loss: 3.5188\n",
      "Epoch 11 | Batch 42/94 | Batch Loss: 3.5033\n",
      "Epoch 11 | Batch 43/94 | Batch Loss: 3.5735\n",
      "Epoch 11 | Batch 44/94 | Batch Loss: 3.5490\n",
      "Epoch 11 | Batch 45/94 | Batch Loss: 3.6343\n",
      "Epoch 11 | Batch 46/94 | Batch Loss: 3.5763\n",
      "Epoch 11 | Batch 47/94 | Batch Loss: 3.5982\n",
      "Epoch 11 | Batch 48/94 | Batch Loss: 3.6893\n",
      "Epoch 11 | Batch 49/94 | Batch Loss: 3.5481\n",
      "Epoch 11 | Batch 50/94 | Batch Loss: 3.6113\n",
      "Epoch 11 | Batch 51/94 | Batch Loss: 3.5111\n",
      "Epoch 11 | Batch 52/94 | Batch Loss: 3.5224\n",
      "Epoch 11 | Batch 53/94 | Batch Loss: 3.6857\n",
      "Epoch 11 | Batch 54/94 | Batch Loss: 3.5098\n",
      "Epoch 11 | Batch 55/94 | Batch Loss: 3.5683\n",
      "Epoch 11 | Batch 56/94 | Batch Loss: 3.4434\n",
      "Epoch 11 | Batch 57/94 | Batch Loss: 3.6306\n",
      "Epoch 11 | Batch 58/94 | Batch Loss: 3.5257\n",
      "Epoch 11 | Batch 59/94 | Batch Loss: 3.4319\n",
      "Epoch 11 | Batch 60/94 | Batch Loss: 3.5775\n",
      "Epoch 11 | Batch 61/94 | Batch Loss: 3.5182\n",
      "Epoch 11 | Batch 62/94 | Batch Loss: 3.3961\n",
      "Epoch 11 | Batch 63/94 | Batch Loss: 3.4635\n",
      "Epoch 11 | Batch 64/94 | Batch Loss: 3.4404\n",
      "Epoch 11 | Batch 65/94 | Batch Loss: 3.5031\n",
      "Epoch 11 | Batch 66/94 | Batch Loss: 3.6057\n",
      "Epoch 11 | Batch 67/94 | Batch Loss: 3.4736\n",
      "Epoch 11 | Batch 68/94 | Batch Loss: 3.5974\n",
      "Epoch 11 | Batch 69/94 | Batch Loss: 3.5259\n",
      "Epoch 11 | Batch 70/94 | Batch Loss: 3.5332\n",
      "Epoch 11 | Batch 71/94 | Batch Loss: 3.5126\n",
      "Epoch 11 | Batch 72/94 | Batch Loss: 3.4532\n",
      "Epoch 11 | Batch 73/94 | Batch Loss: 3.5919\n",
      "Epoch 11 | Batch 74/94 | Batch Loss: 3.4550\n",
      "Epoch 11 | Batch 75/94 | Batch Loss: 3.5872\n",
      "Epoch 11 | Batch 76/94 | Batch Loss: 3.4740\n",
      "Epoch 11 | Batch 77/94 | Batch Loss: 3.5253\n",
      "Epoch 11 | Batch 78/94 | Batch Loss: 3.6209\n",
      "Epoch 11 | Batch 79/94 | Batch Loss: 3.5659\n",
      "Epoch 11 | Batch 80/94 | Batch Loss: 3.6793\n",
      "Epoch 11 | Batch 81/94 | Batch Loss: 3.6403\n",
      "Epoch 11 | Batch 82/94 | Batch Loss: 3.6268\n",
      "Epoch 11 | Batch 83/94 | Batch Loss: 3.5605\n",
      "Epoch 11 | Batch 84/94 | Batch Loss: 3.6064\n",
      "Epoch 11 | Batch 85/94 | Batch Loss: 3.5032\n",
      "Epoch 11 | Batch 86/94 | Batch Loss: 3.5100\n",
      "Epoch 11 | Batch 87/94 | Batch Loss: 3.5479\n",
      "Epoch 11 | Batch 88/94 | Batch Loss: 3.4821\n",
      "Epoch 11 | Batch 89/94 | Batch Loss: 3.5951\n",
      "Epoch 11 | Batch 90/94 | Batch Loss: 3.5789\n",
      "Epoch 11 | Batch 91/94 | Batch Loss: 3.5578\n",
      "Epoch 11 | Batch 92/94 | Batch Loss: 3.5182\n",
      "Epoch 11 | Batch 93/94 | Batch Loss: 3.6094\n",
      "Epoch 11 | Batch 94/94 | Batch Loss: 3.4851\n",
      "Epoch 11 Finished | Average Loss: 3.5606\n",
      "\n",
      "Epoch 12 Starting...\n",
      "Epoch 12 | Batch 1/94 | Batch Loss: 3.5408\n",
      "Epoch 12 | Batch 2/94 | Batch Loss: 3.4908\n",
      "Epoch 12 | Batch 3/94 | Batch Loss: 3.6270\n",
      "Epoch 12 | Batch 4/94 | Batch Loss: 3.5147\n",
      "Epoch 12 | Batch 5/94 | Batch Loss: 3.5897\n",
      "Epoch 12 | Batch 6/94 | Batch Loss: 3.5588\n",
      "Epoch 12 | Batch 7/94 | Batch Loss: 3.4882\n",
      "Epoch 12 | Batch 8/94 | Batch Loss: 3.4711\n",
      "Epoch 12 | Batch 9/94 | Batch Loss: 3.4801\n",
      "Epoch 12 | Batch 10/94 | Batch Loss: 3.5538\n",
      "Epoch 12 | Batch 11/94 | Batch Loss: 3.5197\n",
      "Epoch 12 | Batch 12/94 | Batch Loss: 3.5175\n",
      "Epoch 12 | Batch 13/94 | Batch Loss: 3.4771\n",
      "Epoch 12 | Batch 14/94 | Batch Loss: 3.5403\n",
      "Epoch 12 | Batch 15/94 | Batch Loss: 3.4374\n",
      "Epoch 12 | Batch 16/94 | Batch Loss: 3.5060\n",
      "Epoch 12 | Batch 17/94 | Batch Loss: 3.5273\n",
      "Epoch 12 | Batch 18/94 | Batch Loss: 3.5246\n",
      "Epoch 12 | Batch 19/94 | Batch Loss: 3.5009\n",
      "Epoch 12 | Batch 20/94 | Batch Loss: 3.6667\n",
      "Epoch 12 | Batch 21/94 | Batch Loss: 3.4357\n",
      "Epoch 12 | Batch 22/94 | Batch Loss: 3.5189\n",
      "Epoch 12 | Batch 23/94 | Batch Loss: 3.5486\n",
      "Epoch 12 | Batch 24/94 | Batch Loss: 3.5721\n",
      "Epoch 12 | Batch 25/94 | Batch Loss: 3.6122\n",
      "Epoch 12 | Batch 26/94 | Batch Loss: 3.3958\n",
      "Epoch 12 | Batch 27/94 | Batch Loss: 3.5456\n",
      "Epoch 12 | Batch 28/94 | Batch Loss: 3.5035\n",
      "Epoch 12 | Batch 29/94 | Batch Loss: 3.5434\n",
      "Epoch 12 | Batch 30/94 | Batch Loss: 3.4399\n",
      "Epoch 12 | Batch 31/94 | Batch Loss: 3.4489\n",
      "Epoch 12 | Batch 32/94 | Batch Loss: 3.4217\n",
      "Epoch 12 | Batch 33/94 | Batch Loss: 3.5378\n",
      "Epoch 12 | Batch 34/94 | Batch Loss: 3.4745\n",
      "Epoch 12 | Batch 35/94 | Batch Loss: 3.5001\n",
      "Epoch 12 | Batch 36/94 | Batch Loss: 3.3868\n",
      "Epoch 12 | Batch 37/94 | Batch Loss: 3.4203\n",
      "Epoch 12 | Batch 38/94 | Batch Loss: 3.5549\n",
      "Epoch 12 | Batch 39/94 | Batch Loss: 3.5722\n",
      "Epoch 12 | Batch 40/94 | Batch Loss: 3.4583\n",
      "Epoch 12 | Batch 41/94 | Batch Loss: 3.4727\n",
      "Epoch 12 | Batch 42/94 | Batch Loss: 3.4517\n",
      "Epoch 12 | Batch 43/94 | Batch Loss: 3.4775\n",
      "Epoch 12 | Batch 44/94 | Batch Loss: 3.5055\n",
      "Epoch 12 | Batch 45/94 | Batch Loss: 3.4918\n",
      "Epoch 12 | Batch 46/94 | Batch Loss: 3.4686\n",
      "Epoch 12 | Batch 47/94 | Batch Loss: 3.4602\n",
      "Epoch 12 | Batch 48/94 | Batch Loss: 3.4926\n",
      "Epoch 12 | Batch 49/94 | Batch Loss: 3.5474\n",
      "Epoch 12 | Batch 50/94 | Batch Loss: 3.5692\n",
      "Epoch 12 | Batch 51/94 | Batch Loss: 3.4482\n",
      "Epoch 12 | Batch 52/94 | Batch Loss: 3.3951\n",
      "Epoch 12 | Batch 53/94 | Batch Loss: 3.4626\n",
      "Epoch 12 | Batch 54/94 | Batch Loss: 3.4629\n",
      "Epoch 12 | Batch 55/94 | Batch Loss: 3.4268\n",
      "Epoch 12 | Batch 56/94 | Batch Loss: 3.5099\n",
      "Epoch 12 | Batch 57/94 | Batch Loss: 3.3757\n",
      "Epoch 12 | Batch 58/94 | Batch Loss: 3.3363\n",
      "Epoch 12 | Batch 59/94 | Batch Loss: 3.5883\n",
      "Epoch 12 | Batch 60/94 | Batch Loss: 3.4590\n",
      "Epoch 12 | Batch 61/94 | Batch Loss: 3.5155\n",
      "Epoch 12 | Batch 62/94 | Batch Loss: 3.5232\n",
      "Epoch 12 | Batch 63/94 | Batch Loss: 3.4844\n",
      "Epoch 12 | Batch 64/94 | Batch Loss: 3.4274\n",
      "Epoch 12 | Batch 65/94 | Batch Loss: 3.3951\n",
      "Epoch 12 | Batch 66/94 | Batch Loss: 3.5162\n",
      "Epoch 12 | Batch 67/94 | Batch Loss: 3.6064\n",
      "Epoch 12 | Batch 68/94 | Batch Loss: 3.4233\n",
      "Epoch 12 | Batch 69/94 | Batch Loss: 3.4885\n",
      "Epoch 12 | Batch 70/94 | Batch Loss: 3.4749\n",
      "Epoch 12 | Batch 71/94 | Batch Loss: 3.3807\n",
      "Epoch 12 | Batch 72/94 | Batch Loss: 3.5671\n",
      "Epoch 12 | Batch 73/94 | Batch Loss: 3.5980\n",
      "Epoch 12 | Batch 74/94 | Batch Loss: 3.4609\n",
      "Epoch 12 | Batch 75/94 | Batch Loss: 3.4182\n",
      "Epoch 12 | Batch 76/94 | Batch Loss: 3.5214\n",
      "Epoch 12 | Batch 77/94 | Batch Loss: 3.5173\n",
      "Epoch 12 | Batch 78/94 | Batch Loss: 3.6177\n",
      "Epoch 12 | Batch 79/94 | Batch Loss: 3.4762\n",
      "Epoch 12 | Batch 80/94 | Batch Loss: 3.4997\n",
      "Epoch 12 | Batch 81/94 | Batch Loss: 3.6234\n",
      "Epoch 12 | Batch 82/94 | Batch Loss: 3.4748\n",
      "Epoch 12 | Batch 83/94 | Batch Loss: 3.4286\n",
      "Epoch 12 | Batch 84/94 | Batch Loss: 3.4012\n",
      "Epoch 12 | Batch 85/94 | Batch Loss: 3.6024\n",
      "Epoch 12 | Batch 86/94 | Batch Loss: 3.4416\n",
      "Epoch 12 | Batch 87/94 | Batch Loss: 3.5256\n",
      "Epoch 12 | Batch 88/94 | Batch Loss: 3.4788\n",
      "Epoch 12 | Batch 89/94 | Batch Loss: 3.5233\n",
      "Epoch 12 | Batch 90/94 | Batch Loss: 3.5554\n",
      "Epoch 12 | Batch 91/94 | Batch Loss: 3.5877\n",
      "Epoch 12 | Batch 92/94 | Batch Loss: 3.3452\n",
      "Epoch 12 | Batch 93/94 | Batch Loss: 3.4875\n",
      "Epoch 12 | Batch 94/94 | Batch Loss: 3.5534\n",
      "Epoch 12 Finished | Average Loss: 3.4975\n",
      "\n",
      "Epoch 13 Starting...\n",
      "Epoch 13 | Batch 1/94 | Batch Loss: 3.4687\n",
      "Epoch 13 | Batch 2/94 | Batch Loss: 3.4075\n",
      "Epoch 13 | Batch 3/94 | Batch Loss: 3.5023\n",
      "Epoch 13 | Batch 4/94 | Batch Loss: 3.3441\n",
      "Epoch 13 | Batch 5/94 | Batch Loss: 3.4913\n",
      "Epoch 13 | Batch 6/94 | Batch Loss: 3.5184\n",
      "Epoch 13 | Batch 7/94 | Batch Loss: 3.4291\n",
      "Epoch 13 | Batch 8/94 | Batch Loss: 3.4657\n",
      "Epoch 13 | Batch 9/94 | Batch Loss: 3.4790\n",
      "Epoch 13 | Batch 10/94 | Batch Loss: 3.5116\n",
      "Epoch 13 | Batch 11/94 | Batch Loss: 3.3781\n",
      "Epoch 13 | Batch 12/94 | Batch Loss: 3.4124\n",
      "Epoch 13 | Batch 13/94 | Batch Loss: 3.4873\n",
      "Epoch 13 | Batch 14/94 | Batch Loss: 3.4850\n",
      "Epoch 13 | Batch 15/94 | Batch Loss: 3.4579\n",
      "Epoch 13 | Batch 16/94 | Batch Loss: 3.4241\n",
      "Epoch 13 | Batch 17/94 | Batch Loss: 3.4529\n",
      "Epoch 13 | Batch 18/94 | Batch Loss: 3.4496\n",
      "Epoch 13 | Batch 19/94 | Batch Loss: 3.4074\n",
      "Epoch 13 | Batch 20/94 | Batch Loss: 3.3592\n",
      "Epoch 13 | Batch 21/94 | Batch Loss: 3.4075\n",
      "Epoch 13 | Batch 22/94 | Batch Loss: 3.3559\n",
      "Epoch 13 | Batch 23/94 | Batch Loss: 3.3675\n",
      "Epoch 13 | Batch 24/94 | Batch Loss: 3.4497\n",
      "Epoch 13 | Batch 25/94 | Batch Loss: 3.3993\n",
      "Epoch 13 | Batch 26/94 | Batch Loss: 3.3435\n",
      "Epoch 13 | Batch 27/94 | Batch Loss: 3.4964\n",
      "Epoch 13 | Batch 28/94 | Batch Loss: 3.5149\n",
      "Epoch 13 | Batch 29/94 | Batch Loss: 3.4914\n",
      "Epoch 13 | Batch 30/94 | Batch Loss: 3.4529\n",
      "Epoch 13 | Batch 31/94 | Batch Loss: 3.4144\n",
      "Epoch 13 | Batch 32/94 | Batch Loss: 3.5202\n",
      "Epoch 13 | Batch 33/94 | Batch Loss: 3.4341\n",
      "Epoch 13 | Batch 34/94 | Batch Loss: 3.4205\n",
      "Epoch 13 | Batch 35/94 | Batch Loss: 3.3434\n",
      "Epoch 13 | Batch 36/94 | Batch Loss: 3.3771\n",
      "Epoch 13 | Batch 37/94 | Batch Loss: 3.4011\n",
      "Epoch 13 | Batch 38/94 | Batch Loss: 3.3732\n",
      "Epoch 13 | Batch 39/94 | Batch Loss: 3.3714\n",
      "Epoch 13 | Batch 40/94 | Batch Loss: 3.5669\n",
      "Epoch 13 | Batch 41/94 | Batch Loss: 3.4269\n",
      "Epoch 13 | Batch 42/94 | Batch Loss: 3.4483\n",
      "Epoch 13 | Batch 43/94 | Batch Loss: 3.5412\n",
      "Epoch 13 | Batch 44/94 | Batch Loss: 3.3394\n",
      "Epoch 13 | Batch 45/94 | Batch Loss: 3.4728\n",
      "Epoch 13 | Batch 46/94 | Batch Loss: 3.4598\n",
      "Epoch 13 | Batch 47/94 | Batch Loss: 3.5477\n",
      "Epoch 13 | Batch 48/94 | Batch Loss: 3.4604\n",
      "Epoch 13 | Batch 49/94 | Batch Loss: 3.4946\n",
      "Epoch 13 | Batch 50/94 | Batch Loss: 3.5442\n",
      "Epoch 13 | Batch 51/94 | Batch Loss: 3.2939\n",
      "Epoch 13 | Batch 52/94 | Batch Loss: 3.4850\n",
      "Epoch 13 | Batch 53/94 | Batch Loss: 3.4726\n",
      "Epoch 13 | Batch 54/94 | Batch Loss: 3.5185\n",
      "Epoch 13 | Batch 55/94 | Batch Loss: 3.3942\n",
      "Epoch 13 | Batch 56/94 | Batch Loss: 3.3407\n",
      "Epoch 13 | Batch 57/94 | Batch Loss: 3.2753\n",
      "Epoch 13 | Batch 58/94 | Batch Loss: 3.5185\n",
      "Epoch 13 | Batch 59/94 | Batch Loss: 3.4368\n",
      "Epoch 13 | Batch 60/94 | Batch Loss: 3.4726\n",
      "Epoch 13 | Batch 61/94 | Batch Loss: 3.4239\n",
      "Epoch 13 | Batch 62/94 | Batch Loss: 3.2841\n",
      "Epoch 13 | Batch 63/94 | Batch Loss: 3.4712\n",
      "Epoch 13 | Batch 64/94 | Batch Loss: 3.4686\n",
      "Epoch 13 | Batch 65/94 | Batch Loss: 3.4285\n",
      "Epoch 13 | Batch 66/94 | Batch Loss: 3.4808\n",
      "Epoch 13 | Batch 67/94 | Batch Loss: 3.5109\n",
      "Epoch 13 | Batch 68/94 | Batch Loss: 3.5337\n",
      "Epoch 13 | Batch 69/94 | Batch Loss: 3.3738\n",
      "Epoch 13 | Batch 70/94 | Batch Loss: 3.4125\n",
      "Epoch 13 | Batch 71/94 | Batch Loss: 3.4121\n",
      "Epoch 13 | Batch 72/94 | Batch Loss: 3.5131\n",
      "Epoch 13 | Batch 73/94 | Batch Loss: 3.4371\n",
      "Epoch 13 | Batch 74/94 | Batch Loss: 3.4401\n",
      "Epoch 13 | Batch 75/94 | Batch Loss: 3.2905\n",
      "Epoch 13 | Batch 76/94 | Batch Loss: 3.3843\n",
      "Epoch 13 | Batch 77/94 | Batch Loss: 3.4612\n",
      "Epoch 13 | Batch 78/94 | Batch Loss: 3.3832\n",
      "Epoch 13 | Batch 79/94 | Batch Loss: 3.5396\n",
      "Epoch 13 | Batch 80/94 | Batch Loss: 3.4301\n",
      "Epoch 13 | Batch 81/94 | Batch Loss: 3.5744\n",
      "Epoch 13 | Batch 82/94 | Batch Loss: 3.4974\n",
      "Epoch 13 | Batch 83/94 | Batch Loss: 3.4909\n",
      "Epoch 13 | Batch 84/94 | Batch Loss: 3.4440\n",
      "Epoch 13 | Batch 85/94 | Batch Loss: 3.3998\n",
      "Epoch 13 | Batch 86/94 | Batch Loss: 3.4849\n",
      "Epoch 13 | Batch 87/94 | Batch Loss: 3.4598\n",
      "Epoch 13 | Batch 88/94 | Batch Loss: 3.4145\n",
      "Epoch 13 | Batch 89/94 | Batch Loss: 3.3174\n",
      "Epoch 13 | Batch 90/94 | Batch Loss: 3.3795\n",
      "Epoch 13 | Batch 91/94 | Batch Loss: 3.4374\n",
      "Epoch 13 | Batch 92/94 | Batch Loss: 3.4283\n",
      "Epoch 13 | Batch 93/94 | Batch Loss: 3.3527\n",
      "Epoch 13 | Batch 94/94 | Batch Loss: 3.6334\n",
      "Epoch 13 Finished | Average Loss: 3.4401\n",
      "\n",
      "Epoch 14 Starting...\n",
      "Epoch 14 | Batch 1/94 | Batch Loss: 3.4620\n",
      "Epoch 14 | Batch 2/94 | Batch Loss: 3.4145\n",
      "Epoch 14 | Batch 3/94 | Batch Loss: 3.4286\n",
      "Epoch 14 | Batch 4/94 | Batch Loss: 3.4657\n",
      "Epoch 14 | Batch 5/94 | Batch Loss: 3.4399\n",
      "Epoch 14 | Batch 6/94 | Batch Loss: 3.3760\n",
      "Epoch 14 | Batch 7/94 | Batch Loss: 3.3758\n",
      "Epoch 14 | Batch 8/94 | Batch Loss: 3.3621\n",
      "Epoch 14 | Batch 9/94 | Batch Loss: 3.4809\n",
      "Epoch 14 | Batch 10/94 | Batch Loss: 3.4287\n",
      "Epoch 14 | Batch 11/94 | Batch Loss: 3.5133\n",
      "Epoch 14 | Batch 12/94 | Batch Loss: 3.3318\n",
      "Epoch 14 | Batch 13/94 | Batch Loss: 3.3166\n",
      "Epoch 14 | Batch 14/94 | Batch Loss: 3.4708\n",
      "Epoch 14 | Batch 15/94 | Batch Loss: 3.4871\n",
      "Epoch 14 | Batch 16/94 | Batch Loss: 3.3906\n",
      "Epoch 14 | Batch 17/94 | Batch Loss: 3.4054\n",
      "Epoch 14 | Batch 18/94 | Batch Loss: 3.4376\n",
      "Epoch 14 | Batch 19/94 | Batch Loss: 3.3696\n",
      "Epoch 14 | Batch 20/94 | Batch Loss: 3.3920\n",
      "Epoch 14 | Batch 21/94 | Batch Loss: 3.3546\n",
      "Epoch 14 | Batch 22/94 | Batch Loss: 3.3359\n",
      "Epoch 14 | Batch 23/94 | Batch Loss: 3.3652\n",
      "Epoch 14 | Batch 24/94 | Batch Loss: 3.3705\n",
      "Epoch 14 | Batch 25/94 | Batch Loss: 3.3971\n",
      "Epoch 14 | Batch 26/94 | Batch Loss: 3.3996\n",
      "Epoch 14 | Batch 27/94 | Batch Loss: 3.4946\n",
      "Epoch 14 | Batch 28/94 | Batch Loss: 3.3698\n",
      "Epoch 14 | Batch 29/94 | Batch Loss: 3.3695\n",
      "Epoch 14 | Batch 30/94 | Batch Loss: 3.3486\n",
      "Epoch 14 | Batch 31/94 | Batch Loss: 3.4881\n",
      "Epoch 14 | Batch 32/94 | Batch Loss: 3.4554\n",
      "Epoch 14 | Batch 33/94 | Batch Loss: 3.4471\n",
      "Epoch 14 | Batch 34/94 | Batch Loss: 3.3990\n",
      "Epoch 14 | Batch 35/94 | Batch Loss: 3.1643\n",
      "Epoch 14 | Batch 36/94 | Batch Loss: 3.4307\n",
      "Epoch 14 | Batch 37/94 | Batch Loss: 3.3870\n",
      "Epoch 14 | Batch 38/94 | Batch Loss: 3.5068\n",
      "Epoch 14 | Batch 39/94 | Batch Loss: 3.4360\n",
      "Epoch 14 | Batch 40/94 | Batch Loss: 3.3340\n",
      "Epoch 14 | Batch 41/94 | Batch Loss: 3.4152\n",
      "Epoch 14 | Batch 42/94 | Batch Loss: 3.3389\n",
      "Epoch 14 | Batch 43/94 | Batch Loss: 3.3371\n",
      "Epoch 14 | Batch 44/94 | Batch Loss: 3.4367\n",
      "Epoch 14 | Batch 45/94 | Batch Loss: 3.4559\n",
      "Epoch 14 | Batch 46/94 | Batch Loss: 3.4147\n",
      "Epoch 14 | Batch 47/94 | Batch Loss: 3.4242\n",
      "Epoch 14 | Batch 48/94 | Batch Loss: 3.3276\n",
      "Epoch 14 | Batch 49/94 | Batch Loss: 3.4368\n",
      "Epoch 14 | Batch 50/94 | Batch Loss: 3.2826\n",
      "Epoch 14 | Batch 51/94 | Batch Loss: 3.3528\n",
      "Epoch 14 | Batch 52/94 | Batch Loss: 3.3325\n",
      "Epoch 14 | Batch 53/94 | Batch Loss: 3.3995\n",
      "Epoch 14 | Batch 54/94 | Batch Loss: 3.3667\n",
      "Epoch 14 | Batch 55/94 | Batch Loss: 3.3047\n",
      "Epoch 14 | Batch 56/94 | Batch Loss: 3.4443\n",
      "Epoch 14 | Batch 57/94 | Batch Loss: 3.3586\n",
      "Epoch 14 | Batch 58/94 | Batch Loss: 3.2137\n",
      "Epoch 14 | Batch 59/94 | Batch Loss: 3.3640\n",
      "Epoch 14 | Batch 60/94 | Batch Loss: 3.3932\n",
      "Epoch 14 | Batch 61/94 | Batch Loss: 3.3750\n",
      "Epoch 14 | Batch 62/94 | Batch Loss: 3.4905\n",
      "Epoch 14 | Batch 63/94 | Batch Loss: 3.4308\n",
      "Epoch 14 | Batch 64/94 | Batch Loss: 3.3397\n",
      "Epoch 14 | Batch 65/94 | Batch Loss: 3.4774\n",
      "Epoch 14 | Batch 66/94 | Batch Loss: 3.3449\n",
      "Epoch 14 | Batch 67/94 | Batch Loss: 3.4020\n",
      "Epoch 14 | Batch 68/94 | Batch Loss: 3.3240\n",
      "Epoch 14 | Batch 69/94 | Batch Loss: 3.2798\n",
      "Epoch 14 | Batch 70/94 | Batch Loss: 3.3087\n",
      "Epoch 14 | Batch 71/94 | Batch Loss: 3.4498\n",
      "Epoch 14 | Batch 72/94 | Batch Loss: 3.3437\n",
      "Epoch 14 | Batch 73/94 | Batch Loss: 3.4216\n",
      "Epoch 14 | Batch 74/94 | Batch Loss: 3.2337\n",
      "Epoch 14 | Batch 75/94 | Batch Loss: 3.4772\n",
      "Epoch 14 | Batch 76/94 | Batch Loss: 3.3836\n",
      "Epoch 14 | Batch 77/94 | Batch Loss: 3.2519\n",
      "Epoch 14 | Batch 78/94 | Batch Loss: 3.3006\n",
      "Epoch 14 | Batch 79/94 | Batch Loss: 3.3944\n",
      "Epoch 14 | Batch 80/94 | Batch Loss: 3.2896\n",
      "Epoch 14 | Batch 81/94 | Batch Loss: 3.4268\n",
      "Epoch 14 | Batch 82/94 | Batch Loss: 3.3066\n",
      "Epoch 14 | Batch 83/94 | Batch Loss: 3.3758\n",
      "Epoch 14 | Batch 84/94 | Batch Loss: 3.4045\n",
      "Epoch 14 | Batch 85/94 | Batch Loss: 3.3619\n",
      "Epoch 14 | Batch 86/94 | Batch Loss: 3.4694\n",
      "Epoch 14 | Batch 87/94 | Batch Loss: 3.3335\n",
      "Epoch 14 | Batch 88/94 | Batch Loss: 3.3606\n",
      "Epoch 14 | Batch 89/94 | Batch Loss: 3.2686\n",
      "Epoch 14 | Batch 90/94 | Batch Loss: 3.3898\n",
      "Epoch 14 | Batch 91/94 | Batch Loss: 3.3837\n",
      "Epoch 14 | Batch 92/94 | Batch Loss: 3.3273\n",
      "Epoch 14 | Batch 93/94 | Batch Loss: 3.4100\n",
      "Epoch 14 | Batch 94/94 | Batch Loss: 3.3751\n",
      "Epoch 14 Finished | Average Loss: 3.3842\n",
      "\n",
      "Epoch 15 Starting...\n",
      "Epoch 15 | Batch 1/94 | Batch Loss: 3.3913\n",
      "Epoch 15 | Batch 2/94 | Batch Loss: 3.4188\n",
      "Epoch 15 | Batch 3/94 | Batch Loss: 3.3629\n",
      "Epoch 15 | Batch 4/94 | Batch Loss: 3.3034\n",
      "Epoch 15 | Batch 5/94 | Batch Loss: 3.2934\n",
      "Epoch 15 | Batch 6/94 | Batch Loss: 3.2778\n",
      "Epoch 15 | Batch 7/94 | Batch Loss: 3.4162\n",
      "Epoch 15 | Batch 8/94 | Batch Loss: 3.3171\n",
      "Epoch 15 | Batch 9/94 | Batch Loss: 3.3277\n",
      "Epoch 15 | Batch 10/94 | Batch Loss: 3.2444\n",
      "Epoch 15 | Batch 11/94 | Batch Loss: 3.4718\n",
      "Epoch 15 | Batch 12/94 | Batch Loss: 3.3845\n",
      "Epoch 15 | Batch 13/94 | Batch Loss: 3.3866\n",
      "Epoch 15 | Batch 14/94 | Batch Loss: 3.2992\n",
      "Epoch 15 | Batch 15/94 | Batch Loss: 3.3996\n",
      "Epoch 15 | Batch 16/94 | Batch Loss: 3.3576\n",
      "Epoch 15 | Batch 17/94 | Batch Loss: 3.3099\n",
      "Epoch 15 | Batch 18/94 | Batch Loss: 3.3817\n",
      "Epoch 15 | Batch 19/94 | Batch Loss: 3.4400\n",
      "Epoch 15 | Batch 20/94 | Batch Loss: 3.3257\n",
      "Epoch 15 | Batch 21/94 | Batch Loss: 3.2319\n",
      "Epoch 15 | Batch 22/94 | Batch Loss: 3.3923\n",
      "Epoch 15 | Batch 23/94 | Batch Loss: 3.4113\n",
      "Epoch 15 | Batch 24/94 | Batch Loss: 3.2697\n",
      "Epoch 15 | Batch 25/94 | Batch Loss: 3.3433\n",
      "Epoch 15 | Batch 26/94 | Batch Loss: 3.4217\n",
      "Epoch 15 | Batch 27/94 | Batch Loss: 3.2339\n",
      "Epoch 15 | Batch 28/94 | Batch Loss: 3.4188\n",
      "Epoch 15 | Batch 29/94 | Batch Loss: 3.3335\n",
      "Epoch 15 | Batch 30/94 | Batch Loss: 3.3085\n",
      "Epoch 15 | Batch 31/94 | Batch Loss: 3.3003\n",
      "Epoch 15 | Batch 32/94 | Batch Loss: 3.2918\n",
      "Epoch 15 | Batch 33/94 | Batch Loss: 3.2519\n",
      "Epoch 15 | Batch 34/94 | Batch Loss: 3.4799\n",
      "Epoch 15 | Batch 35/94 | Batch Loss: 3.2411\n",
      "Epoch 15 | Batch 36/94 | Batch Loss: 3.3249\n",
      "Epoch 15 | Batch 37/94 | Batch Loss: 3.2703\n",
      "Epoch 15 | Batch 38/94 | Batch Loss: 3.4361\n",
      "Epoch 15 | Batch 39/94 | Batch Loss: 3.3039\n",
      "Epoch 15 | Batch 40/94 | Batch Loss: 3.3248\n",
      "Epoch 15 | Batch 41/94 | Batch Loss: 3.3421\n",
      "Epoch 15 | Batch 42/94 | Batch Loss: 3.3574\n",
      "Epoch 15 | Batch 43/94 | Batch Loss: 3.4009\n",
      "Epoch 15 | Batch 44/94 | Batch Loss: 3.2903\n",
      "Epoch 15 | Batch 45/94 | Batch Loss: 3.3286\n",
      "Epoch 15 | Batch 46/94 | Batch Loss: 3.3045\n",
      "Epoch 15 | Batch 47/94 | Batch Loss: 3.2195\n",
      "Epoch 15 | Batch 48/94 | Batch Loss: 3.2796\n",
      "Epoch 15 | Batch 49/94 | Batch Loss: 3.3263\n",
      "Epoch 15 | Batch 50/94 | Batch Loss: 3.2705\n",
      "Epoch 15 | Batch 51/94 | Batch Loss: 3.2906\n",
      "Epoch 15 | Batch 52/94 | Batch Loss: 3.2441\n",
      "Epoch 15 | Batch 53/94 | Batch Loss: 3.3683\n",
      "Epoch 15 | Batch 54/94 | Batch Loss: 3.3910\n",
      "Epoch 15 | Batch 55/94 | Batch Loss: 3.3361\n",
      "Epoch 15 | Batch 56/94 | Batch Loss: 3.3070\n",
      "Epoch 15 | Batch 57/94 | Batch Loss: 3.3222\n",
      "Epoch 15 | Batch 58/94 | Batch Loss: 3.3405\n",
      "Epoch 15 | Batch 59/94 | Batch Loss: 3.4579\n",
      "Epoch 15 | Batch 60/94 | Batch Loss: 3.2684\n",
      "Epoch 15 | Batch 61/94 | Batch Loss: 3.4218\n",
      "Epoch 15 | Batch 62/94 | Batch Loss: 3.3381\n",
      "Epoch 15 | Batch 63/94 | Batch Loss: 3.3617\n",
      "Epoch 15 | Batch 64/94 | Batch Loss: 3.3447\n",
      "Epoch 15 | Batch 65/94 | Batch Loss: 3.3168\n",
      "Epoch 15 | Batch 66/94 | Batch Loss: 3.3537\n",
      "Epoch 15 | Batch 67/94 | Batch Loss: 3.2029\n",
      "Epoch 15 | Batch 68/94 | Batch Loss: 3.2600\n",
      "Epoch 15 | Batch 69/94 | Batch Loss: 3.3324\n",
      "Epoch 15 | Batch 70/94 | Batch Loss: 3.2970\n",
      "Epoch 15 | Batch 71/94 | Batch Loss: 3.2169\n",
      "Epoch 15 | Batch 72/94 | Batch Loss: 3.3384\n",
      "Epoch 15 | Batch 73/94 | Batch Loss: 3.3263\n",
      "Epoch 15 | Batch 74/94 | Batch Loss: 3.4049\n",
      "Epoch 15 | Batch 75/94 | Batch Loss: 3.3294\n",
      "Epoch 15 | Batch 76/94 | Batch Loss: 3.1969\n",
      "Epoch 15 | Batch 77/94 | Batch Loss: 3.4214\n",
      "Epoch 15 | Batch 78/94 | Batch Loss: 3.4078\n",
      "Epoch 15 | Batch 79/94 | Batch Loss: 3.3138\n",
      "Epoch 15 | Batch 80/94 | Batch Loss: 3.3060\n",
      "Epoch 15 | Batch 81/94 | Batch Loss: 3.2946\n",
      "Epoch 15 | Batch 82/94 | Batch Loss: 3.2710\n",
      "Epoch 15 | Batch 83/94 | Batch Loss: 3.3536\n",
      "Epoch 15 | Batch 84/94 | Batch Loss: 3.4053\n",
      "Epoch 15 | Batch 85/94 | Batch Loss: 3.3624\n",
      "Epoch 15 | Batch 86/94 | Batch Loss: 3.3892\n",
      "Epoch 15 | Batch 87/94 | Batch Loss: 3.3977\n",
      "Epoch 15 | Batch 88/94 | Batch Loss: 3.2787\n",
      "Epoch 15 | Batch 89/94 | Batch Loss: 3.2896\n",
      "Epoch 15 | Batch 90/94 | Batch Loss: 3.2064\n",
      "Epoch 15 | Batch 91/94 | Batch Loss: 3.2779\n",
      "Epoch 15 | Batch 92/94 | Batch Loss: 3.3858\n",
      "Epoch 15 | Batch 93/94 | Batch Loss: 3.3417\n",
      "Epoch 15 | Batch 94/94 | Batch Loss: 3.3415\n",
      "Epoch 15 Finished | Average Loss: 3.3323\n",
      "\n",
      "Epoch 16 Starting...\n",
      "Epoch 16 | Batch 1/94 | Batch Loss: 3.2888\n",
      "Epoch 16 | Batch 2/94 | Batch Loss: 3.3181\n",
      "Epoch 16 | Batch 3/94 | Batch Loss: 3.2518\n",
      "Epoch 16 | Batch 4/94 | Batch Loss: 3.3401\n",
      "Epoch 16 | Batch 5/94 | Batch Loss: 3.3147\n",
      "Epoch 16 | Batch 6/94 | Batch Loss: 3.2505\n",
      "Epoch 16 | Batch 7/94 | Batch Loss: 3.1697\n",
      "Epoch 16 | Batch 8/94 | Batch Loss: 3.2738\n",
      "Epoch 16 | Batch 9/94 | Batch Loss: 3.2654\n",
      "Epoch 16 | Batch 10/94 | Batch Loss: 3.3846\n",
      "Epoch 16 | Batch 11/94 | Batch Loss: 3.3169\n",
      "Epoch 16 | Batch 12/94 | Batch Loss: 3.2190\n",
      "Epoch 16 | Batch 13/94 | Batch Loss: 3.2962\n",
      "Epoch 16 | Batch 14/94 | Batch Loss: 3.2030\n",
      "Epoch 16 | Batch 15/94 | Batch Loss: 3.1816\n",
      "Epoch 16 | Batch 16/94 | Batch Loss: 3.2255\n",
      "Epoch 16 | Batch 17/94 | Batch Loss: 3.2918\n",
      "Epoch 16 | Batch 18/94 | Batch Loss: 3.2827\n",
      "Epoch 16 | Batch 19/94 | Batch Loss: 3.3490\n",
      "Epoch 16 | Batch 20/94 | Batch Loss: 3.3132\n",
      "Epoch 16 | Batch 21/94 | Batch Loss: 3.3080\n",
      "Epoch 16 | Batch 22/94 | Batch Loss: 3.3219\n",
      "Epoch 16 | Batch 23/94 | Batch Loss: 3.4134\n",
      "Epoch 16 | Batch 24/94 | Batch Loss: 3.2614\n",
      "Epoch 16 | Batch 25/94 | Batch Loss: 3.2598\n",
      "Epoch 16 | Batch 26/94 | Batch Loss: 3.3178\n",
      "Epoch 16 | Batch 27/94 | Batch Loss: 3.4151\n",
      "Epoch 16 | Batch 28/94 | Batch Loss: 3.3144\n",
      "Epoch 16 | Batch 29/94 | Batch Loss: 3.3266\n",
      "Epoch 16 | Batch 30/94 | Batch Loss: 3.3249\n",
      "Epoch 16 | Batch 31/94 | Batch Loss: 3.2768\n",
      "Epoch 16 | Batch 32/94 | Batch Loss: 3.2739\n",
      "Epoch 16 | Batch 33/94 | Batch Loss: 3.2866\n",
      "Epoch 16 | Batch 34/94 | Batch Loss: 3.3532\n",
      "Epoch 16 | Batch 35/94 | Batch Loss: 3.3262\n",
      "Epoch 16 | Batch 36/94 | Batch Loss: 3.2348\n",
      "Epoch 16 | Batch 37/94 | Batch Loss: 3.3156\n",
      "Epoch 16 | Batch 38/94 | Batch Loss: 3.1893\n",
      "Epoch 16 | Batch 39/94 | Batch Loss: 3.2721\n",
      "Epoch 16 | Batch 40/94 | Batch Loss: 3.2434\n",
      "Epoch 16 | Batch 41/94 | Batch Loss: 3.2476\n",
      "Epoch 16 | Batch 42/94 | Batch Loss: 3.2788\n",
      "Epoch 16 | Batch 43/94 | Batch Loss: 3.2862\n",
      "Epoch 16 | Batch 44/94 | Batch Loss: 3.2681\n",
      "Epoch 16 | Batch 45/94 | Batch Loss: 3.3073\n",
      "Epoch 16 | Batch 46/94 | Batch Loss: 3.1717\n",
      "Epoch 16 | Batch 47/94 | Batch Loss: 3.3814\n",
      "Epoch 16 | Batch 48/94 | Batch Loss: 3.1801\n",
      "Epoch 16 | Batch 49/94 | Batch Loss: 3.3731\n",
      "Epoch 16 | Batch 50/94 | Batch Loss: 3.2607\n",
      "Epoch 16 | Batch 51/94 | Batch Loss: 3.3525\n",
      "Epoch 16 | Batch 52/94 | Batch Loss: 3.3291\n",
      "Epoch 16 | Batch 53/94 | Batch Loss: 3.2353\n",
      "Epoch 16 | Batch 54/94 | Batch Loss: 3.2708\n",
      "Epoch 16 | Batch 55/94 | Batch Loss: 3.2748\n",
      "Epoch 16 | Batch 56/94 | Batch Loss: 3.3231\n",
      "Epoch 16 | Batch 57/94 | Batch Loss: 3.2290\n",
      "Epoch 16 | Batch 58/94 | Batch Loss: 3.2698\n",
      "Epoch 16 | Batch 59/94 | Batch Loss: 3.2751\n",
      "Epoch 16 | Batch 60/94 | Batch Loss: 3.2638\n",
      "Epoch 16 | Batch 61/94 | Batch Loss: 3.3959\n",
      "Epoch 16 | Batch 62/94 | Batch Loss: 3.3078\n",
      "Epoch 16 | Batch 63/94 | Batch Loss: 3.3180\n",
      "Epoch 16 | Batch 64/94 | Batch Loss: 3.2428\n",
      "Epoch 16 | Batch 65/94 | Batch Loss: 3.3212\n",
      "Epoch 16 | Batch 66/94 | Batch Loss: 3.1515\n",
      "Epoch 16 | Batch 67/94 | Batch Loss: 3.2669\n",
      "Epoch 16 | Batch 68/94 | Batch Loss: 3.3590\n",
      "Epoch 16 | Batch 69/94 | Batch Loss: 3.3402\n",
      "Epoch 16 | Batch 70/94 | Batch Loss: 3.2997\n",
      "Epoch 16 | Batch 71/94 | Batch Loss: 3.2346\n",
      "Epoch 16 | Batch 72/94 | Batch Loss: 3.2978\n",
      "Epoch 16 | Batch 73/94 | Batch Loss: 3.2829\n",
      "Epoch 16 | Batch 74/94 | Batch Loss: 3.3512\n",
      "Epoch 16 | Batch 75/94 | Batch Loss: 3.3250\n",
      "Epoch 16 | Batch 76/94 | Batch Loss: 3.2888\n",
      "Epoch 16 | Batch 77/94 | Batch Loss: 3.2944\n",
      "Epoch 16 | Batch 78/94 | Batch Loss: 3.3469\n",
      "Epoch 16 | Batch 79/94 | Batch Loss: 3.2140\n",
      "Epoch 16 | Batch 80/94 | Batch Loss: 3.2662\n",
      "Epoch 16 | Batch 81/94 | Batch Loss: 3.2420\n",
      "Epoch 16 | Batch 82/94 | Batch Loss: 3.3504\n",
      "Epoch 16 | Batch 83/94 | Batch Loss: 3.2738\n",
      "Epoch 16 | Batch 84/94 | Batch Loss: 3.2491\n",
      "Epoch 16 | Batch 85/94 | Batch Loss: 3.2470\n",
      "Epoch 16 | Batch 86/94 | Batch Loss: 3.3631\n",
      "Epoch 16 | Batch 87/94 | Batch Loss: 3.2751\n",
      "Epoch 16 | Batch 88/94 | Batch Loss: 3.2879\n",
      "Epoch 16 | Batch 89/94 | Batch Loss: 3.2353\n",
      "Epoch 16 | Batch 90/94 | Batch Loss: 3.1493\n",
      "Epoch 16 | Batch 91/94 | Batch Loss: 3.3010\n",
      "Epoch 16 | Batch 92/94 | Batch Loss: 3.2686\n",
      "Epoch 16 | Batch 93/94 | Batch Loss: 3.3018\n",
      "Epoch 16 | Batch 94/94 | Batch Loss: 3.1898\n",
      "Epoch 16 Finished | Average Loss: 3.2850\n",
      "\n",
      "Epoch 17 Starting...\n",
      "Epoch 17 | Batch 1/94 | Batch Loss: 3.2252\n",
      "Epoch 17 | Batch 2/94 | Batch Loss: 3.3048\n",
      "Epoch 17 | Batch 3/94 | Batch Loss: 3.3402\n",
      "Epoch 17 | Batch 4/94 | Batch Loss: 3.2612\n",
      "Epoch 17 | Batch 5/94 | Batch Loss: 3.2200\n",
      "Epoch 17 | Batch 6/94 | Batch Loss: 3.2715\n",
      "Epoch 17 | Batch 7/94 | Batch Loss: 3.1776\n",
      "Epoch 17 | Batch 8/94 | Batch Loss: 3.3297\n",
      "Epoch 17 | Batch 9/94 | Batch Loss: 3.3295\n",
      "Epoch 17 | Batch 10/94 | Batch Loss: 3.2592\n",
      "Epoch 17 | Batch 11/94 | Batch Loss: 3.3538\n",
      "Epoch 17 | Batch 12/94 | Batch Loss: 3.1966\n",
      "Epoch 17 | Batch 13/94 | Batch Loss: 3.2734\n",
      "Epoch 17 | Batch 14/94 | Batch Loss: 3.2791\n",
      "Epoch 17 | Batch 15/94 | Batch Loss: 3.1830\n",
      "Epoch 17 | Batch 16/94 | Batch Loss: 3.2207\n",
      "Epoch 17 | Batch 17/94 | Batch Loss: 3.3512\n",
      "Epoch 17 | Batch 18/94 | Batch Loss: 3.3269\n",
      "Epoch 17 | Batch 19/94 | Batch Loss: 3.2960\n",
      "Epoch 17 | Batch 20/94 | Batch Loss: 3.3055\n",
      "Epoch 17 | Batch 21/94 | Batch Loss: 3.1903\n",
      "Epoch 17 | Batch 22/94 | Batch Loss: 3.1803\n",
      "Epoch 17 | Batch 23/94 | Batch Loss: 3.2209\n",
      "Epoch 17 | Batch 24/94 | Batch Loss: 3.1785\n",
      "Epoch 17 | Batch 25/94 | Batch Loss: 3.2038\n",
      "Epoch 17 | Batch 26/94 | Batch Loss: 3.2131\n",
      "Epoch 17 | Batch 27/94 | Batch Loss: 3.3257\n",
      "Epoch 17 | Batch 28/94 | Batch Loss: 3.3065\n",
      "Epoch 17 | Batch 29/94 | Batch Loss: 3.2459\n",
      "Epoch 17 | Batch 30/94 | Batch Loss: 3.2718\n",
      "Epoch 17 | Batch 31/94 | Batch Loss: 3.3846\n",
      "Epoch 17 | Batch 32/94 | Batch Loss: 3.2442\n",
      "Epoch 17 | Batch 33/94 | Batch Loss: 3.1735\n",
      "Epoch 17 | Batch 34/94 | Batch Loss: 3.0684\n",
      "Epoch 17 | Batch 35/94 | Batch Loss: 3.2983\n",
      "Epoch 17 | Batch 36/94 | Batch Loss: 3.3351\n",
      "Epoch 17 | Batch 37/94 | Batch Loss: 3.2335\n",
      "Epoch 17 | Batch 38/94 | Batch Loss: 3.2731\n",
      "Epoch 17 | Batch 39/94 | Batch Loss: 3.2255\n",
      "Epoch 17 | Batch 40/94 | Batch Loss: 3.1562\n",
      "Epoch 17 | Batch 41/94 | Batch Loss: 3.2269\n",
      "Epoch 17 | Batch 42/94 | Batch Loss: 3.2380\n",
      "Epoch 17 | Batch 43/94 | Batch Loss: 3.2751\n",
      "Epoch 17 | Batch 44/94 | Batch Loss: 3.2158\n",
      "Epoch 17 | Batch 45/94 | Batch Loss: 3.1807\n",
      "Epoch 17 | Batch 46/94 | Batch Loss: 3.3139\n",
      "Epoch 17 | Batch 47/94 | Batch Loss: 3.2844\n",
      "Epoch 17 | Batch 48/94 | Batch Loss: 3.2715\n",
      "Epoch 17 | Batch 49/94 | Batch Loss: 3.2652\n",
      "Epoch 17 | Batch 50/94 | Batch Loss: 3.2218\n",
      "Epoch 17 | Batch 51/94 | Batch Loss: 3.1662\n",
      "Epoch 17 | Batch 52/94 | Batch Loss: 3.2033\n",
      "Epoch 17 | Batch 53/94 | Batch Loss: 3.1829\n",
      "Epoch 17 | Batch 54/94 | Batch Loss: 3.2339\n",
      "Epoch 17 | Batch 55/94 | Batch Loss: 3.1812\n",
      "Epoch 17 | Batch 56/94 | Batch Loss: 3.0219\n",
      "Epoch 17 | Batch 57/94 | Batch Loss: 3.1388\n",
      "Epoch 17 | Batch 58/94 | Batch Loss: 3.2443\n",
      "Epoch 17 | Batch 59/94 | Batch Loss: 3.1859\n",
      "Epoch 17 | Batch 60/94 | Batch Loss: 3.2398\n",
      "Epoch 17 | Batch 61/94 | Batch Loss: 3.2015\n",
      "Epoch 17 | Batch 62/94 | Batch Loss: 3.0904\n",
      "Epoch 17 | Batch 63/94 | Batch Loss: 3.2941\n",
      "Epoch 17 | Batch 64/94 | Batch Loss: 3.2661\n",
      "Epoch 17 | Batch 65/94 | Batch Loss: 3.2196\n",
      "Epoch 17 | Batch 66/94 | Batch Loss: 3.2245\n",
      "Epoch 17 | Batch 67/94 | Batch Loss: 3.1470\n",
      "Epoch 17 | Batch 68/94 | Batch Loss: 3.2510\n",
      "Epoch 17 | Batch 69/94 | Batch Loss: 3.2691\n",
      "Epoch 17 | Batch 70/94 | Batch Loss: 3.3280\n",
      "Epoch 17 | Batch 71/94 | Batch Loss: 3.1577\n",
      "Epoch 17 | Batch 72/94 | Batch Loss: 3.2290\n",
      "Epoch 17 | Batch 73/94 | Batch Loss: 3.1825\n",
      "Epoch 17 | Batch 74/94 | Batch Loss: 3.0585\n",
      "Epoch 17 | Batch 75/94 | Batch Loss: 3.2930\n",
      "Epoch 17 | Batch 76/94 | Batch Loss: 3.1404\n",
      "Epoch 17 | Batch 77/94 | Batch Loss: 3.2310\n",
      "Epoch 17 | Batch 78/94 | Batch Loss: 3.3603\n",
      "Epoch 17 | Batch 79/94 | Batch Loss: 3.3248\n",
      "Epoch 17 | Batch 80/94 | Batch Loss: 3.1598\n",
      "Epoch 17 | Batch 81/94 | Batch Loss: 3.2316\n",
      "Epoch 17 | Batch 82/94 | Batch Loss: 3.3150\n",
      "Epoch 17 | Batch 83/94 | Batch Loss: 3.1332\n",
      "Epoch 17 | Batch 84/94 | Batch Loss: 3.2711\n",
      "Epoch 17 | Batch 85/94 | Batch Loss: 3.2576\n",
      "Epoch 17 | Batch 86/94 | Batch Loss: 3.2583\n",
      "Epoch 17 | Batch 87/94 | Batch Loss: 3.1733\n",
      "Epoch 17 | Batch 88/94 | Batch Loss: 3.2437\n",
      "Epoch 17 | Batch 89/94 | Batch Loss: 3.2820\n",
      "Epoch 17 | Batch 90/94 | Batch Loss: 3.2697\n",
      "Epoch 17 | Batch 91/94 | Batch Loss: 3.2162\n",
      "Epoch 17 | Batch 92/94 | Batch Loss: 3.2298\n",
      "Epoch 17 | Batch 93/94 | Batch Loss: 3.3282\n",
      "Epoch 17 | Batch 94/94 | Batch Loss: 3.3062\n",
      "Epoch 17 Finished | Average Loss: 3.2390\n",
      "\n",
      "Epoch 18 Starting...\n",
      "Epoch 18 | Batch 1/94 | Batch Loss: 3.2328\n",
      "Epoch 18 | Batch 2/94 | Batch Loss: 3.1987\n",
      "Epoch 18 | Batch 3/94 | Batch Loss: 3.2539\n",
      "Epoch 18 | Batch 4/94 | Batch Loss: 3.1952\n",
      "Epoch 18 | Batch 5/94 | Batch Loss: 3.1840\n",
      "Epoch 18 | Batch 6/94 | Batch Loss: 3.3122\n",
      "Epoch 18 | Batch 7/94 | Batch Loss: 3.2400\n",
      "Epoch 18 | Batch 8/94 | Batch Loss: 3.2303\n",
      "Epoch 18 | Batch 9/94 | Batch Loss: 3.1534\n",
      "Epoch 18 | Batch 10/94 | Batch Loss: 3.1755\n",
      "Epoch 18 | Batch 11/94 | Batch Loss: 3.2238\n",
      "Epoch 18 | Batch 12/94 | Batch Loss: 3.2373\n",
      "Epoch 18 | Batch 13/94 | Batch Loss: 3.0780\n",
      "Epoch 18 | Batch 14/94 | Batch Loss: 3.0456\n",
      "Epoch 18 | Batch 15/94 | Batch Loss: 3.2741\n",
      "Epoch 18 | Batch 16/94 | Batch Loss: 3.1770\n",
      "Epoch 18 | Batch 17/94 | Batch Loss: 3.2471\n",
      "Epoch 18 | Batch 18/94 | Batch Loss: 3.1735\n",
      "Epoch 18 | Batch 19/94 | Batch Loss: 3.2455\n",
      "Epoch 18 | Batch 20/94 | Batch Loss: 3.1825\n",
      "Epoch 18 | Batch 21/94 | Batch Loss: 3.2737\n",
      "Epoch 18 | Batch 22/94 | Batch Loss: 3.2185\n",
      "Epoch 18 | Batch 23/94 | Batch Loss: 3.2699\n",
      "Epoch 18 | Batch 24/94 | Batch Loss: 3.1732\n",
      "Epoch 18 | Batch 25/94 | Batch Loss: 3.1500\n",
      "Epoch 18 | Batch 26/94 | Batch Loss: 3.3228\n",
      "Epoch 18 | Batch 27/94 | Batch Loss: 3.2746\n",
      "Epoch 18 | Batch 28/94 | Batch Loss: 3.2395\n",
      "Epoch 18 | Batch 29/94 | Batch Loss: 3.1228\n",
      "Epoch 18 | Batch 30/94 | Batch Loss: 3.2165\n",
      "Epoch 18 | Batch 31/94 | Batch Loss: 3.1284\n",
      "Epoch 18 | Batch 32/94 | Batch Loss: 3.1624\n",
      "Epoch 18 | Batch 33/94 | Batch Loss: 3.1964\n",
      "Epoch 18 | Batch 34/94 | Batch Loss: 3.2003\n",
      "Epoch 18 | Batch 35/94 | Batch Loss: 3.1705\n",
      "Epoch 18 | Batch 36/94 | Batch Loss: 3.0746\n",
      "Epoch 18 | Batch 37/94 | Batch Loss: 3.2426\n",
      "Epoch 18 | Batch 38/94 | Batch Loss: 3.3039\n",
      "Epoch 18 | Batch 39/94 | Batch Loss: 3.1619\n",
      "Epoch 18 | Batch 40/94 | Batch Loss: 3.1302\n",
      "Epoch 18 | Batch 41/94 | Batch Loss: 3.1963\n",
      "Epoch 18 | Batch 42/94 | Batch Loss: 3.1231\n",
      "Epoch 18 | Batch 43/94 | Batch Loss: 3.0921\n",
      "Epoch 18 | Batch 44/94 | Batch Loss: 3.2551\n",
      "Epoch 18 | Batch 45/94 | Batch Loss: 3.1745\n",
      "Epoch 18 | Batch 46/94 | Batch Loss: 3.3224\n",
      "Epoch 18 | Batch 47/94 | Batch Loss: 3.1981\n",
      "Epoch 18 | Batch 48/94 | Batch Loss: 3.1918\n",
      "Epoch 18 | Batch 49/94 | Batch Loss: 3.1294\n",
      "Epoch 18 | Batch 50/94 | Batch Loss: 3.1177\n",
      "Epoch 18 | Batch 51/94 | Batch Loss: 3.1274\n",
      "Epoch 18 | Batch 52/94 | Batch Loss: 3.1927\n",
      "Epoch 18 | Batch 53/94 | Batch Loss: 3.1153\n",
      "Epoch 18 | Batch 54/94 | Batch Loss: 3.1619\n",
      "Epoch 18 | Batch 55/94 | Batch Loss: 3.0914\n",
      "Epoch 18 | Batch 56/94 | Batch Loss: 3.2684\n",
      "Epoch 18 | Batch 57/94 | Batch Loss: 3.2012\n",
      "Epoch 18 | Batch 58/94 | Batch Loss: 3.2673\n",
      "Epoch 18 | Batch 59/94 | Batch Loss: 3.0882\n",
      "Epoch 18 | Batch 60/94 | Batch Loss: 3.1284\n",
      "Epoch 18 | Batch 61/94 | Batch Loss: 3.1382\n",
      "Epoch 18 | Batch 62/94 | Batch Loss: 3.1778\n",
      "Epoch 18 | Batch 63/94 | Batch Loss: 3.2071\n",
      "Epoch 18 | Batch 64/94 | Batch Loss: 3.2284\n",
      "Epoch 18 | Batch 65/94 | Batch Loss: 3.2722\n",
      "Epoch 18 | Batch 66/94 | Batch Loss: 3.2428\n",
      "Epoch 18 | Batch 67/94 | Batch Loss: 3.1567\n",
      "Epoch 18 | Batch 68/94 | Batch Loss: 3.2388\n",
      "Epoch 18 | Batch 69/94 | Batch Loss: 3.2874\n",
      "Epoch 18 | Batch 70/94 | Batch Loss: 3.0552\n",
      "Epoch 18 | Batch 71/94 | Batch Loss: 3.2316\n",
      "Epoch 18 | Batch 72/94 | Batch Loss: 3.1408\n",
      "Epoch 18 | Batch 73/94 | Batch Loss: 3.1907\n",
      "Epoch 18 | Batch 74/94 | Batch Loss: 3.2566\n",
      "Epoch 18 | Batch 75/94 | Batch Loss: 3.1972\n",
      "Epoch 18 | Batch 76/94 | Batch Loss: 3.2285\n",
      "Epoch 18 | Batch 77/94 | Batch Loss: 3.2435\n",
      "Epoch 18 | Batch 78/94 | Batch Loss: 3.2303\n",
      "Epoch 18 | Batch 79/94 | Batch Loss: 3.0700\n",
      "Epoch 18 | Batch 80/94 | Batch Loss: 3.0605\n",
      "Epoch 18 | Batch 81/94 | Batch Loss: 3.1495\n",
      "Epoch 18 | Batch 82/94 | Batch Loss: 3.1822\n",
      "Epoch 18 | Batch 83/94 | Batch Loss: 3.1760\n",
      "Epoch 18 | Batch 84/94 | Batch Loss: 3.1510\n",
      "Epoch 18 | Batch 85/94 | Batch Loss: 3.1731\n",
      "Epoch 18 | Batch 86/94 | Batch Loss: 3.2877\n",
      "Epoch 18 | Batch 87/94 | Batch Loss: 3.3029\n",
      "Epoch 18 | Batch 88/94 | Batch Loss: 3.3101\n",
      "Epoch 18 | Batch 89/94 | Batch Loss: 3.2030\n",
      "Epoch 18 | Batch 90/94 | Batch Loss: 3.2121\n",
      "Epoch 18 | Batch 91/94 | Batch Loss: 3.0363\n",
      "Epoch 18 | Batch 92/94 | Batch Loss: 3.0978\n",
      "Epoch 18 | Batch 93/94 | Batch Loss: 3.2201\n",
      "Epoch 18 | Batch 94/94 | Batch Loss: 3.1879\n",
      "Epoch 18 Finished | Average Loss: 3.1923\n",
      "\n",
      "Epoch 19 Starting...\n",
      "Epoch 19 | Batch 1/94 | Batch Loss: 3.2729\n",
      "Epoch 19 | Batch 2/94 | Batch Loss: 3.2275\n",
      "Epoch 19 | Batch 3/94 | Batch Loss: 3.1136\n",
      "Epoch 19 | Batch 4/94 | Batch Loss: 2.9763\n",
      "Epoch 19 | Batch 5/94 | Batch Loss: 3.1218\n",
      "Epoch 19 | Batch 6/94 | Batch Loss: 3.0354\n",
      "Epoch 19 | Batch 7/94 | Batch Loss: 3.1861\n",
      "Epoch 19 | Batch 8/94 | Batch Loss: 3.1989\n",
      "Epoch 19 | Batch 9/94 | Batch Loss: 3.0336\n",
      "Epoch 19 | Batch 10/94 | Batch Loss: 3.1503\n",
      "Epoch 19 | Batch 11/94 | Batch Loss: 3.2127\n",
      "Epoch 19 | Batch 12/94 | Batch Loss: 3.0350\n",
      "Epoch 19 | Batch 13/94 | Batch Loss: 3.1361\n",
      "Epoch 19 | Batch 14/94 | Batch Loss: 3.0165\n",
      "Epoch 19 | Batch 15/94 | Batch Loss: 3.0682\n",
      "Epoch 19 | Batch 16/94 | Batch Loss: 3.2220\n",
      "Epoch 19 | Batch 17/94 | Batch Loss: 3.1769\n",
      "Epoch 19 | Batch 18/94 | Batch Loss: 3.1434\n",
      "Epoch 19 | Batch 19/94 | Batch Loss: 3.1517\n",
      "Epoch 19 | Batch 20/94 | Batch Loss: 3.1815\n",
      "Epoch 19 | Batch 21/94 | Batch Loss: 3.2873\n",
      "Epoch 19 | Batch 22/94 | Batch Loss: 3.1694\n",
      "Epoch 19 | Batch 23/94 | Batch Loss: 3.2093\n",
      "Epoch 19 | Batch 24/94 | Batch Loss: 3.1862\n",
      "Epoch 19 | Batch 25/94 | Batch Loss: 3.1499\n",
      "Epoch 19 | Batch 26/94 | Batch Loss: 3.0896\n",
      "Epoch 19 | Batch 27/94 | Batch Loss: 3.0872\n",
      "Epoch 19 | Batch 28/94 | Batch Loss: 3.1883\n",
      "Epoch 19 | Batch 29/94 | Batch Loss: 3.1962\n",
      "Epoch 19 | Batch 30/94 | Batch Loss: 3.1191\n",
      "Epoch 19 | Batch 31/94 | Batch Loss: 3.1323\n",
      "Epoch 19 | Batch 32/94 | Batch Loss: 3.2247\n",
      "Epoch 19 | Batch 33/94 | Batch Loss: 3.1395\n",
      "Epoch 19 | Batch 34/94 | Batch Loss: 3.1141\n",
      "Epoch 19 | Batch 35/94 | Batch Loss: 3.0698\n",
      "Epoch 19 | Batch 36/94 | Batch Loss: 3.1071\n",
      "Epoch 19 | Batch 37/94 | Batch Loss: 3.0923\n",
      "Epoch 19 | Batch 38/94 | Batch Loss: 3.1042\n",
      "Epoch 19 | Batch 39/94 | Batch Loss: 3.1567\n",
      "Epoch 19 | Batch 40/94 | Batch Loss: 3.2173\n",
      "Epoch 19 | Batch 41/94 | Batch Loss: 3.2362\n",
      "Epoch 19 | Batch 42/94 | Batch Loss: 3.3112\n",
      "Epoch 19 | Batch 43/94 | Batch Loss: 3.1643\n",
      "Epoch 19 | Batch 44/94 | Batch Loss: 3.1461\n",
      "Epoch 19 | Batch 45/94 | Batch Loss: 3.1432\n",
      "Epoch 19 | Batch 46/94 | Batch Loss: 3.2872\n",
      "Epoch 19 | Batch 47/94 | Batch Loss: 3.1271\n",
      "Epoch 19 | Batch 48/94 | Batch Loss: 3.1886\n",
      "Epoch 19 | Batch 49/94 | Batch Loss: 3.1607\n",
      "Epoch 19 | Batch 50/94 | Batch Loss: 3.1286\n",
      "Epoch 19 | Batch 51/94 | Batch Loss: 3.0978\n",
      "Epoch 19 | Batch 52/94 | Batch Loss: 3.1339\n",
      "Epoch 19 | Batch 53/94 | Batch Loss: 3.1457\n",
      "Epoch 19 | Batch 54/94 | Batch Loss: 3.2761\n",
      "Epoch 19 | Batch 55/94 | Batch Loss: 3.0462\n",
      "Epoch 19 | Batch 56/94 | Batch Loss: 3.1398\n",
      "Epoch 19 | Batch 57/94 | Batch Loss: 3.0222\n",
      "Epoch 19 | Batch 58/94 | Batch Loss: 3.1452\n",
      "Epoch 19 | Batch 59/94 | Batch Loss: 3.1633\n",
      "Epoch 19 | Batch 60/94 | Batch Loss: 3.1636\n",
      "Epoch 19 | Batch 61/94 | Batch Loss: 3.2425\n",
      "Epoch 19 | Batch 62/94 | Batch Loss: 3.2492\n",
      "Epoch 19 | Batch 63/94 | Batch Loss: 3.1115\n",
      "Epoch 19 | Batch 64/94 | Batch Loss: 3.2318\n",
      "Epoch 19 | Batch 65/94 | Batch Loss: 3.1293\n",
      "Epoch 19 | Batch 66/94 | Batch Loss: 3.0967\n",
      "Epoch 19 | Batch 67/94 | Batch Loss: 3.0531\n",
      "Epoch 19 | Batch 68/94 | Batch Loss: 3.1483\n",
      "Epoch 19 | Batch 69/94 | Batch Loss: 3.1077\n",
      "Epoch 19 | Batch 70/94 | Batch Loss: 3.2035\n",
      "Epoch 19 | Batch 71/94 | Batch Loss: 3.1079\n",
      "Epoch 19 | Batch 72/94 | Batch Loss: 3.0667\n",
      "Epoch 19 | Batch 73/94 | Batch Loss: 3.0567\n",
      "Epoch 19 | Batch 74/94 | Batch Loss: 3.0451\n",
      "Epoch 19 | Batch 75/94 | Batch Loss: 3.1811\n",
      "Epoch 19 | Batch 76/94 | Batch Loss: 3.1752\n",
      "Epoch 19 | Batch 77/94 | Batch Loss: 3.1672\n",
      "Epoch 19 | Batch 78/94 | Batch Loss: 3.2326\n",
      "Epoch 19 | Batch 79/94 | Batch Loss: 3.1840\n",
      "Epoch 19 | Batch 80/94 | Batch Loss: 3.1527\n",
      "Epoch 19 | Batch 81/94 | Batch Loss: 2.9524\n",
      "Epoch 19 | Batch 82/94 | Batch Loss: 3.3514\n",
      "Epoch 19 | Batch 83/94 | Batch Loss: 3.1892\n",
      "Epoch 19 | Batch 84/94 | Batch Loss: 3.0678\n",
      "Epoch 19 | Batch 85/94 | Batch Loss: 3.1946\n",
      "Epoch 19 | Batch 86/94 | Batch Loss: 3.0784\n",
      "Epoch 19 | Batch 87/94 | Batch Loss: 3.1443\n",
      "Epoch 19 | Batch 88/94 | Batch Loss: 3.2062\n",
      "Epoch 19 | Batch 89/94 | Batch Loss: 3.0862\n",
      "Epoch 19 | Batch 90/94 | Batch Loss: 3.0275\n",
      "Epoch 19 | Batch 91/94 | Batch Loss: 3.1966\n",
      "Epoch 19 | Batch 92/94 | Batch Loss: 3.2228\n",
      "Epoch 19 | Batch 93/94 | Batch Loss: 3.0823\n",
      "Epoch 19 | Batch 94/94 | Batch Loss: 3.1235\n",
      "Epoch 19 Finished | Average Loss: 3.1467\n",
      "\n",
      "Epoch 20 Starting...\n",
      "Epoch 20 | Batch 1/94 | Batch Loss: 3.0406\n",
      "Epoch 20 | Batch 2/94 | Batch Loss: 3.1139\n",
      "Epoch 20 | Batch 3/94 | Batch Loss: 3.1407\n",
      "Epoch 20 | Batch 4/94 | Batch Loss: 3.1398\n",
      "Epoch 20 | Batch 5/94 | Batch Loss: 3.1397\n",
      "Epoch 20 | Batch 6/94 | Batch Loss: 3.1777\n",
      "Epoch 20 | Batch 7/94 | Batch Loss: 3.1779\n",
      "Epoch 20 | Batch 8/94 | Batch Loss: 3.1736\n",
      "Epoch 20 | Batch 9/94 | Batch Loss: 3.0546\n",
      "Epoch 20 | Batch 10/94 | Batch Loss: 3.0308\n",
      "Epoch 20 | Batch 11/94 | Batch Loss: 3.2097\n",
      "Epoch 20 | Batch 12/94 | Batch Loss: 3.0531\n",
      "Epoch 20 | Batch 13/94 | Batch Loss: 3.1301\n",
      "Epoch 20 | Batch 14/94 | Batch Loss: 3.2165\n",
      "Epoch 20 | Batch 15/94 | Batch Loss: 2.9852\n",
      "Epoch 20 | Batch 16/94 | Batch Loss: 3.1011\n",
      "Epoch 20 | Batch 17/94 | Batch Loss: 3.1044\n",
      "Epoch 20 | Batch 18/94 | Batch Loss: 2.9551\n",
      "Epoch 20 | Batch 19/94 | Batch Loss: 3.0520\n",
      "Epoch 20 | Batch 20/94 | Batch Loss: 3.1182\n",
      "Epoch 20 | Batch 21/94 | Batch Loss: 3.2321\n",
      "Epoch 20 | Batch 22/94 | Batch Loss: 3.0953\n",
      "Epoch 20 | Batch 23/94 | Batch Loss: 3.0953\n",
      "Epoch 20 | Batch 24/94 | Batch Loss: 3.2596\n",
      "Epoch 20 | Batch 25/94 | Batch Loss: 3.1997\n",
      "Epoch 20 | Batch 26/94 | Batch Loss: 3.0566\n",
      "Epoch 20 | Batch 27/94 | Batch Loss: 3.0849\n",
      "Epoch 20 | Batch 28/94 | Batch Loss: 3.0380\n",
      "Epoch 20 | Batch 29/94 | Batch Loss: 3.0376\n",
      "Epoch 20 | Batch 30/94 | Batch Loss: 3.1703\n",
      "Epoch 20 | Batch 31/94 | Batch Loss: 3.1550\n",
      "Epoch 20 | Batch 32/94 | Batch Loss: 3.1391\n",
      "Epoch 20 | Batch 33/94 | Batch Loss: 3.1428\n",
      "Epoch 20 | Batch 34/94 | Batch Loss: 3.1325\n",
      "Epoch 20 | Batch 35/94 | Batch Loss: 3.1701\n",
      "Epoch 20 | Batch 36/94 | Batch Loss: 3.1237\n",
      "Epoch 20 | Batch 37/94 | Batch Loss: 3.1042\n",
      "Epoch 20 | Batch 38/94 | Batch Loss: 3.0338\n",
      "Epoch 20 | Batch 39/94 | Batch Loss: 3.0480\n",
      "Epoch 20 | Batch 40/94 | Batch Loss: 3.0833\n",
      "Epoch 20 | Batch 41/94 | Batch Loss: 3.0380\n",
      "Epoch 20 | Batch 42/94 | Batch Loss: 3.0278\n",
      "Epoch 20 | Batch 43/94 | Batch Loss: 3.0907\n",
      "Epoch 20 | Batch 44/94 | Batch Loss: 3.1277\n",
      "Epoch 20 | Batch 45/94 | Batch Loss: 3.0626\n",
      "Epoch 20 | Batch 46/94 | Batch Loss: 3.0080\n",
      "Epoch 20 | Batch 47/94 | Batch Loss: 3.2028\n",
      "Epoch 20 | Batch 48/94 | Batch Loss: 3.1042\n",
      "Epoch 20 | Batch 49/94 | Batch Loss: 3.0918\n",
      "Epoch 20 | Batch 50/94 | Batch Loss: 3.0555\n",
      "Epoch 20 | Batch 51/94 | Batch Loss: 3.0685\n",
      "Epoch 20 | Batch 52/94 | Batch Loss: 3.1053\n",
      "Epoch 20 | Batch 53/94 | Batch Loss: 3.1082\n",
      "Epoch 20 | Batch 54/94 | Batch Loss: 3.1062\n",
      "Epoch 20 | Batch 55/94 | Batch Loss: 3.1161\n",
      "Epoch 20 | Batch 56/94 | Batch Loss: 3.1521\n",
      "Epoch 20 | Batch 57/94 | Batch Loss: 3.1361\n",
      "Epoch 20 | Batch 58/94 | Batch Loss: 3.0833\n",
      "Epoch 20 | Batch 59/94 | Batch Loss: 3.0012\n",
      "Epoch 20 | Batch 60/94 | Batch Loss: 3.2484\n",
      "Epoch 20 | Batch 61/94 | Batch Loss: 3.1511\n",
      "Epoch 20 | Batch 62/94 | Batch Loss: 3.0554\n",
      "Epoch 20 | Batch 63/94 | Batch Loss: 3.1582\n",
      "Epoch 20 | Batch 64/94 | Batch Loss: 3.1974\n",
      "Epoch 20 | Batch 65/94 | Batch Loss: 3.1054\n",
      "Epoch 20 | Batch 66/94 | Batch Loss: 3.1135\n",
      "Epoch 20 | Batch 67/94 | Batch Loss: 3.1863\n",
      "Epoch 20 | Batch 68/94 | Batch Loss: 3.2234\n",
      "Epoch 20 | Batch 69/94 | Batch Loss: 3.0106\n",
      "Epoch 20 | Batch 70/94 | Batch Loss: 3.0882\n",
      "Epoch 20 | Batch 71/94 | Batch Loss: 3.1106\n",
      "Epoch 20 | Batch 72/94 | Batch Loss: 3.0600\n",
      "Epoch 20 | Batch 73/94 | Batch Loss: 3.1065\n",
      "Epoch 20 | Batch 74/94 | Batch Loss: 3.0682\n",
      "Epoch 20 | Batch 75/94 | Batch Loss: 3.0831\n",
      "Epoch 20 | Batch 76/94 | Batch Loss: 3.0749\n",
      "Epoch 20 | Batch 77/94 | Batch Loss: 3.1648\n",
      "Epoch 20 | Batch 78/94 | Batch Loss: 3.1518\n",
      "Epoch 20 | Batch 79/94 | Batch Loss: 3.1245\n",
      "Epoch 20 | Batch 80/94 | Batch Loss: 3.1561\n",
      "Epoch 20 | Batch 81/94 | Batch Loss: 3.0280\n",
      "Epoch 20 | Batch 82/94 | Batch Loss: 3.1043\n",
      "Epoch 20 | Batch 83/94 | Batch Loss: 2.9760\n",
      "Epoch 20 | Batch 84/94 | Batch Loss: 3.0635\n",
      "Epoch 20 | Batch 85/94 | Batch Loss: 3.1096\n",
      "Epoch 20 | Batch 86/94 | Batch Loss: 3.0604\n",
      "Epoch 20 | Batch 87/94 | Batch Loss: 3.0944\n",
      "Epoch 20 | Batch 88/94 | Batch Loss: 3.1800\n",
      "Epoch 20 | Batch 89/94 | Batch Loss: 3.0262\n",
      "Epoch 20 | Batch 90/94 | Batch Loss: 3.1220\n",
      "Epoch 20 | Batch 91/94 | Batch Loss: 3.0206\n",
      "Epoch 20 | Batch 92/94 | Batch Loss: 3.1421\n",
      "Epoch 20 | Batch 93/94 | Batch Loss: 3.0471\n",
      "Epoch 20 | Batch 94/94 | Batch Loss: 3.0587\n",
      "Epoch 20 Finished | Average Loss: 3.1050\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU Score: 0.51\n",
      "ROUGE-1: 0.00\n",
      "ROUGE-2: 0.00\n",
      "ROUGE-L: 0.00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# 1. Load Tokenizers\n",
    "tokenizer_en = spm.SentencePieceProcessor(model_file='bpe_en.model')\n",
    "tokenizer_ur = spm.SentencePieceProcessor(model_file='bpe_ur.model')\n",
    "\n",
    "# 2. Dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_lines, tgt_lines, src_tokenizer, tgt_tokenizer, max_len=128):\n",
    "        self.src_lines = src_lines\n",
    "        self.tgt_lines = tgt_lines\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = [self.src_tokenizer.bos_id()] + self.src_tokenizer.encode(self.src_lines[idx], out_type=int) + [self.src_tokenizer.eos_id()]\n",
    "        tgt = [self.tgt_tokenizer.bos_id()] + self.tgt_tokenizer.encode(self.tgt_lines[idx], out_type=int) + [self.tgt_tokenizer.eos_id()]\n",
    "        \n",
    "        src = src[:self.max_len]\n",
    "        tgt = tgt[:self.max_len]\n",
    "\n",
    "        src += [0] * (self.max_len - len(src))\n",
    "        tgt += [0] * (self.max_len - len(tgt))\n",
    "        \n",
    "        return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n",
    "\n",
    "# 3. Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, nhead=8, num_layers=4, dim_feedforward=512, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(5000, d_model), requires_grad=True)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True, # <-- Important to fix nested tensor warning\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.src_tok_emb(src) + self.positional_encoding[:src.size(1)]\n",
    "        tgt_emb = self.tgt_tok_emb(tgt) + self.positional_encoding[:tgt.size(1)]\n",
    "\n",
    "        src_padding_mask = (src == 0)\n",
    "        tgt_padding_mask = (tgt == 0)\n",
    "\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n",
    "\n",
    "        output = self.transformer(\n",
    "            src_emb,\n",
    "            tgt_emb,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=tgt_mask\n",
    "        )\n",
    "\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# 4. Data (assuming 'data' dictionary is loaded correctly)\n",
    "train_src = data['quran']['train.en']\n",
    "train_tgt = data['quran']['train.ur']\n",
    "test_src = data['quran']['test.en']\n",
    "test_tgt = data['quran']['test.ur']\n",
    "\n",
    "train_dataset = TranslationDataset(train_src, train_tgt, tokenizer_en, tokenizer_ur)\n",
    "test_dataset = TranslationDataset(test_src, test_tgt, tokenizer_en, tokenizer_ur)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# 5. Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TransformerModel(\n",
    "    tokenizer_en.get_piece_size(),\n",
    "    tokenizer_ur.get_piece_size()\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "# 6. Training loop\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    print(f\"\\nEpoch {epoch+1} Starting...\")\n",
    "\n",
    "    for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input)\n",
    "        output = output.reshape(-1, output.shape[-1])\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "\n",
    "        loss = criterion(output, tgt_output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Batch {batch_idx+1}/{len(train_loader)} | Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1} Finished | Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 7. Visualize loss\n",
    "plt.plot(train_losses, marker='o')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 8. Evaluation (BLEU + ROUGE)\n",
    "model.eval()\n",
    "references = []\n",
    "hypotheses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for src, tgt in test_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        output = model(src, tgt_input)\n",
    "        output = output.argmax(dim=-1)\n",
    "\n",
    "        for i in range(output.size(0)):\n",
    "            pred_tokens = output[i].cpu().numpy().tolist()\n",
    "            true_tokens = tgt[i, 1:].cpu().numpy().tolist()\n",
    "\n",
    "            # Filter padding\n",
    "            pred_tokens = [tok for tok in pred_tokens if tok != 0]\n",
    "            true_tokens = [tok for tok in true_tokens if tok != 0]\n",
    "\n",
    "            pred_sentence = tokenizer_ur.decode(pred_tokens)\n",
    "            true_sentence = tokenizer_ur.decode(true_tokens)\n",
    "\n",
    "            hypotheses.append(pred_sentence.split())\n",
    "            references.append([true_sentence.split()])\n",
    "\n",
    "# BLEU score\n",
    "smoothie = SmoothingFunction().method4\n",
    "bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
    "print(f\"\\nBLEU Score: {bleu_score*100:.2f}\")\n",
    "\n",
    "# ROUGE score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge1_list, rouge2_list, rougel_list = [], [], []\n",
    "\n",
    "for ref, hyp in zip(references, hypotheses):\n",
    "    ref_sent = ' '.join(ref[0])\n",
    "    hyp_sent = ' '.join(hyp)\n",
    "    scores = scorer.score(ref_sent, hyp_sent)\n",
    "\n",
    "    rouge1_list.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_list.append(scores['rouge2'].fmeasure)\n",
    "    rougel_list.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "print(f\"ROUGE-1: {np.mean(rouge1_list)*100:.2f}\")\n",
    "print(f\"ROUGE-2: {np.mean(rouge2_list)*100:.2f}\")\n",
    "print(f\"ROUGE-L: {np.mean(rougel_list)*100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f84b856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Starting...\n",
      "Epoch 1 | Batch 1/94 | Batch Loss: 7.0721\n",
      "Epoch 1 | Batch 2/94 | Batch Loss: 6.8752\n",
      "Epoch 1 | Batch 3/94 | Batch Loss: 6.7337\n",
      "Epoch 1 | Batch 4/94 | Batch Loss: 6.6342\n",
      "Epoch 1 | Batch 5/94 | Batch Loss: 6.5837\n",
      "Epoch 1 | Batch 6/94 | Batch Loss: 6.5062\n",
      "Epoch 1 | Batch 7/94 | Batch Loss: 6.4551\n",
      "Epoch 1 | Batch 8/94 | Batch Loss: 6.4195\n",
      "Epoch 1 | Batch 9/94 | Batch Loss: 6.3723\n",
      "Epoch 1 | Batch 10/94 | Batch Loss: 6.3598\n",
      "Epoch 1 | Batch 11/94 | Batch Loss: 6.3071\n",
      "Epoch 1 | Batch 12/94 | Batch Loss: 6.3128\n",
      "Epoch 1 | Batch 13/94 | Batch Loss: 6.2735\n",
      "Epoch 1 | Batch 14/94 | Batch Loss: 6.2704\n",
      "Epoch 1 | Batch 15/94 | Batch Loss: 6.2431\n",
      "Epoch 1 | Batch 16/94 | Batch Loss: 6.2147\n",
      "Epoch 1 | Batch 17/94 | Batch Loss: 6.1857\n",
      "Epoch 1 | Batch 18/94 | Batch Loss: 6.1709\n",
      "Epoch 1 | Batch 19/94 | Batch Loss: 6.1622\n",
      "Epoch 1 | Batch 20/94 | Batch Loss: 6.1356\n",
      "Epoch 1 | Batch 21/94 | Batch Loss: 6.1103\n",
      "Epoch 1 | Batch 22/94 | Batch Loss: 6.0551\n",
      "Epoch 1 | Batch 23/94 | Batch Loss: 6.0904\n",
      "Epoch 1 | Batch 24/94 | Batch Loss: 6.0255\n",
      "Epoch 1 | Batch 25/94 | Batch Loss: 6.0225\n",
      "Epoch 1 | Batch 26/94 | Batch Loss: 6.0248\n",
      "Epoch 1 | Batch 27/94 | Batch Loss: 6.0035\n",
      "Epoch 1 | Batch 28/94 | Batch Loss: 5.9960\n",
      "Epoch 1 | Batch 29/94 | Batch Loss: 5.9655\n",
      "Epoch 1 | Batch 30/94 | Batch Loss: 5.9444\n",
      "Epoch 1 | Batch 31/94 | Batch Loss: 5.9373\n",
      "Epoch 1 | Batch 32/94 | Batch Loss: 5.9024\n",
      "Epoch 1 | Batch 33/94 | Batch Loss: 5.9531\n",
      "Epoch 1 | Batch 34/94 | Batch Loss: 5.9052\n",
      "Epoch 1 | Batch 35/94 | Batch Loss: 5.8710\n",
      "Epoch 1 | Batch 36/94 | Batch Loss: 5.8904\n",
      "Epoch 1 | Batch 37/94 | Batch Loss: 5.8463\n",
      "Epoch 1 | Batch 38/94 | Batch Loss: 5.8546\n",
      "Epoch 1 | Batch 39/94 | Batch Loss: 5.8023\n",
      "Epoch 1 | Batch 40/94 | Batch Loss: 5.8218\n",
      "Epoch 1 | Batch 41/94 | Batch Loss: 5.8108\n",
      "Epoch 1 | Batch 42/94 | Batch Loss: 5.7951\n",
      "Epoch 1 | Batch 43/94 | Batch Loss: 5.7826\n",
      "Epoch 1 | Batch 44/94 | Batch Loss: 5.8075\n",
      "Epoch 1 | Batch 45/94 | Batch Loss: 5.7537\n",
      "Epoch 1 | Batch 46/94 | Batch Loss: 5.7509\n",
      "Epoch 1 | Batch 47/94 | Batch Loss: 5.7246\n",
      "Epoch 1 | Batch 48/94 | Batch Loss: 5.7077\n",
      "Epoch 1 | Batch 49/94 | Batch Loss: 5.7286\n",
      "Epoch 1 | Batch 50/94 | Batch Loss: 5.7400\n",
      "Epoch 1 | Batch 51/94 | Batch Loss: 5.7102\n",
      "Epoch 1 | Batch 52/94 | Batch Loss: 5.6701\n",
      "Epoch 1 | Batch 53/94 | Batch Loss: 5.6863\n",
      "Epoch 1 | Batch 54/94 | Batch Loss: 5.6861\n",
      "Epoch 1 | Batch 55/94 | Batch Loss: 5.6695\n",
      "Epoch 1 | Batch 56/94 | Batch Loss: 5.6459\n",
      "Epoch 1 | Batch 57/94 | Batch Loss: 5.6976\n",
      "Epoch 1 | Batch 58/94 | Batch Loss: 5.6703\n",
      "Epoch 1 | Batch 59/94 | Batch Loss: 5.6574\n",
      "Epoch 1 | Batch 60/94 | Batch Loss: 5.6338\n",
      "Epoch 1 | Batch 61/94 | Batch Loss: 5.6502\n",
      "Epoch 1 | Batch 62/94 | Batch Loss: 5.6334\n",
      "Epoch 1 | Batch 63/94 | Batch Loss: 5.5785\n",
      "Epoch 1 | Batch 64/94 | Batch Loss: 5.6017\n",
      "Epoch 1 | Batch 65/94 | Batch Loss: 5.5701\n",
      "Epoch 1 | Batch 66/94 | Batch Loss: 5.5426\n",
      "Epoch 1 | Batch 67/94 | Batch Loss: 5.5531\n",
      "Epoch 1 | Batch 68/94 | Batch Loss: 5.5557\n",
      "Epoch 1 | Batch 69/94 | Batch Loss: 5.5593\n",
      "Epoch 1 | Batch 70/94 | Batch Loss: 5.4923\n",
      "Epoch 1 | Batch 71/94 | Batch Loss: 5.5681\n",
      "Epoch 1 | Batch 72/94 | Batch Loss: 5.5616\n",
      "Epoch 1 | Batch 73/94 | Batch Loss: 5.5040\n",
      "Epoch 1 | Batch 74/94 | Batch Loss: 5.4936\n",
      "Epoch 1 | Batch 75/94 | Batch Loss: 5.5146\n",
      "Epoch 1 | Batch 76/94 | Batch Loss: 5.4997\n",
      "Epoch 1 | Batch 77/94 | Batch Loss: 5.5011\n",
      "Epoch 1 | Batch 78/94 | Batch Loss: 5.4864\n",
      "Epoch 1 | Batch 79/94 | Batch Loss: 5.4490\n",
      "Epoch 1 | Batch 80/94 | Batch Loss: 5.4262\n",
      "Epoch 1 | Batch 81/94 | Batch Loss: 5.4372\n",
      "Epoch 1 | Batch 82/94 | Batch Loss: 5.3985\n",
      "Epoch 1 | Batch 83/94 | Batch Loss: 5.4715\n",
      "Epoch 1 | Batch 84/94 | Batch Loss: 5.4473\n",
      "Epoch 1 | Batch 85/94 | Batch Loss: 5.4306\n",
      "Epoch 1 | Batch 86/94 | Batch Loss: 5.4546\n",
      "Epoch 1 | Batch 87/94 | Batch Loss: 5.3985\n",
      "Epoch 1 | Batch 88/94 | Batch Loss: 5.4182\n",
      "Epoch 1 | Batch 89/94 | Batch Loss: 5.4030\n",
      "Epoch 1 | Batch 90/94 | Batch Loss: 5.3988\n",
      "Epoch 1 | Batch 91/94 | Batch Loss: 5.4015\n",
      "Epoch 1 | Batch 92/94 | Batch Loss: 5.3801\n",
      "Epoch 1 | Batch 93/94 | Batch Loss: 5.4056\n",
      "Epoch 1 | Batch 94/94 | Batch Loss: 5.3114\n",
      "Epoch 1 Finished | Average Loss: 5.8312\n",
      "\n",
      "Epoch 2 Starting...\n",
      "Epoch 2 | Batch 1/94 | Batch Loss: 5.3673\n",
      "Epoch 2 | Batch 2/94 | Batch Loss: 5.3348\n",
      "Epoch 2 | Batch 3/94 | Batch Loss: 5.3334\n",
      "Epoch 2 | Batch 4/94 | Batch Loss: 5.3807\n",
      "Epoch 2 | Batch 5/94 | Batch Loss: 5.2855\n",
      "Epoch 2 | Batch 6/94 | Batch Loss: 5.2910\n",
      "Epoch 2 | Batch 7/94 | Batch Loss: 5.3578\n",
      "Epoch 2 | Batch 8/94 | Batch Loss: 5.3492\n",
      "Epoch 2 | Batch 9/94 | Batch Loss: 5.2399\n",
      "Epoch 2 | Batch 10/94 | Batch Loss: 5.2901\n",
      "Epoch 2 | Batch 11/94 | Batch Loss: 5.3087\n",
      "Epoch 2 | Batch 12/94 | Batch Loss: 5.2264\n",
      "Epoch 2 | Batch 13/94 | Batch Loss: 5.3156\n",
      "Epoch 2 | Batch 14/94 | Batch Loss: 5.1991\n",
      "Epoch 2 | Batch 15/94 | Batch Loss: 5.2562\n",
      "Epoch 2 | Batch 16/94 | Batch Loss: 5.2532\n",
      "Epoch 2 | Batch 17/94 | Batch Loss: 5.2245\n",
      "Epoch 2 | Batch 18/94 | Batch Loss: 5.2458\n",
      "Epoch 2 | Batch 19/94 | Batch Loss: 5.2747\n",
      "Epoch 2 | Batch 20/94 | Batch Loss: 5.2304\n",
      "Epoch 2 | Batch 21/94 | Batch Loss: 5.2142\n",
      "Epoch 2 | Batch 22/94 | Batch Loss: 5.2133\n",
      "Epoch 2 | Batch 23/94 | Batch Loss: 5.2118\n",
      "Epoch 2 | Batch 24/94 | Batch Loss: 5.1949\n",
      "Epoch 2 | Batch 25/94 | Batch Loss: 5.1980\n",
      "Epoch 2 | Batch 26/94 | Batch Loss: 5.1970\n",
      "Epoch 2 | Batch 27/94 | Batch Loss: 5.1975\n",
      "Epoch 2 | Batch 28/94 | Batch Loss: 5.1973\n",
      "Epoch 2 | Batch 29/94 | Batch Loss: 5.1522\n",
      "Epoch 2 | Batch 30/94 | Batch Loss: 5.1646\n",
      "Epoch 2 | Batch 31/94 | Batch Loss: 5.1375\n",
      "Epoch 2 | Batch 32/94 | Batch Loss: 5.1749\n",
      "Epoch 2 | Batch 33/94 | Batch Loss: 5.1521\n",
      "Epoch 2 | Batch 34/94 | Batch Loss: 5.1266\n",
      "Epoch 2 | Batch 35/94 | Batch Loss: 5.1414\n",
      "Epoch 2 | Batch 36/94 | Batch Loss: 5.0810\n",
      "Epoch 2 | Batch 37/94 | Batch Loss: 5.1332\n",
      "Epoch 2 | Batch 38/94 | Batch Loss: 5.0873\n",
      "Epoch 2 | Batch 39/94 | Batch Loss: 5.1202\n",
      "Epoch 2 | Batch 40/94 | Batch Loss: 5.0879\n",
      "Epoch 2 | Batch 41/94 | Batch Loss: 5.1261\n",
      "Epoch 2 | Batch 42/94 | Batch Loss: 5.0943\n",
      "Epoch 2 | Batch 43/94 | Batch Loss: 5.0835\n",
      "Epoch 2 | Batch 44/94 | Batch Loss: 5.0779\n",
      "Epoch 2 | Batch 45/94 | Batch Loss: 5.0527\n",
      "Epoch 2 | Batch 46/94 | Batch Loss: 5.0198\n",
      "Epoch 2 | Batch 47/94 | Batch Loss: 5.0248\n",
      "Epoch 2 | Batch 48/94 | Batch Loss: 5.0500\n",
      "Epoch 2 | Batch 49/94 | Batch Loss: 5.0501\n",
      "Epoch 2 | Batch 50/94 | Batch Loss: 4.9451\n",
      "Epoch 2 | Batch 51/94 | Batch Loss: 5.0775\n",
      "Epoch 2 | Batch 52/94 | Batch Loss: 5.0600\n",
      "Epoch 2 | Batch 53/94 | Batch Loss: 4.9570\n",
      "Epoch 2 | Batch 54/94 | Batch Loss: 4.9856\n",
      "Epoch 2 | Batch 55/94 | Batch Loss: 5.0101\n",
      "Epoch 2 | Batch 56/94 | Batch Loss: 4.9678\n",
      "Epoch 2 | Batch 57/94 | Batch Loss: 5.1138\n",
      "Epoch 2 | Batch 58/94 | Batch Loss: 4.8918\n",
      "Epoch 2 | Batch 59/94 | Batch Loss: 4.9794\n",
      "Epoch 2 | Batch 60/94 | Batch Loss: 4.9752\n",
      "Epoch 2 | Batch 61/94 | Batch Loss: 4.9969\n",
      "Epoch 2 | Batch 62/94 | Batch Loss: 5.0010\n",
      "Epoch 2 | Batch 63/94 | Batch Loss: 4.9775\n",
      "Epoch 2 | Batch 64/94 | Batch Loss: 4.9255\n",
      "Epoch 2 | Batch 65/94 | Batch Loss: 4.9243\n",
      "Epoch 2 | Batch 66/94 | Batch Loss: 4.9680\n",
      "Epoch 2 | Batch 67/94 | Batch Loss: 4.9500\n",
      "Epoch 2 | Batch 68/94 | Batch Loss: 4.9814\n",
      "Epoch 2 | Batch 69/94 | Batch Loss: 4.9717\n",
      "Epoch 2 | Batch 70/94 | Batch Loss: 5.0339\n",
      "Epoch 2 | Batch 71/94 | Batch Loss: 4.9068\n",
      "Epoch 2 | Batch 72/94 | Batch Loss: 4.9328\n",
      "Epoch 2 | Batch 73/94 | Batch Loss: 4.9072\n",
      "Epoch 2 | Batch 74/94 | Batch Loss: 4.8962\n",
      "Epoch 2 | Batch 75/94 | Batch Loss: 4.8732\n",
      "Epoch 2 | Batch 76/94 | Batch Loss: 4.8948\n",
      "Epoch 2 | Batch 77/94 | Batch Loss: 4.8543\n",
      "Epoch 2 | Batch 78/94 | Batch Loss: 4.9212\n",
      "Epoch 2 | Batch 79/94 | Batch Loss: 4.9347\n",
      "Epoch 2 | Batch 80/94 | Batch Loss: 4.8596\n",
      "Epoch 2 | Batch 81/94 | Batch Loss: 4.9317\n",
      "Epoch 2 | Batch 82/94 | Batch Loss: 4.8705\n",
      "Epoch 2 | Batch 83/94 | Batch Loss: 4.8579\n",
      "Epoch 2 | Batch 84/94 | Batch Loss: 4.8603\n",
      "Epoch 2 | Batch 85/94 | Batch Loss: 4.8835\n",
      "Epoch 2 | Batch 86/94 | Batch Loss: 4.8079\n",
      "Epoch 2 | Batch 87/94 | Batch Loss: 4.7933\n",
      "Epoch 2 | Batch 88/94 | Batch Loss: 4.8362\n",
      "Epoch 2 | Batch 89/94 | Batch Loss: 4.8516\n",
      "Epoch 2 | Batch 90/94 | Batch Loss: 4.8735\n",
      "Epoch 2 | Batch 91/94 | Batch Loss: 4.9028\n",
      "Epoch 2 | Batch 92/94 | Batch Loss: 4.8737\n",
      "Epoch 2 | Batch 93/94 | Batch Loss: 4.8451\n",
      "Epoch 2 | Batch 94/94 | Batch Loss: 4.8566\n",
      "Epoch 2 Finished | Average Loss: 5.0664\n",
      "\n",
      "Epoch 3 Starting...\n",
      "Epoch 3 | Batch 1/94 | Batch Loss: 4.7396\n",
      "Epoch 3 | Batch 2/94 | Batch Loss: 4.7284\n",
      "Epoch 3 | Batch 3/94 | Batch Loss: 4.8082\n",
      "Epoch 3 | Batch 4/94 | Batch Loss: 4.8356\n",
      "Epoch 3 | Batch 5/94 | Batch Loss: 4.7546\n",
      "Epoch 3 | Batch 6/94 | Batch Loss: 4.7506\n",
      "Epoch 3 | Batch 7/94 | Batch Loss: 4.7822\n",
      "Epoch 3 | Batch 8/94 | Batch Loss: 4.8606\n",
      "Epoch 3 | Batch 9/94 | Batch Loss: 4.7693\n",
      "Epoch 3 | Batch 10/94 | Batch Loss: 4.7760\n",
      "Epoch 3 | Batch 11/94 | Batch Loss: 4.7311\n",
      "Epoch 3 | Batch 12/94 | Batch Loss: 4.7868\n",
      "Epoch 3 | Batch 13/94 | Batch Loss: 4.8029\n",
      "Epoch 3 | Batch 14/94 | Batch Loss: 4.6768\n",
      "Epoch 3 | Batch 15/94 | Batch Loss: 4.6936\n",
      "Epoch 3 | Batch 16/94 | Batch Loss: 4.7046\n",
      "Epoch 3 | Batch 17/94 | Batch Loss: 4.7549\n",
      "Epoch 3 | Batch 18/94 | Batch Loss: 4.6748\n",
      "Epoch 3 | Batch 19/94 | Batch Loss: 4.6470\n",
      "Epoch 3 | Batch 20/94 | Batch Loss: 4.6854\n",
      "Epoch 3 | Batch 21/94 | Batch Loss: 4.6684\n",
      "Epoch 3 | Batch 22/94 | Batch Loss: 4.7501\n",
      "Epoch 3 | Batch 23/94 | Batch Loss: 4.7057\n",
      "Epoch 3 | Batch 24/94 | Batch Loss: 4.6722\n",
      "Epoch 3 | Batch 25/94 | Batch Loss: 4.6952\n",
      "Epoch 3 | Batch 26/94 | Batch Loss: 4.7776\n",
      "Epoch 3 | Batch 27/94 | Batch Loss: 4.7355\n",
      "Epoch 3 | Batch 28/94 | Batch Loss: 4.6812\n",
      "Epoch 3 | Batch 29/94 | Batch Loss: 4.6535\n",
      "Epoch 3 | Batch 30/94 | Batch Loss: 4.6620\n",
      "Epoch 3 | Batch 31/94 | Batch Loss: 4.6517\n",
      "Epoch 3 | Batch 32/94 | Batch Loss: 4.6857\n",
      "Epoch 3 | Batch 33/94 | Batch Loss: 4.6842\n",
      "Epoch 3 | Batch 34/94 | Batch Loss: 4.7096\n",
      "Epoch 3 | Batch 35/94 | Batch Loss: 4.6898\n",
      "Epoch 3 | Batch 36/94 | Batch Loss: 4.6877\n",
      "Epoch 3 | Batch 37/94 | Batch Loss: 4.5424\n",
      "Epoch 3 | Batch 38/94 | Batch Loss: 4.6369\n",
      "Epoch 3 | Batch 39/94 | Batch Loss: 4.5927\n",
      "Epoch 3 | Batch 40/94 | Batch Loss: 4.6056\n",
      "Epoch 3 | Batch 41/94 | Batch Loss: 4.5312\n",
      "Epoch 3 | Batch 42/94 | Batch Loss: 4.6252\n",
      "Epoch 3 | Batch 43/94 | Batch Loss: 4.6791\n",
      "Epoch 3 | Batch 44/94 | Batch Loss: 4.5883\n",
      "Epoch 3 | Batch 45/94 | Batch Loss: 4.5780\n",
      "Epoch 3 | Batch 46/94 | Batch Loss: 4.5967\n",
      "Epoch 3 | Batch 47/94 | Batch Loss: 4.6434\n",
      "Epoch 3 | Batch 48/94 | Batch Loss: 4.5653\n",
      "Epoch 3 | Batch 49/94 | Batch Loss: 4.6370\n",
      "Epoch 3 | Batch 50/94 | Batch Loss: 4.5707\n",
      "Epoch 3 | Batch 51/94 | Batch Loss: 4.6164\n",
      "Epoch 3 | Batch 52/94 | Batch Loss: 4.5928\n",
      "Epoch 3 | Batch 53/94 | Batch Loss: 4.5897\n",
      "Epoch 3 | Batch 54/94 | Batch Loss: 4.5651\n",
      "Epoch 3 | Batch 55/94 | Batch Loss: 4.6148\n",
      "Epoch 3 | Batch 56/94 | Batch Loss: 4.6071\n",
      "Epoch 3 | Batch 57/94 | Batch Loss: 4.6421\n",
      "Epoch 3 | Batch 58/94 | Batch Loss: 4.5205\n",
      "Epoch 3 | Batch 59/94 | Batch Loss: 4.6443\n",
      "Epoch 3 | Batch 60/94 | Batch Loss: 4.6534\n",
      "Epoch 3 | Batch 61/94 | Batch Loss: 4.5028\n",
      "Epoch 3 | Batch 62/94 | Batch Loss: 4.5575\n",
      "Epoch 3 | Batch 63/94 | Batch Loss: 4.4607\n",
      "Epoch 3 | Batch 64/94 | Batch Loss: 4.4616\n",
      "Epoch 3 | Batch 65/94 | Batch Loss: 4.5569\n",
      "Epoch 3 | Batch 66/94 | Batch Loss: 4.5099\n",
      "Epoch 3 | Batch 67/94 | Batch Loss: 4.5665\n",
      "Epoch 3 | Batch 68/94 | Batch Loss: 4.5340\n",
      "Epoch 3 | Batch 69/94 | Batch Loss: 4.6430\n",
      "Epoch 3 | Batch 70/94 | Batch Loss: 4.4887\n",
      "Epoch 3 | Batch 71/94 | Batch Loss: 4.5198\n",
      "Epoch 3 | Batch 72/94 | Batch Loss: 4.5338\n",
      "Epoch 3 | Batch 73/94 | Batch Loss: 4.5206\n",
      "Epoch 3 | Batch 74/94 | Batch Loss: 4.5874\n",
      "Epoch 3 | Batch 75/94 | Batch Loss: 4.5576\n",
      "Epoch 3 | Batch 76/94 | Batch Loss: 4.5248\n",
      "Epoch 3 | Batch 77/94 | Batch Loss: 4.4852\n",
      "Epoch 3 | Batch 78/94 | Batch Loss: 4.4881\n",
      "Epoch 3 | Batch 79/94 | Batch Loss: 4.4928\n",
      "Epoch 3 | Batch 80/94 | Batch Loss: 4.4245\n",
      "Epoch 3 | Batch 81/94 | Batch Loss: 4.5932\n",
      "Epoch 3 | Batch 82/94 | Batch Loss: 4.5580\n",
      "Epoch 3 | Batch 83/94 | Batch Loss: 4.4750\n",
      "Epoch 3 | Batch 84/94 | Batch Loss: 4.5509\n",
      "Epoch 3 | Batch 85/94 | Batch Loss: 4.4376\n",
      "Epoch 3 | Batch 86/94 | Batch Loss: 4.4708\n",
      "Epoch 3 | Batch 87/94 | Batch Loss: 4.4310\n",
      "Epoch 3 | Batch 88/94 | Batch Loss: 4.5168\n",
      "Epoch 3 | Batch 89/94 | Batch Loss: 4.4969\n",
      "Epoch 3 | Batch 90/94 | Batch Loss: 4.4748\n",
      "Epoch 3 | Batch 91/94 | Batch Loss: 4.4639\n",
      "Epoch 3 | Batch 92/94 | Batch Loss: 4.3676\n",
      "Epoch 3 | Batch 93/94 | Batch Loss: 4.4244\n",
      "Epoch 3 | Batch 94/94 | Batch Loss: 4.4854\n",
      "Epoch 3 Finished | Average Loss: 4.6136\n",
      "\n",
      "Epoch 4 Starting...\n",
      "Epoch 4 | Batch 1/94 | Batch Loss: 4.5261\n",
      "Epoch 4 | Batch 2/94 | Batch Loss: 4.3659\n",
      "Epoch 4 | Batch 3/94 | Batch Loss: 4.3973\n",
      "Epoch 4 | Batch 4/94 | Batch Loss: 4.4242\n",
      "Epoch 4 | Batch 5/94 | Batch Loss: 4.4010\n",
      "Epoch 4 | Batch 6/94 | Batch Loss: 4.4092\n",
      "Epoch 4 | Batch 7/94 | Batch Loss: 4.4336\n",
      "Epoch 4 | Batch 8/94 | Batch Loss: 4.4300\n",
      "Epoch 4 | Batch 9/94 | Batch Loss: 4.3793\n",
      "Epoch 4 | Batch 10/94 | Batch Loss: 4.4625\n",
      "Epoch 4 | Batch 11/94 | Batch Loss: 4.3436\n",
      "Epoch 4 | Batch 12/94 | Batch Loss: 4.3846\n",
      "Epoch 4 | Batch 13/94 | Batch Loss: 4.4712\n",
      "Epoch 4 | Batch 14/94 | Batch Loss: 4.4171\n",
      "Epoch 4 | Batch 15/94 | Batch Loss: 4.4303\n",
      "Epoch 4 | Batch 16/94 | Batch Loss: 4.4109\n",
      "Epoch 4 | Batch 17/94 | Batch Loss: 4.4036\n",
      "Epoch 4 | Batch 18/94 | Batch Loss: 4.3779\n",
      "Epoch 4 | Batch 19/94 | Batch Loss: 4.3836\n",
      "Epoch 4 | Batch 20/94 | Batch Loss: 4.4259\n",
      "Epoch 4 | Batch 21/94 | Batch Loss: 4.4329\n",
      "Epoch 4 | Batch 22/94 | Batch Loss: 4.3787\n",
      "Epoch 4 | Batch 23/94 | Batch Loss: 4.3828\n",
      "Epoch 4 | Batch 24/94 | Batch Loss: 4.3337\n",
      "Epoch 4 | Batch 25/94 | Batch Loss: 4.3838\n",
      "Epoch 4 | Batch 26/94 | Batch Loss: 4.3936\n",
      "Epoch 4 | Batch 27/94 | Batch Loss: 4.3529\n",
      "Epoch 4 | Batch 28/94 | Batch Loss: 4.3583\n",
      "Epoch 4 | Batch 29/94 | Batch Loss: 4.3652\n",
      "Epoch 4 | Batch 30/94 | Batch Loss: 4.3312\n",
      "Epoch 4 | Batch 31/94 | Batch Loss: 4.3292\n",
      "Epoch 4 | Batch 32/94 | Batch Loss: 4.3513\n",
      "Epoch 4 | Batch 33/94 | Batch Loss: 4.3343\n",
      "Epoch 4 | Batch 34/94 | Batch Loss: 4.3604\n",
      "Epoch 4 | Batch 35/94 | Batch Loss: 4.3184\n",
      "Epoch 4 | Batch 36/94 | Batch Loss: 4.3566\n",
      "Epoch 4 | Batch 37/94 | Batch Loss: 4.3204\n",
      "Epoch 4 | Batch 38/94 | Batch Loss: 4.4287\n",
      "Epoch 4 | Batch 39/94 | Batch Loss: 4.3785\n",
      "Epoch 4 | Batch 40/94 | Batch Loss: 4.3344\n",
      "Epoch 4 | Batch 41/94 | Batch Loss: 4.3480\n",
      "Epoch 4 | Batch 42/94 | Batch Loss: 4.3086\n",
      "Epoch 4 | Batch 43/94 | Batch Loss: 4.2911\n",
      "Epoch 4 | Batch 44/94 | Batch Loss: 4.3419\n",
      "Epoch 4 | Batch 45/94 | Batch Loss: 4.4118\n",
      "Epoch 4 | Batch 46/94 | Batch Loss: 4.4126\n",
      "Epoch 4 | Batch 47/94 | Batch Loss: 4.3529\n",
      "Epoch 4 | Batch 48/94 | Batch Loss: 4.3232\n",
      "Epoch 4 | Batch 49/94 | Batch Loss: 4.3261\n",
      "Epoch 4 | Batch 50/94 | Batch Loss: 4.2669\n",
      "Epoch 4 | Batch 51/94 | Batch Loss: 4.3700\n",
      "Epoch 4 | Batch 52/94 | Batch Loss: 4.2864\n",
      "Epoch 4 | Batch 53/94 | Batch Loss: 4.4181\n",
      "Epoch 4 | Batch 54/94 | Batch Loss: 4.4225\n",
      "Epoch 4 | Batch 55/94 | Batch Loss: 4.3591\n",
      "Epoch 4 | Batch 56/94 | Batch Loss: 4.3914\n",
      "Epoch 4 | Batch 57/94 | Batch Loss: 4.3008\n",
      "Epoch 4 | Batch 58/94 | Batch Loss: 4.2471\n",
      "Epoch 4 | Batch 59/94 | Batch Loss: 4.3492\n",
      "Epoch 4 | Batch 60/94 | Batch Loss: 4.2681\n",
      "Epoch 4 | Batch 61/94 | Batch Loss: 4.2489\n",
      "Epoch 4 | Batch 62/94 | Batch Loss: 4.3466\n",
      "Epoch 4 | Batch 63/94 | Batch Loss: 4.2561\n",
      "Epoch 4 | Batch 64/94 | Batch Loss: 4.2348\n",
      "Epoch 4 | Batch 65/94 | Batch Loss: 4.2733\n",
      "Epoch 4 | Batch 66/94 | Batch Loss: 4.2324\n",
      "Epoch 4 | Batch 67/94 | Batch Loss: 4.3510\n",
      "Epoch 4 | Batch 68/94 | Batch Loss: 4.3605\n",
      "Epoch 4 | Batch 69/94 | Batch Loss: 4.3023\n",
      "Epoch 4 | Batch 70/94 | Batch Loss: 4.3838\n",
      "Epoch 4 | Batch 71/94 | Batch Loss: 4.2513\n",
      "Epoch 4 | Batch 72/94 | Batch Loss: 4.2613\n",
      "Epoch 4 | Batch 73/94 | Batch Loss: 4.2900\n",
      "Epoch 4 | Batch 74/94 | Batch Loss: 4.2134\n",
      "Epoch 4 | Batch 75/94 | Batch Loss: 4.2160\n",
      "Epoch 4 | Batch 76/94 | Batch Loss: 4.2623\n",
      "Epoch 4 | Batch 77/94 | Batch Loss: 4.2512\n",
      "Epoch 4 | Batch 78/94 | Batch Loss: 4.3353\n",
      "Epoch 4 | Batch 79/94 | Batch Loss: 4.2529\n",
      "Epoch 4 | Batch 80/94 | Batch Loss: 4.3157\n",
      "Epoch 4 | Batch 81/94 | Batch Loss: 4.2629\n",
      "Epoch 4 | Batch 82/94 | Batch Loss: 4.3453\n",
      "Epoch 4 | Batch 83/94 | Batch Loss: 4.2241\n",
      "Epoch 4 | Batch 84/94 | Batch Loss: 4.2971\n",
      "Epoch 4 | Batch 85/94 | Batch Loss: 4.2865\n",
      "Epoch 4 | Batch 86/94 | Batch Loss: 4.2577\n",
      "Epoch 4 | Batch 87/94 | Batch Loss: 4.2271\n",
      "Epoch 4 | Batch 88/94 | Batch Loss: 4.2423\n",
      "Epoch 4 | Batch 89/94 | Batch Loss: 4.2346\n",
      "Epoch 4 | Batch 90/94 | Batch Loss: 4.3426\n",
      "Epoch 4 | Batch 91/94 | Batch Loss: 4.2795\n",
      "Epoch 4 | Batch 92/94 | Batch Loss: 4.3074\n",
      "Epoch 4 | Batch 93/94 | Batch Loss: 4.2607\n",
      "Epoch 4 | Batch 94/94 | Batch Loss: 4.1643\n",
      "Epoch 4 Finished | Average Loss: 4.3369\n",
      "\n",
      "Epoch 5 Starting...\n",
      "Epoch 5 | Batch 1/94 | Batch Loss: 4.3112\n",
      "Epoch 5 | Batch 2/94 | Batch Loss: 4.1710\n",
      "Epoch 5 | Batch 3/94 | Batch Loss: 4.0807\n",
      "Epoch 5 | Batch 4/94 | Batch Loss: 4.1670\n",
      "Epoch 5 | Batch 5/94 | Batch Loss: 4.2272\n",
      "Epoch 5 | Batch 6/94 | Batch Loss: 4.0507\n",
      "Epoch 5 | Batch 7/94 | Batch Loss: 4.1953\n",
      "Epoch 5 | Batch 8/94 | Batch Loss: 4.3002\n",
      "Epoch 5 | Batch 9/94 | Batch Loss: 4.1626\n",
      "Epoch 5 | Batch 10/94 | Batch Loss: 4.1390\n",
      "Epoch 5 | Batch 11/94 | Batch Loss: 4.3421\n",
      "Epoch 5 | Batch 12/94 | Batch Loss: 4.1832\n",
      "Epoch 5 | Batch 13/94 | Batch Loss: 4.1325\n",
      "Epoch 5 | Batch 14/94 | Batch Loss: 4.2146\n",
      "Epoch 5 | Batch 15/94 | Batch Loss: 4.2112\n",
      "Epoch 5 | Batch 16/94 | Batch Loss: 4.1684\n",
      "Epoch 5 | Batch 17/94 | Batch Loss: 4.1884\n",
      "Epoch 5 | Batch 18/94 | Batch Loss: 4.2630\n",
      "Epoch 5 | Batch 19/94 | Batch Loss: 4.2068\n",
      "Epoch 5 | Batch 20/94 | Batch Loss: 4.1331\n",
      "Epoch 5 | Batch 21/94 | Batch Loss: 4.1047\n",
      "Epoch 5 | Batch 22/94 | Batch Loss: 4.1453\n",
      "Epoch 5 | Batch 23/94 | Batch Loss: 4.2096\n",
      "Epoch 5 | Batch 24/94 | Batch Loss: 4.1379\n",
      "Epoch 5 | Batch 25/94 | Batch Loss: 4.1472\n",
      "Epoch 5 | Batch 26/94 | Batch Loss: 4.1557\n",
      "Epoch 5 | Batch 27/94 | Batch Loss: 4.0870\n",
      "Epoch 5 | Batch 28/94 | Batch Loss: 4.1583\n",
      "Epoch 5 | Batch 29/94 | Batch Loss: 4.2092\n",
      "Epoch 5 | Batch 30/94 | Batch Loss: 4.2104\n",
      "Epoch 5 | Batch 31/94 | Batch Loss: 4.1880\n",
      "Epoch 5 | Batch 32/94 | Batch Loss: 4.0742\n",
      "Epoch 5 | Batch 33/94 | Batch Loss: 4.1635\n",
      "Epoch 5 | Batch 34/94 | Batch Loss: 4.2304\n",
      "Epoch 5 | Batch 35/94 | Batch Loss: 4.1909\n",
      "Epoch 5 | Batch 36/94 | Batch Loss: 4.1600\n",
      "Epoch 5 | Batch 37/94 | Batch Loss: 4.2368\n",
      "Epoch 5 | Batch 38/94 | Batch Loss: 4.1632\n",
      "Epoch 5 | Batch 39/94 | Batch Loss: 4.2018\n",
      "Epoch 5 | Batch 40/94 | Batch Loss: 4.2252\n",
      "Epoch 5 | Batch 41/94 | Batch Loss: 4.1302\n",
      "Epoch 5 | Batch 42/94 | Batch Loss: 4.1481\n",
      "Epoch 5 | Batch 43/94 | Batch Loss: 4.1325\n",
      "Epoch 5 | Batch 44/94 | Batch Loss: 4.1780\n",
      "Epoch 5 | Batch 45/94 | Batch Loss: 4.1830\n",
      "Epoch 5 | Batch 46/94 | Batch Loss: 4.1875\n",
      "Epoch 5 | Batch 47/94 | Batch Loss: 4.1508\n",
      "Epoch 5 | Batch 48/94 | Batch Loss: 4.1470\n",
      "Epoch 5 | Batch 49/94 | Batch Loss: 4.1985\n",
      "Epoch 5 | Batch 50/94 | Batch Loss: 4.2287\n",
      "Epoch 5 | Batch 51/94 | Batch Loss: 4.1348\n",
      "Epoch 5 | Batch 52/94 | Batch Loss: 4.1382\n",
      "Epoch 5 | Batch 53/94 | Batch Loss: 4.1405\n",
      "Epoch 5 | Batch 54/94 | Batch Loss: 4.2059\n",
      "Epoch 5 | Batch 55/94 | Batch Loss: 4.1252\n",
      "Epoch 5 | Batch 56/94 | Batch Loss: 4.0744\n",
      "Epoch 5 | Batch 57/94 | Batch Loss: 4.1493\n",
      "Epoch 5 | Batch 58/94 | Batch Loss: 4.1685\n",
      "Epoch 5 | Batch 59/94 | Batch Loss: 4.1559\n",
      "Epoch 5 | Batch 60/94 | Batch Loss: 4.1515\n",
      "Epoch 5 | Batch 61/94 | Batch Loss: 4.2133\n",
      "Epoch 5 | Batch 62/94 | Batch Loss: 4.1038\n",
      "Epoch 5 | Batch 63/94 | Batch Loss: 4.1065\n",
      "Epoch 5 | Batch 64/94 | Batch Loss: 4.1768\n",
      "Epoch 5 | Batch 65/94 | Batch Loss: 4.1036\n",
      "Epoch 5 | Batch 66/94 | Batch Loss: 4.0810\n",
      "Epoch 5 | Batch 67/94 | Batch Loss: 4.2226\n",
      "Epoch 5 | Batch 68/94 | Batch Loss: 4.0848\n",
      "Epoch 5 | Batch 69/94 | Batch Loss: 4.1060\n",
      "Epoch 5 | Batch 70/94 | Batch Loss: 4.1003\n",
      "Epoch 5 | Batch 71/94 | Batch Loss: 4.1645\n",
      "Epoch 5 | Batch 72/94 | Batch Loss: 4.1398\n",
      "Epoch 5 | Batch 73/94 | Batch Loss: 4.1362\n",
      "Epoch 5 | Batch 74/94 | Batch Loss: 4.0088\n",
      "Epoch 5 | Batch 75/94 | Batch Loss: 4.1572\n",
      "Epoch 5 | Batch 76/94 | Batch Loss: 4.0234\n",
      "Epoch 5 | Batch 77/94 | Batch Loss: 4.1275\n",
      "Epoch 5 | Batch 78/94 | Batch Loss: 4.0733\n",
      "Epoch 5 | Batch 79/94 | Batch Loss: 4.1213\n",
      "Epoch 5 | Batch 80/94 | Batch Loss: 4.1350\n",
      "Epoch 5 | Batch 81/94 | Batch Loss: 4.2201\n",
      "Epoch 5 | Batch 82/94 | Batch Loss: 4.0096\n",
      "Epoch 5 | Batch 83/94 | Batch Loss: 4.0392\n",
      "Epoch 5 | Batch 84/94 | Batch Loss: 4.2423\n",
      "Epoch 5 | Batch 85/94 | Batch Loss: 4.1564\n",
      "Epoch 5 | Batch 86/94 | Batch Loss: 4.0576\n",
      "Epoch 5 | Batch 87/94 | Batch Loss: 4.0835\n",
      "Epoch 5 | Batch 88/94 | Batch Loss: 4.0797\n",
      "Epoch 5 | Batch 89/94 | Batch Loss: 4.0706\n",
      "Epoch 5 | Batch 90/94 | Batch Loss: 4.1008\n",
      "Epoch 5 | Batch 91/94 | Batch Loss: 4.0578\n",
      "Epoch 5 | Batch 92/94 | Batch Loss: 3.9405\n",
      "Epoch 5 | Batch 93/94 | Batch Loss: 4.1803\n",
      "Epoch 5 | Batch 94/94 | Batch Loss: 4.0452\n",
      "Epoch 5 Finished | Average Loss: 4.1494\n",
      "\n",
      "Epoch 6 Starting...\n",
      "Epoch 6 | Batch 1/94 | Batch Loss: 4.1238\n",
      "Epoch 6 | Batch 2/94 | Batch Loss: 4.0263\n",
      "Epoch 6 | Batch 3/94 | Batch Loss: 4.0011\n",
      "Epoch 6 | Batch 4/94 | Batch Loss: 4.0923\n",
      "Epoch 6 | Batch 5/94 | Batch Loss: 4.0236\n",
      "Epoch 6 | Batch 6/94 | Batch Loss: 4.0181\n",
      "Epoch 6 | Batch 7/94 | Batch Loss: 4.0110\n",
      "Epoch 6 | Batch 8/94 | Batch Loss: 4.1243\n",
      "Epoch 6 | Batch 9/94 | Batch Loss: 3.9851\n",
      "Epoch 6 | Batch 10/94 | Batch Loss: 4.0316\n",
      "Epoch 6 | Batch 11/94 | Batch Loss: 4.0083\n",
      "Epoch 6 | Batch 12/94 | Batch Loss: 3.9879\n",
      "Epoch 6 | Batch 13/94 | Batch Loss: 4.0497\n",
      "Epoch 6 | Batch 14/94 | Batch Loss: 4.2072\n",
      "Epoch 6 | Batch 15/94 | Batch Loss: 4.0939\n",
      "Epoch 6 | Batch 16/94 | Batch Loss: 4.0855\n",
      "Epoch 6 | Batch 17/94 | Batch Loss: 4.0770\n",
      "Epoch 6 | Batch 18/94 | Batch Loss: 4.0435\n",
      "Epoch 6 | Batch 19/94 | Batch Loss: 3.9853\n",
      "Epoch 6 | Batch 20/94 | Batch Loss: 4.0386\n",
      "Epoch 6 | Batch 21/94 | Batch Loss: 4.0514\n",
      "Epoch 6 | Batch 22/94 | Batch Loss: 4.0528\n",
      "Epoch 6 | Batch 23/94 | Batch Loss: 4.1025\n",
      "Epoch 6 | Batch 24/94 | Batch Loss: 3.9913\n",
      "Epoch 6 | Batch 25/94 | Batch Loss: 4.0200\n",
      "Epoch 6 | Batch 26/94 | Batch Loss: 4.0986\n",
      "Epoch 6 | Batch 27/94 | Batch Loss: 3.9530\n",
      "Epoch 6 | Batch 28/94 | Batch Loss: 4.0041\n",
      "Epoch 6 | Batch 29/94 | Batch Loss: 4.0482\n",
      "Epoch 6 | Batch 30/94 | Batch Loss: 4.0351\n",
      "Epoch 6 | Batch 31/94 | Batch Loss: 4.0276\n",
      "Epoch 6 | Batch 32/94 | Batch Loss: 4.0596\n",
      "Epoch 6 | Batch 33/94 | Batch Loss: 4.0195\n",
      "Epoch 6 | Batch 34/94 | Batch Loss: 3.9673\n",
      "Epoch 6 | Batch 35/94 | Batch Loss: 3.9948\n",
      "Epoch 6 | Batch 36/94 | Batch Loss: 3.9540\n",
      "Epoch 6 | Batch 37/94 | Batch Loss: 3.9939\n",
      "Epoch 6 | Batch 38/94 | Batch Loss: 3.9672\n",
      "Epoch 6 | Batch 39/94 | Batch Loss: 3.9800\n",
      "Epoch 6 | Batch 40/94 | Batch Loss: 3.9288\n",
      "Epoch 6 | Batch 41/94 | Batch Loss: 4.0503\n",
      "Epoch 6 | Batch 42/94 | Batch Loss: 3.8915\n",
      "Epoch 6 | Batch 43/94 | Batch Loss: 4.0198\n",
      "Epoch 6 | Batch 44/94 | Batch Loss: 4.0250\n",
      "Epoch 6 | Batch 45/94 | Batch Loss: 3.9551\n",
      "Epoch 6 | Batch 46/94 | Batch Loss: 4.0063\n",
      "Epoch 6 | Batch 47/94 | Batch Loss: 3.9245\n",
      "Epoch 6 | Batch 48/94 | Batch Loss: 4.0134\n",
      "Epoch 6 | Batch 49/94 | Batch Loss: 4.0441\n",
      "Epoch 6 | Batch 50/94 | Batch Loss: 4.0310\n",
      "Epoch 6 | Batch 51/94 | Batch Loss: 4.0167\n",
      "Epoch 6 | Batch 52/94 | Batch Loss: 4.0193\n",
      "Epoch 6 | Batch 53/94 | Batch Loss: 4.0402\n",
      "Epoch 6 | Batch 54/94 | Batch Loss: 4.0118\n",
      "Epoch 6 | Batch 55/94 | Batch Loss: 4.0124\n",
      "Epoch 6 | Batch 56/94 | Batch Loss: 3.9271\n",
      "Epoch 6 | Batch 57/94 | Batch Loss: 3.9121\n",
      "Epoch 6 | Batch 58/94 | Batch Loss: 4.0707\n",
      "Epoch 6 | Batch 59/94 | Batch Loss: 3.9681\n",
      "Epoch 6 | Batch 60/94 | Batch Loss: 4.0140\n",
      "Epoch 6 | Batch 61/94 | Batch Loss: 4.0876\n",
      "Epoch 6 | Batch 62/94 | Batch Loss: 4.0027\n",
      "Epoch 6 | Batch 63/94 | Batch Loss: 4.0068\n",
      "Epoch 6 | Batch 64/94 | Batch Loss: 4.0131\n",
      "Epoch 6 | Batch 65/94 | Batch Loss: 4.0298\n",
      "Epoch 6 | Batch 66/94 | Batch Loss: 3.9384\n",
      "Epoch 6 | Batch 67/94 | Batch Loss: 4.0133\n",
      "Epoch 6 | Batch 68/94 | Batch Loss: 3.9795\n",
      "Epoch 6 | Batch 69/94 | Batch Loss: 3.8995\n",
      "Epoch 6 | Batch 70/94 | Batch Loss: 3.9458\n",
      "Epoch 6 | Batch 71/94 | Batch Loss: 3.9717\n",
      "Epoch 6 | Batch 72/94 | Batch Loss: 4.0364\n",
      "Epoch 6 | Batch 73/94 | Batch Loss: 4.0456\n",
      "Epoch 6 | Batch 74/94 | Batch Loss: 4.0463\n",
      "Epoch 6 | Batch 75/94 | Batch Loss: 3.8832\n",
      "Epoch 6 | Batch 76/94 | Batch Loss: 3.9948\n",
      "Epoch 6 | Batch 77/94 | Batch Loss: 4.0213\n",
      "Epoch 6 | Batch 78/94 | Batch Loss: 3.9183\n",
      "Epoch 6 | Batch 79/94 | Batch Loss: 3.8558\n",
      "Epoch 6 | Batch 80/94 | Batch Loss: 4.0049\n",
      "Epoch 6 | Batch 81/94 | Batch Loss: 3.9922\n",
      "Epoch 6 | Batch 82/94 | Batch Loss: 3.9196\n",
      "Epoch 6 | Batch 83/94 | Batch Loss: 3.9914\n",
      "Epoch 6 | Batch 84/94 | Batch Loss: 3.9774\n",
      "Epoch 6 | Batch 85/94 | Batch Loss: 4.0115\n",
      "Epoch 6 | Batch 86/94 | Batch Loss: 3.9372\n",
      "Epoch 6 | Batch 87/94 | Batch Loss: 3.9457\n",
      "Epoch 6 | Batch 88/94 | Batch Loss: 4.0310\n",
      "Epoch 6 | Batch 89/94 | Batch Loss: 3.9630\n",
      "Epoch 6 | Batch 90/94 | Batch Loss: 4.0544\n",
      "Epoch 6 | Batch 91/94 | Batch Loss: 3.9315\n",
      "Epoch 6 | Batch 92/94 | Batch Loss: 3.9953\n",
      "Epoch 6 | Batch 93/94 | Batch Loss: 4.0417\n",
      "Epoch 6 | Batch 94/94 | Batch Loss: 3.7125\n",
      "Epoch 6 Finished | Average Loss: 4.0044\n",
      "\n",
      "Epoch 7 Starting...\n",
      "Epoch 7 | Batch 1/94 | Batch Loss: 3.9502\n",
      "Epoch 7 | Batch 2/94 | Batch Loss: 3.8991\n",
      "Epoch 7 | Batch 3/94 | Batch Loss: 3.9416\n",
      "Epoch 7 | Batch 4/94 | Batch Loss: 3.9720\n",
      "Epoch 7 | Batch 5/94 | Batch Loss: 3.9212\n",
      "Epoch 7 | Batch 6/94 | Batch Loss: 3.9488\n",
      "Epoch 7 | Batch 7/94 | Batch Loss: 3.8530\n",
      "Epoch 7 | Batch 8/94 | Batch Loss: 4.0343\n",
      "Epoch 7 | Batch 9/94 | Batch Loss: 3.9096\n",
      "Epoch 7 | Batch 10/94 | Batch Loss: 3.9427\n",
      "Epoch 7 | Batch 11/94 | Batch Loss: 4.0316\n",
      "Epoch 7 | Batch 12/94 | Batch Loss: 3.9067\n",
      "Epoch 7 | Batch 13/94 | Batch Loss: 3.8761\n",
      "Epoch 7 | Batch 14/94 | Batch Loss: 3.9747\n",
      "Epoch 7 | Batch 15/94 | Batch Loss: 3.9897\n",
      "Epoch 7 | Batch 16/94 | Batch Loss: 3.8712\n",
      "Epoch 7 | Batch 17/94 | Batch Loss: 3.7971\n",
      "Epoch 7 | Batch 18/94 | Batch Loss: 3.9051\n",
      "Epoch 7 | Batch 19/94 | Batch Loss: 3.8917\n",
      "Epoch 7 | Batch 20/94 | Batch Loss: 3.9722\n",
      "Epoch 7 | Batch 21/94 | Batch Loss: 3.9174\n",
      "Epoch 7 | Batch 22/94 | Batch Loss: 3.9078\n",
      "Epoch 7 | Batch 23/94 | Batch Loss: 3.8521\n",
      "Epoch 7 | Batch 24/94 | Batch Loss: 3.8984\n",
      "Epoch 7 | Batch 25/94 | Batch Loss: 3.8928\n",
      "Epoch 7 | Batch 26/94 | Batch Loss: 3.8953\n",
      "Epoch 7 | Batch 27/94 | Batch Loss: 3.8636\n",
      "Epoch 7 | Batch 28/94 | Batch Loss: 3.8271\n",
      "Epoch 7 | Batch 29/94 | Batch Loss: 3.8683\n",
      "Epoch 7 | Batch 30/94 | Batch Loss: 3.8761\n",
      "Epoch 7 | Batch 31/94 | Batch Loss: 3.8517\n",
      "Epoch 7 | Batch 32/94 | Batch Loss: 3.8618\n",
      "Epoch 7 | Batch 33/94 | Batch Loss: 3.9459\n",
      "Epoch 7 | Batch 34/94 | Batch Loss: 3.8088\n",
      "Epoch 7 | Batch 35/94 | Batch Loss: 3.8356\n",
      "Epoch 7 | Batch 36/94 | Batch Loss: 3.9031\n",
      "Epoch 7 | Batch 37/94 | Batch Loss: 3.8697\n",
      "Epoch 7 | Batch 38/94 | Batch Loss: 4.0036\n",
      "Epoch 7 | Batch 39/94 | Batch Loss: 3.9718\n",
      "Epoch 7 | Batch 40/94 | Batch Loss: 4.0008\n",
      "Epoch 7 | Batch 41/94 | Batch Loss: 3.9288\n",
      "Epoch 7 | Batch 42/94 | Batch Loss: 3.9012\n",
      "Epoch 7 | Batch 43/94 | Batch Loss: 3.9213\n",
      "Epoch 7 | Batch 44/94 | Batch Loss: 3.8385\n",
      "Epoch 7 | Batch 45/94 | Batch Loss: 3.9199\n",
      "Epoch 7 | Batch 46/94 | Batch Loss: 3.8989\n",
      "Epoch 7 | Batch 47/94 | Batch Loss: 3.8514\n",
      "Epoch 7 | Batch 48/94 | Batch Loss: 3.8334\n",
      "Epoch 7 | Batch 49/94 | Batch Loss: 3.7970\n",
      "Epoch 7 | Batch 50/94 | Batch Loss: 3.9044\n",
      "Epoch 7 | Batch 51/94 | Batch Loss: 3.8073\n",
      "Epoch 7 | Batch 52/94 | Batch Loss: 3.8494\n",
      "Epoch 7 | Batch 53/94 | Batch Loss: 3.8144\n",
      "Epoch 7 | Batch 54/94 | Batch Loss: 3.8506\n",
      "Epoch 7 | Batch 55/94 | Batch Loss: 3.8954\n",
      "Epoch 7 | Batch 56/94 | Batch Loss: 3.8626\n",
      "Epoch 7 | Batch 57/94 | Batch Loss: 4.0158\n",
      "Epoch 7 | Batch 58/94 | Batch Loss: 3.9438\n",
      "Epoch 7 | Batch 59/94 | Batch Loss: 3.8669\n",
      "Epoch 7 | Batch 60/94 | Batch Loss: 3.9727\n",
      "Epoch 7 | Batch 61/94 | Batch Loss: 3.8268\n",
      "Epoch 7 | Batch 62/94 | Batch Loss: 3.9879\n",
      "Epoch 7 | Batch 63/94 | Batch Loss: 3.8950\n",
      "Epoch 7 | Batch 64/94 | Batch Loss: 3.8468\n",
      "Epoch 7 | Batch 65/94 | Batch Loss: 3.9110\n",
      "Epoch 7 | Batch 66/94 | Batch Loss: 3.9648\n",
      "Epoch 7 | Batch 67/94 | Batch Loss: 3.9228\n",
      "Epoch 7 | Batch 68/94 | Batch Loss: 3.8216\n",
      "Epoch 7 | Batch 69/94 | Batch Loss: 3.9763\n",
      "Epoch 7 | Batch 70/94 | Batch Loss: 3.8661\n",
      "Epoch 7 | Batch 71/94 | Batch Loss: 3.9154\n",
      "Epoch 7 | Batch 72/94 | Batch Loss: 3.8469\n",
      "Epoch 7 | Batch 73/94 | Batch Loss: 3.9309\n",
      "Epoch 7 | Batch 74/94 | Batch Loss: 3.8187\n",
      "Epoch 7 | Batch 75/94 | Batch Loss: 3.8676\n",
      "Epoch 7 | Batch 76/94 | Batch Loss: 3.9129\n",
      "Epoch 7 | Batch 77/94 | Batch Loss: 3.8386\n",
      "Epoch 7 | Batch 78/94 | Batch Loss: 3.8421\n",
      "Epoch 7 | Batch 79/94 | Batch Loss: 3.8285\n",
      "Epoch 7 | Batch 80/94 | Batch Loss: 3.8482\n",
      "Epoch 7 | Batch 81/94 | Batch Loss: 3.9003\n",
      "Epoch 7 | Batch 82/94 | Batch Loss: 3.8864\n",
      "Epoch 7 | Batch 83/94 | Batch Loss: 3.9039\n",
      "Epoch 7 | Batch 84/94 | Batch Loss: 3.8725\n",
      "Epoch 7 | Batch 85/94 | Batch Loss: 3.7742\n",
      "Epoch 7 | Batch 86/94 | Batch Loss: 3.8335\n",
      "Epoch 7 | Batch 87/94 | Batch Loss: 3.7866\n",
      "Epoch 7 | Batch 88/94 | Batch Loss: 3.8961\n",
      "Epoch 7 | Batch 89/94 | Batch Loss: 3.8943\n",
      "Epoch 7 | Batch 90/94 | Batch Loss: 3.8236\n",
      "Epoch 7 | Batch 91/94 | Batch Loss: 3.8706\n",
      "Epoch 7 | Batch 92/94 | Batch Loss: 3.8626\n",
      "Epoch 7 | Batch 93/94 | Batch Loss: 3.8792\n",
      "Epoch 7 | Batch 94/94 | Batch Loss: 3.8264\n",
      "Epoch 7 Finished | Average Loss: 3.8914\n",
      "\n",
      "Epoch 8 Starting...\n",
      "Epoch 8 | Batch 1/94 | Batch Loss: 3.9288\n",
      "Epoch 8 | Batch 2/94 | Batch Loss: 3.8433\n",
      "Epoch 8 | Batch 3/94 | Batch Loss: 3.7977\n",
      "Epoch 8 | Batch 4/94 | Batch Loss: 3.7686\n",
      "Epoch 8 | Batch 5/94 | Batch Loss: 3.8590\n",
      "Epoch 8 | Batch 6/94 | Batch Loss: 3.8636\n",
      "Epoch 8 | Batch 7/94 | Batch Loss: 3.8746\n",
      "Epoch 8 | Batch 8/94 | Batch Loss: 3.7576\n",
      "Epoch 8 | Batch 9/94 | Batch Loss: 3.8951\n",
      "Epoch 8 | Batch 10/94 | Batch Loss: 3.7036\n",
      "Epoch 8 | Batch 11/94 | Batch Loss: 3.8265\n",
      "Epoch 8 | Batch 12/94 | Batch Loss: 3.8233\n",
      "Epoch 8 | Batch 13/94 | Batch Loss: 3.7839\n",
      "Epoch 8 | Batch 14/94 | Batch Loss: 3.7679\n",
      "Epoch 8 | Batch 15/94 | Batch Loss: 3.7629\n",
      "Epoch 8 | Batch 16/94 | Batch Loss: 3.8252\n",
      "Epoch 8 | Batch 17/94 | Batch Loss: 3.8102\n",
      "Epoch 8 | Batch 18/94 | Batch Loss: 3.7811\n",
      "Epoch 8 | Batch 19/94 | Batch Loss: 3.8434\n",
      "Epoch 8 | Batch 20/94 | Batch Loss: 3.8102\n",
      "Epoch 8 | Batch 21/94 | Batch Loss: 3.8521\n",
      "Epoch 8 | Batch 22/94 | Batch Loss: 3.8207\n",
      "Epoch 8 | Batch 23/94 | Batch Loss: 3.7839\n",
      "Epoch 8 | Batch 24/94 | Batch Loss: 3.8317\n",
      "Epoch 8 | Batch 25/94 | Batch Loss: 3.8443\n",
      "Epoch 8 | Batch 26/94 | Batch Loss: 3.7773\n",
      "Epoch 8 | Batch 27/94 | Batch Loss: 3.7237\n",
      "Epoch 8 | Batch 28/94 | Batch Loss: 3.7615\n",
      "Epoch 8 | Batch 29/94 | Batch Loss: 3.7644\n",
      "Epoch 8 | Batch 30/94 | Batch Loss: 3.7525\n",
      "Epoch 8 | Batch 31/94 | Batch Loss: 3.7296\n",
      "Epoch 8 | Batch 32/94 | Batch Loss: 3.8173\n",
      "Epoch 8 | Batch 33/94 | Batch Loss: 3.8459\n",
      "Epoch 8 | Batch 34/94 | Batch Loss: 3.8228\n",
      "Epoch 8 | Batch 35/94 | Batch Loss: 3.8056\n",
      "Epoch 8 | Batch 36/94 | Batch Loss: 3.8828\n",
      "Epoch 8 | Batch 37/94 | Batch Loss: 3.7964\n",
      "Epoch 8 | Batch 38/94 | Batch Loss: 3.8979\n",
      "Epoch 8 | Batch 39/94 | Batch Loss: 3.8010\n",
      "Epoch 8 | Batch 40/94 | Batch Loss: 3.8481\n",
      "Epoch 8 | Batch 41/94 | Batch Loss: 3.7817\n",
      "Epoch 8 | Batch 42/94 | Batch Loss: 3.7720\n",
      "Epoch 8 | Batch 43/94 | Batch Loss: 3.8204\n",
      "Epoch 8 | Batch 44/94 | Batch Loss: 3.8225\n",
      "Epoch 8 | Batch 45/94 | Batch Loss: 3.7932\n",
      "Epoch 8 | Batch 46/94 | Batch Loss: 3.8201\n",
      "Epoch 8 | Batch 47/94 | Batch Loss: 3.7001\n",
      "Epoch 8 | Batch 48/94 | Batch Loss: 3.7581\n",
      "Epoch 8 | Batch 49/94 | Batch Loss: 3.8222\n",
      "Epoch 8 | Batch 50/94 | Batch Loss: 3.9266\n",
      "Epoch 8 | Batch 51/94 | Batch Loss: 3.6889\n",
      "Epoch 8 | Batch 52/94 | Batch Loss: 3.7559\n",
      "Epoch 8 | Batch 53/94 | Batch Loss: 3.7483\n",
      "Epoch 8 | Batch 54/94 | Batch Loss: 3.7747\n",
      "Epoch 8 | Batch 55/94 | Batch Loss: 3.8392\n",
      "Epoch 8 | Batch 56/94 | Batch Loss: 3.7560\n",
      "Epoch 8 | Batch 57/94 | Batch Loss: 3.9083\n",
      "Epoch 8 | Batch 58/94 | Batch Loss: 3.8605\n",
      "Epoch 8 | Batch 59/94 | Batch Loss: 3.7430\n",
      "Epoch 8 | Batch 60/94 | Batch Loss: 3.7927\n",
      "Epoch 8 | Batch 61/94 | Batch Loss: 3.7943\n",
      "Epoch 8 | Batch 62/94 | Batch Loss: 3.8547\n",
      "Epoch 8 | Batch 63/94 | Batch Loss: 3.7600\n",
      "Epoch 8 | Batch 64/94 | Batch Loss: 3.7370\n",
      "Epoch 8 | Batch 65/94 | Batch Loss: 3.8195\n",
      "Epoch 8 | Batch 66/94 | Batch Loss: 3.7356\n",
      "Epoch 8 | Batch 67/94 | Batch Loss: 3.8402\n",
      "Epoch 8 | Batch 68/94 | Batch Loss: 3.7711\n",
      "Epoch 8 | Batch 69/94 | Batch Loss: 3.8449\n",
      "Epoch 8 | Batch 70/94 | Batch Loss: 3.7393\n",
      "Epoch 8 | Batch 71/94 | Batch Loss: 3.8690\n",
      "Epoch 8 | Batch 72/94 | Batch Loss: 3.8329\n",
      "Epoch 8 | Batch 73/94 | Batch Loss: 3.7006\n",
      "Epoch 8 | Batch 74/94 | Batch Loss: 3.8443\n",
      "Epoch 8 | Batch 75/94 | Batch Loss: 3.7788\n",
      "Epoch 8 | Batch 76/94 | Batch Loss: 3.7498\n",
      "Epoch 8 | Batch 77/94 | Batch Loss: 3.8306\n",
      "Epoch 8 | Batch 78/94 | Batch Loss: 3.8389\n",
      "Epoch 8 | Batch 79/94 | Batch Loss: 3.7169\n",
      "Epoch 8 | Batch 80/94 | Batch Loss: 3.7680\n",
      "Epoch 8 | Batch 81/94 | Batch Loss: 3.6486\n",
      "Epoch 8 | Batch 82/94 | Batch Loss: 3.7623\n",
      "Epoch 8 | Batch 83/94 | Batch Loss: 3.7709\n",
      "Epoch 8 | Batch 84/94 | Batch Loss: 3.6847\n",
      "Epoch 8 | Batch 85/94 | Batch Loss: 3.8386\n",
      "Epoch 8 | Batch 86/94 | Batch Loss: 3.7552\n",
      "Epoch 8 | Batch 87/94 | Batch Loss: 3.7794\n",
      "Epoch 8 | Batch 88/94 | Batch Loss: 3.8362\n",
      "Epoch 8 | Batch 89/94 | Batch Loss: 3.5289\n",
      "Epoch 8 | Batch 90/94 | Batch Loss: 3.8145\n",
      "Epoch 8 | Batch 91/94 | Batch Loss: 3.8275\n",
      "Epoch 8 | Batch 92/94 | Batch Loss: 3.8208\n",
      "Epoch 8 | Batch 93/94 | Batch Loss: 3.7823\n",
      "Epoch 8 | Batch 94/94 | Batch Loss: 3.7365\n",
      "Epoch 8 Finished | Average Loss: 3.7956\n",
      "\n",
      "Epoch 9 Starting...\n",
      "Epoch 9 | Batch 1/94 | Batch Loss: 3.8407\n",
      "Epoch 9 | Batch 2/94 | Batch Loss: 3.7155\n",
      "Epoch 9 | Batch 3/94 | Batch Loss: 3.6602\n",
      "Epoch 9 | Batch 4/94 | Batch Loss: 3.5934\n",
      "Epoch 9 | Batch 5/94 | Batch Loss: 3.7710\n",
      "Epoch 9 | Batch 6/94 | Batch Loss: 3.5709\n",
      "Epoch 9 | Batch 7/94 | Batch Loss: 3.7160\n",
      "Epoch 9 | Batch 8/94 | Batch Loss: 3.7549\n",
      "Epoch 9 | Batch 9/94 | Batch Loss: 3.7099\n",
      "Epoch 9 | Batch 10/94 | Batch Loss: 3.7389\n",
      "Epoch 9 | Batch 11/94 | Batch Loss: 3.6042\n",
      "Epoch 9 | Batch 12/94 | Batch Loss: 3.7020\n",
      "Epoch 9 | Batch 13/94 | Batch Loss: 3.7033\n",
      "Epoch 9 | Batch 14/94 | Batch Loss: 3.7912\n",
      "Epoch 9 | Batch 15/94 | Batch Loss: 3.7254\n",
      "Epoch 9 | Batch 16/94 | Batch Loss: 3.7216\n",
      "Epoch 9 | Batch 17/94 | Batch Loss: 3.6595\n",
      "Epoch 9 | Batch 18/94 | Batch Loss: 3.7950\n",
      "Epoch 9 | Batch 19/94 | Batch Loss: 3.7118\n",
      "Epoch 9 | Batch 20/94 | Batch Loss: 3.7652\n",
      "Epoch 9 | Batch 21/94 | Batch Loss: 3.7562\n",
      "Epoch 9 | Batch 22/94 | Batch Loss: 3.6538\n",
      "Epoch 9 | Batch 23/94 | Batch Loss: 3.7255\n",
      "Epoch 9 | Batch 24/94 | Batch Loss: 3.7347\n",
      "Epoch 9 | Batch 25/94 | Batch Loss: 3.7762\n",
      "Epoch 9 | Batch 26/94 | Batch Loss: 3.6317\n",
      "Epoch 9 | Batch 27/94 | Batch Loss: 3.6935\n",
      "Epoch 9 | Batch 28/94 | Batch Loss: 3.7079\n",
      "Epoch 9 | Batch 29/94 | Batch Loss: 3.7461\n",
      "Epoch 9 | Batch 30/94 | Batch Loss: 3.6829\n",
      "Epoch 9 | Batch 31/94 | Batch Loss: 3.7760\n",
      "Epoch 9 | Batch 32/94 | Batch Loss: 3.6482\n",
      "Epoch 9 | Batch 33/94 | Batch Loss: 3.7583\n",
      "Epoch 9 | Batch 34/94 | Batch Loss: 3.6642\n",
      "Epoch 9 | Batch 35/94 | Batch Loss: 3.7638\n",
      "Epoch 9 | Batch 36/94 | Batch Loss: 3.7020\n",
      "Epoch 9 | Batch 37/94 | Batch Loss: 3.7921\n",
      "Epoch 9 | Batch 38/94 | Batch Loss: 3.7473\n",
      "Epoch 9 | Batch 39/94 | Batch Loss: 3.8182\n",
      "Epoch 9 | Batch 40/94 | Batch Loss: 3.6554\n",
      "Epoch 9 | Batch 41/94 | Batch Loss: 3.6928\n",
      "Epoch 9 | Batch 42/94 | Batch Loss: 3.7049\n",
      "Epoch 9 | Batch 43/94 | Batch Loss: 3.7516\n",
      "Epoch 9 | Batch 44/94 | Batch Loss: 3.7592\n",
      "Epoch 9 | Batch 45/94 | Batch Loss: 3.7278\n",
      "Epoch 9 | Batch 46/94 | Batch Loss: 3.6086\n",
      "Epoch 9 | Batch 47/94 | Batch Loss: 3.7407\n",
      "Epoch 9 | Batch 48/94 | Batch Loss: 3.7572\n",
      "Epoch 9 | Batch 49/94 | Batch Loss: 3.7220\n",
      "Epoch 9 | Batch 50/94 | Batch Loss: 3.7715\n",
      "Epoch 9 | Batch 51/94 | Batch Loss: 3.7098\n",
      "Epoch 9 | Batch 52/94 | Batch Loss: 3.7098\n",
      "Epoch 9 | Batch 53/94 | Batch Loss: 3.6093\n",
      "Epoch 9 | Batch 54/94 | Batch Loss: 3.6661\n",
      "Epoch 9 | Batch 55/94 | Batch Loss: 3.6994\n",
      "Epoch 9 | Batch 56/94 | Batch Loss: 3.6760\n",
      "Epoch 9 | Batch 57/94 | Batch Loss: 3.6667\n",
      "Epoch 9 | Batch 58/94 | Batch Loss: 3.7074\n",
      "Epoch 9 | Batch 59/94 | Batch Loss: 3.8152\n",
      "Epoch 9 | Batch 60/94 | Batch Loss: 3.6783\n",
      "Epoch 9 | Batch 61/94 | Batch Loss: 3.7352\n",
      "Epoch 9 | Batch 62/94 | Batch Loss: 3.7544\n",
      "Epoch 9 | Batch 63/94 | Batch Loss: 3.7278\n",
      "Epoch 9 | Batch 64/94 | Batch Loss: 3.7346\n",
      "Epoch 9 | Batch 65/94 | Batch Loss: 3.6154\n",
      "Epoch 9 | Batch 66/94 | Batch Loss: 3.8043\n",
      "Epoch 9 | Batch 67/94 | Batch Loss: 3.7845\n",
      "Epoch 9 | Batch 68/94 | Batch Loss: 3.7236\n",
      "Epoch 9 | Batch 69/94 | Batch Loss: 3.7156\n",
      "Epoch 9 | Batch 70/94 | Batch Loss: 3.7387\n",
      "Epoch 9 | Batch 71/94 | Batch Loss: 3.8528\n",
      "Epoch 9 | Batch 72/94 | Batch Loss: 3.6578\n",
      "Epoch 9 | Batch 73/94 | Batch Loss: 3.6681\n",
      "Epoch 9 | Batch 74/94 | Batch Loss: 3.7131\n",
      "Epoch 9 | Batch 75/94 | Batch Loss: 3.7050\n",
      "Epoch 9 | Batch 76/94 | Batch Loss: 3.6603\n",
      "Epoch 9 | Batch 77/94 | Batch Loss: 3.6875\n",
      "Epoch 9 | Batch 78/94 | Batch Loss: 3.7441\n",
      "Epoch 9 | Batch 79/94 | Batch Loss: 3.7101\n",
      "Epoch 9 | Batch 80/94 | Batch Loss: 3.7294\n",
      "Epoch 9 | Batch 81/94 | Batch Loss: 3.6904\n",
      "Epoch 9 | Batch 82/94 | Batch Loss: 3.6191\n",
      "Epoch 9 | Batch 83/94 | Batch Loss: 3.6618\n",
      "Epoch 9 | Batch 84/94 | Batch Loss: 3.6456\n",
      "Epoch 9 | Batch 85/94 | Batch Loss: 3.6629\n",
      "Epoch 9 | Batch 86/94 | Batch Loss: 3.7060\n",
      "Epoch 9 | Batch 87/94 | Batch Loss: 3.6309\n",
      "Epoch 9 | Batch 88/94 | Batch Loss: 3.7387\n",
      "Epoch 9 | Batch 89/94 | Batch Loss: 3.6888\n",
      "Epoch 9 | Batch 90/94 | Batch Loss: 3.5975\n",
      "Epoch 9 | Batch 91/94 | Batch Loss: 3.6426\n",
      "Epoch 9 | Batch 92/94 | Batch Loss: 3.6787\n",
      "Epoch 9 | Batch 93/94 | Batch Loss: 3.6352\n",
      "Epoch 9 | Batch 94/94 | Batch Loss: 3.7367\n",
      "Epoch 9 Finished | Average Loss: 3.7091\n",
      "\n",
      "Epoch 10 Starting...\n",
      "Epoch 10 | Batch 1/94 | Batch Loss: 3.5865\n",
      "Epoch 10 | Batch 2/94 | Batch Loss: 3.6409\n",
      "Epoch 10 | Batch 3/94 | Batch Loss: 3.6594\n",
      "Epoch 10 | Batch 4/94 | Batch Loss: 3.7371\n",
      "Epoch 10 | Batch 5/94 | Batch Loss: 3.6505\n",
      "Epoch 10 | Batch 6/94 | Batch Loss: 3.7087\n",
      "Epoch 10 | Batch 7/94 | Batch Loss: 3.5661\n",
      "Epoch 10 | Batch 8/94 | Batch Loss: 3.6978\n",
      "Epoch 10 | Batch 9/94 | Batch Loss: 3.7181\n",
      "Epoch 10 | Batch 10/94 | Batch Loss: 3.7213\n",
      "Epoch 10 | Batch 11/94 | Batch Loss: 3.6354\n",
      "Epoch 10 | Batch 12/94 | Batch Loss: 3.5858\n",
      "Epoch 10 | Batch 13/94 | Batch Loss: 3.6142\n",
      "Epoch 10 | Batch 14/94 | Batch Loss: 3.5680\n",
      "Epoch 10 | Batch 15/94 | Batch Loss: 3.5263\n",
      "Epoch 10 | Batch 16/94 | Batch Loss: 3.6634\n",
      "Epoch 10 | Batch 17/94 | Batch Loss: 3.7328\n",
      "Epoch 10 | Batch 18/94 | Batch Loss: 3.7134\n",
      "Epoch 10 | Batch 19/94 | Batch Loss: 3.6256\n",
      "Epoch 10 | Batch 20/94 | Batch Loss: 3.6884\n",
      "Epoch 10 | Batch 21/94 | Batch Loss: 3.6161\n",
      "Epoch 10 | Batch 22/94 | Batch Loss: 3.5718\n",
      "Epoch 10 | Batch 23/94 | Batch Loss: 3.6280\n",
      "Epoch 10 | Batch 24/94 | Batch Loss: 3.6998\n",
      "Epoch 10 | Batch 25/94 | Batch Loss: 3.5910\n",
      "Epoch 10 | Batch 26/94 | Batch Loss: 3.7665\n",
      "Epoch 10 | Batch 27/94 | Batch Loss: 3.6701\n",
      "Epoch 10 | Batch 28/94 | Batch Loss: 3.6791\n",
      "Epoch 10 | Batch 29/94 | Batch Loss: 3.6571\n",
      "Epoch 10 | Batch 30/94 | Batch Loss: 3.6158\n",
      "Epoch 10 | Batch 31/94 | Batch Loss: 3.6806\n",
      "Epoch 10 | Batch 32/94 | Batch Loss: 3.6713\n",
      "Epoch 10 | Batch 33/94 | Batch Loss: 3.5555\n",
      "Epoch 10 | Batch 34/94 | Batch Loss: 3.6908\n",
      "Epoch 10 | Batch 35/94 | Batch Loss: 3.6332\n",
      "Epoch 10 | Batch 36/94 | Batch Loss: 3.5917\n",
      "Epoch 10 | Batch 37/94 | Batch Loss: 3.6246\n",
      "Epoch 10 | Batch 38/94 | Batch Loss: 3.7266\n",
      "Epoch 10 | Batch 39/94 | Batch Loss: 3.5997\n",
      "Epoch 10 | Batch 40/94 | Batch Loss: 3.6346\n",
      "Epoch 10 | Batch 41/94 | Batch Loss: 3.5541\n",
      "Epoch 10 | Batch 42/94 | Batch Loss: 3.6628\n",
      "Epoch 10 | Batch 43/94 | Batch Loss: 3.5049\n",
      "Epoch 10 | Batch 44/94 | Batch Loss: 3.6789\n",
      "Epoch 10 | Batch 45/94 | Batch Loss: 3.6196\n",
      "Epoch 10 | Batch 46/94 | Batch Loss: 3.7141\n",
      "Epoch 10 | Batch 47/94 | Batch Loss: 3.6185\n",
      "Epoch 10 | Batch 48/94 | Batch Loss: 3.6000\n",
      "Epoch 10 | Batch 49/94 | Batch Loss: 3.7406\n",
      "Epoch 10 | Batch 50/94 | Batch Loss: 3.7089\n",
      "Epoch 10 | Batch 51/94 | Batch Loss: 3.6279\n",
      "Epoch 10 | Batch 52/94 | Batch Loss: 3.8066\n",
      "Epoch 10 | Batch 53/94 | Batch Loss: 3.6455\n",
      "Epoch 10 | Batch 54/94 | Batch Loss: 3.5589\n",
      "Epoch 10 | Batch 55/94 | Batch Loss: 3.6476\n",
      "Epoch 10 | Batch 56/94 | Batch Loss: 3.6090\n",
      "Epoch 10 | Batch 57/94 | Batch Loss: 3.5985\n",
      "Epoch 10 | Batch 58/94 | Batch Loss: 3.6541\n",
      "Epoch 10 | Batch 59/94 | Batch Loss: 3.6725\n",
      "Epoch 10 | Batch 60/94 | Batch Loss: 3.6320\n",
      "Epoch 10 | Batch 61/94 | Batch Loss: 3.5978\n",
      "Epoch 10 | Batch 62/94 | Batch Loss: 3.6815\n",
      "Epoch 10 | Batch 63/94 | Batch Loss: 3.6095\n",
      "Epoch 10 | Batch 64/94 | Batch Loss: 3.6735\n",
      "Epoch 10 | Batch 65/94 | Batch Loss: 3.6303\n",
      "Epoch 10 | Batch 66/94 | Batch Loss: 3.6400\n",
      "Epoch 10 | Batch 67/94 | Batch Loss: 3.5721\n",
      "Epoch 10 | Batch 68/94 | Batch Loss: 3.7019\n",
      "Epoch 10 | Batch 69/94 | Batch Loss: 3.5614\n",
      "Epoch 10 | Batch 70/94 | Batch Loss: 3.7993\n",
      "Epoch 10 | Batch 71/94 | Batch Loss: 3.5953\n",
      "Epoch 10 | Batch 72/94 | Batch Loss: 3.6717\n",
      "Epoch 10 | Batch 73/94 | Batch Loss: 3.6131\n",
      "Epoch 10 | Batch 74/94 | Batch Loss: 3.5946\n",
      "Epoch 10 | Batch 75/94 | Batch Loss: 3.5495\n",
      "Epoch 10 | Batch 76/94 | Batch Loss: 3.5658\n",
      "Epoch 10 | Batch 77/94 | Batch Loss: 3.6626\n",
      "Epoch 10 | Batch 78/94 | Batch Loss: 3.5997\n",
      "Epoch 10 | Batch 79/94 | Batch Loss: 3.6229\n",
      "Epoch 10 | Batch 80/94 | Batch Loss: 3.6516\n",
      "Epoch 10 | Batch 81/94 | Batch Loss: 3.6347\n",
      "Epoch 10 | Batch 82/94 | Batch Loss: 3.6421\n",
      "Epoch 10 | Batch 83/94 | Batch Loss: 3.6511\n",
      "Epoch 10 | Batch 84/94 | Batch Loss: 3.6662\n",
      "Epoch 10 | Batch 85/94 | Batch Loss: 3.7075\n",
      "Epoch 10 | Batch 86/94 | Batch Loss: 3.5840\n",
      "Epoch 10 | Batch 87/94 | Batch Loss: 3.6253\n",
      "Epoch 10 | Batch 88/94 | Batch Loss: 3.5437\n",
      "Epoch 10 | Batch 89/94 | Batch Loss: 3.6514\n",
      "Epoch 10 | Batch 90/94 | Batch Loss: 3.4435\n",
      "Epoch 10 | Batch 91/94 | Batch Loss: 3.6037\n",
      "Epoch 10 | Batch 92/94 | Batch Loss: 3.6229\n",
      "Epoch 10 | Batch 93/94 | Batch Loss: 3.4987\n",
      "Epoch 10 | Batch 94/94 | Batch Loss: 3.5929\n",
      "Epoch 10 Finished | Average Loss: 3.6368\n",
      "\n",
      "Epoch 11 Starting...\n",
      "Epoch 11 | Batch 1/94 | Batch Loss: 3.6482\n",
      "Epoch 11 | Batch 2/94 | Batch Loss: 3.5550\n",
      "Epoch 11 | Batch 3/94 | Batch Loss: 3.6230\n",
      "Epoch 11 | Batch 4/94 | Batch Loss: 3.5346\n",
      "Epoch 11 | Batch 5/94 | Batch Loss: 3.5768\n",
      "Epoch 11 | Batch 6/94 | Batch Loss: 3.5337\n",
      "Epoch 11 | Batch 7/94 | Batch Loss: 3.6774\n",
      "Epoch 11 | Batch 8/94 | Batch Loss: 3.5500\n",
      "Epoch 11 | Batch 9/94 | Batch Loss: 3.5857\n",
      "Epoch 11 | Batch 10/94 | Batch Loss: 3.6195\n",
      "Epoch 11 | Batch 11/94 | Batch Loss: 3.5694\n",
      "Epoch 11 | Batch 12/94 | Batch Loss: 3.6080\n",
      "Epoch 11 | Batch 13/94 | Batch Loss: 3.4534\n",
      "Epoch 11 | Batch 14/94 | Batch Loss: 3.6285\n",
      "Epoch 11 | Batch 15/94 | Batch Loss: 3.6481\n",
      "Epoch 11 | Batch 16/94 | Batch Loss: 3.5439\n",
      "Epoch 11 | Batch 17/94 | Batch Loss: 3.5690\n",
      "Epoch 11 | Batch 18/94 | Batch Loss: 3.5998\n",
      "Epoch 11 | Batch 19/94 | Batch Loss: 3.5586\n",
      "Epoch 11 | Batch 20/94 | Batch Loss: 3.6058\n",
      "Epoch 11 | Batch 21/94 | Batch Loss: 3.5924\n",
      "Epoch 11 | Batch 22/94 | Batch Loss: 3.5608\n",
      "Epoch 11 | Batch 23/94 | Batch Loss: 3.5551\n",
      "Epoch 11 | Batch 24/94 | Batch Loss: 3.6469\n",
      "Epoch 11 | Batch 25/94 | Batch Loss: 3.4832\n",
      "Epoch 11 | Batch 26/94 | Batch Loss: 3.5524\n",
      "Epoch 11 | Batch 27/94 | Batch Loss: 3.6496\n",
      "Epoch 11 | Batch 28/94 | Batch Loss: 3.6158\n",
      "Epoch 11 | Batch 29/94 | Batch Loss: 3.5786\n",
      "Epoch 11 | Batch 30/94 | Batch Loss: 3.6145\n",
      "Epoch 11 | Batch 31/94 | Batch Loss: 3.5017\n",
      "Epoch 11 | Batch 32/94 | Batch Loss: 3.6453\n",
      "Epoch 11 | Batch 33/94 | Batch Loss: 3.5686\n",
      "Epoch 11 | Batch 34/94 | Batch Loss: 3.5474\n",
      "Epoch 11 | Batch 35/94 | Batch Loss: 3.4663\n",
      "Epoch 11 | Batch 36/94 | Batch Loss: 3.6574\n",
      "Epoch 11 | Batch 37/94 | Batch Loss: 3.6797\n",
      "Epoch 11 | Batch 38/94 | Batch Loss: 3.5825\n",
      "Epoch 11 | Batch 39/94 | Batch Loss: 3.5452\n",
      "Epoch 11 | Batch 40/94 | Batch Loss: 3.5410\n",
      "Epoch 11 | Batch 41/94 | Batch Loss: 3.6098\n",
      "Epoch 11 | Batch 42/94 | Batch Loss: 3.4546\n",
      "Epoch 11 | Batch 43/94 | Batch Loss: 3.4346\n",
      "Epoch 11 | Batch 44/94 | Batch Loss: 3.5586\n",
      "Epoch 11 | Batch 45/94 | Batch Loss: 3.6312\n",
      "Epoch 11 | Batch 46/94 | Batch Loss: 3.5678\n",
      "Epoch 11 | Batch 47/94 | Batch Loss: 3.5414\n",
      "Epoch 11 | Batch 48/94 | Batch Loss: 3.7032\n",
      "Epoch 11 | Batch 49/94 | Batch Loss: 3.6074\n",
      "Epoch 11 | Batch 50/94 | Batch Loss: 3.5146\n",
      "Epoch 11 | Batch 51/94 | Batch Loss: 3.6275\n",
      "Epoch 11 | Batch 52/94 | Batch Loss: 3.4815\n",
      "Epoch 11 | Batch 53/94 | Batch Loss: 3.6864\n",
      "Epoch 11 | Batch 54/94 | Batch Loss: 3.5909\n",
      "Epoch 11 | Batch 55/94 | Batch Loss: 3.5557\n",
      "Epoch 11 | Batch 56/94 | Batch Loss: 3.5239\n",
      "Epoch 11 | Batch 57/94 | Batch Loss: 3.5833\n",
      "Epoch 11 | Batch 58/94 | Batch Loss: 3.5269\n",
      "Epoch 11 | Batch 59/94 | Batch Loss: 3.5820\n",
      "Epoch 11 | Batch 60/94 | Batch Loss: 3.5516\n",
      "Epoch 11 | Batch 61/94 | Batch Loss: 3.6056\n",
      "Epoch 11 | Batch 62/94 | Batch Loss: 3.5458\n",
      "Epoch 11 | Batch 63/94 | Batch Loss: 3.5941\n",
      "Epoch 11 | Batch 64/94 | Batch Loss: 3.5136\n",
      "Epoch 11 | Batch 65/94 | Batch Loss: 3.5115\n",
      "Epoch 11 | Batch 66/94 | Batch Loss: 3.5990\n",
      "Epoch 11 | Batch 67/94 | Batch Loss: 3.5572\n",
      "Epoch 11 | Batch 68/94 | Batch Loss: 3.4764\n",
      "Epoch 11 | Batch 69/94 | Batch Loss: 3.5300\n",
      "Epoch 11 | Batch 70/94 | Batch Loss: 3.5179\n",
      "Epoch 11 | Batch 71/94 | Batch Loss: 3.6281\n",
      "Epoch 11 | Batch 72/94 | Batch Loss: 3.6452\n",
      "Epoch 11 | Batch 73/94 | Batch Loss: 3.6104\n",
      "Epoch 11 | Batch 74/94 | Batch Loss: 3.5900\n",
      "Epoch 11 | Batch 75/94 | Batch Loss: 3.5897\n",
      "Epoch 11 | Batch 76/94 | Batch Loss: 3.5342\n",
      "Epoch 11 | Batch 77/94 | Batch Loss: 3.5841\n",
      "Epoch 11 | Batch 78/94 | Batch Loss: 3.5223\n",
      "Epoch 11 | Batch 79/94 | Batch Loss: 3.6054\n",
      "Epoch 11 | Batch 80/94 | Batch Loss: 3.5815\n",
      "Epoch 11 | Batch 81/94 | Batch Loss: 3.5709\n",
      "Epoch 11 | Batch 82/94 | Batch Loss: 3.5127\n",
      "Epoch 11 | Batch 83/94 | Batch Loss: 3.5397\n",
      "Epoch 11 | Batch 84/94 | Batch Loss: 3.5250\n",
      "Epoch 11 | Batch 85/94 | Batch Loss: 3.4720\n",
      "Epoch 11 | Batch 86/94 | Batch Loss: 3.6325\n",
      "Epoch 11 | Batch 87/94 | Batch Loss: 3.3960\n",
      "Epoch 11 | Batch 88/94 | Batch Loss: 3.5313\n",
      "Epoch 11 | Batch 89/94 | Batch Loss: 3.5037\n",
      "Epoch 11 | Batch 90/94 | Batch Loss: 3.6244\n",
      "Epoch 11 | Batch 91/94 | Batch Loss: 3.5682\n",
      "Epoch 11 | Batch 92/94 | Batch Loss: 3.5231\n",
      "Epoch 11 | Batch 93/94 | Batch Loss: 3.6154\n",
      "Epoch 11 | Batch 94/94 | Batch Loss: 3.6729\n",
      "Epoch 11 Finished | Average Loss: 3.5706\n",
      "\n",
      "Epoch 12 Starting...\n",
      "Epoch 12 | Batch 1/94 | Batch Loss: 3.5987\n",
      "Epoch 12 | Batch 2/94 | Batch Loss: 3.5665\n",
      "Epoch 12 | Batch 3/94 | Batch Loss: 3.5344\n",
      "Epoch 12 | Batch 4/94 | Batch Loss: 3.6206\n",
      "Epoch 12 | Batch 5/94 | Batch Loss: 3.5098\n",
      "Epoch 12 | Batch 6/94 | Batch Loss: 3.5649\n",
      "Epoch 12 | Batch 7/94 | Batch Loss: 3.5481\n",
      "Epoch 12 | Batch 8/94 | Batch Loss: 3.5174\n",
      "Epoch 12 | Batch 9/94 | Batch Loss: 3.4657\n",
      "Epoch 12 | Batch 10/94 | Batch Loss: 3.5020\n",
      "Epoch 12 | Batch 11/94 | Batch Loss: 3.5077\n",
      "Epoch 12 | Batch 12/94 | Batch Loss: 3.5710\n",
      "Epoch 12 | Batch 13/94 | Batch Loss: 3.4784\n",
      "Epoch 12 | Batch 14/94 | Batch Loss: 3.5552\n",
      "Epoch 12 | Batch 15/94 | Batch Loss: 3.5338\n",
      "Epoch 12 | Batch 16/94 | Batch Loss: 3.5873\n",
      "Epoch 12 | Batch 17/94 | Batch Loss: 3.5847\n",
      "Epoch 12 | Batch 18/94 | Batch Loss: 3.5244\n",
      "Epoch 12 | Batch 19/94 | Batch Loss: 3.5651\n",
      "Epoch 12 | Batch 20/94 | Batch Loss: 3.5588\n",
      "Epoch 12 | Batch 21/94 | Batch Loss: 3.5169\n",
      "Epoch 12 | Batch 22/94 | Batch Loss: 3.4973\n",
      "Epoch 12 | Batch 23/94 | Batch Loss: 3.3919\n",
      "Epoch 12 | Batch 24/94 | Batch Loss: 3.5483\n",
      "Epoch 12 | Batch 25/94 | Batch Loss: 3.5555\n",
      "Epoch 12 | Batch 26/94 | Batch Loss: 3.4829\n",
      "Epoch 12 | Batch 27/94 | Batch Loss: 3.4838\n",
      "Epoch 12 | Batch 28/94 | Batch Loss: 3.5070\n",
      "Epoch 12 | Batch 29/94 | Batch Loss: 3.4802\n",
      "Epoch 12 | Batch 30/94 | Batch Loss: 3.5770\n",
      "Epoch 12 | Batch 31/94 | Batch Loss: 3.4412\n",
      "Epoch 12 | Batch 32/94 | Batch Loss: 3.4743\n",
      "Epoch 12 | Batch 33/94 | Batch Loss: 3.4633\n",
      "Epoch 12 | Batch 34/94 | Batch Loss: 3.4454\n",
      "Epoch 12 | Batch 35/94 | Batch Loss: 3.5921\n",
      "Epoch 12 | Batch 36/94 | Batch Loss: 3.5252\n",
      "Epoch 12 | Batch 37/94 | Batch Loss: 3.5038\n",
      "Epoch 12 | Batch 38/94 | Batch Loss: 3.5050\n",
      "Epoch 12 | Batch 39/94 | Batch Loss: 3.5803\n",
      "Epoch 12 | Batch 40/94 | Batch Loss: 3.4787\n",
      "Epoch 12 | Batch 41/94 | Batch Loss: 3.5060\n",
      "Epoch 12 | Batch 42/94 | Batch Loss: 3.4704\n",
      "Epoch 12 | Batch 43/94 | Batch Loss: 3.5523\n",
      "Epoch 12 | Batch 44/94 | Batch Loss: 3.5034\n",
      "Epoch 12 | Batch 45/94 | Batch Loss: 3.5684\n",
      "Epoch 12 | Batch 46/94 | Batch Loss: 3.4716\n",
      "Epoch 12 | Batch 47/94 | Batch Loss: 3.5009\n",
      "Epoch 12 | Batch 48/94 | Batch Loss: 3.4919\n",
      "Epoch 12 | Batch 49/94 | Batch Loss: 3.5540\n",
      "Epoch 12 | Batch 50/94 | Batch Loss: 3.3905\n",
      "Epoch 12 | Batch 51/94 | Batch Loss: 3.4257\n",
      "Epoch 12 | Batch 52/94 | Batch Loss: 3.5299\n",
      "Epoch 12 | Batch 53/94 | Batch Loss: 3.4664\n",
      "Epoch 12 | Batch 54/94 | Batch Loss: 3.4799\n",
      "Epoch 12 | Batch 55/94 | Batch Loss: 3.5532\n",
      "Epoch 12 | Batch 56/94 | Batch Loss: 3.4065\n",
      "Epoch 12 | Batch 57/94 | Batch Loss: 3.5701\n",
      "Epoch 12 | Batch 58/94 | Batch Loss: 3.4635\n",
      "Epoch 12 | Batch 59/94 | Batch Loss: 3.4481\n",
      "Epoch 12 | Batch 60/94 | Batch Loss: 3.5177\n",
      "Epoch 12 | Batch 61/94 | Batch Loss: 3.4146\n",
      "Epoch 12 | Batch 62/94 | Batch Loss: 3.4991\n",
      "Epoch 12 | Batch 63/94 | Batch Loss: 3.5768\n",
      "Epoch 12 | Batch 64/94 | Batch Loss: 3.4660\n",
      "Epoch 12 | Batch 65/94 | Batch Loss: 3.5078\n",
      "Epoch 12 | Batch 66/94 | Batch Loss: 3.3908\n",
      "Epoch 12 | Batch 67/94 | Batch Loss: 3.5206\n",
      "Epoch 12 | Batch 68/94 | Batch Loss: 3.5267\n",
      "Epoch 12 | Batch 69/94 | Batch Loss: 3.4385\n",
      "Epoch 12 | Batch 70/94 | Batch Loss: 3.5036\n",
      "Epoch 12 | Batch 71/94 | Batch Loss: 3.4862\n",
      "Epoch 12 | Batch 72/94 | Batch Loss: 3.4858\n",
      "Epoch 12 | Batch 73/94 | Batch Loss: 3.5112\n",
      "Epoch 12 | Batch 74/94 | Batch Loss: 3.5274\n",
      "Epoch 12 | Batch 75/94 | Batch Loss: 3.3470\n",
      "Epoch 12 | Batch 76/94 | Batch Loss: 3.5291\n",
      "Epoch 12 | Batch 77/94 | Batch Loss: 3.4622\n",
      "Epoch 12 | Batch 78/94 | Batch Loss: 3.4636\n",
      "Epoch 12 | Batch 79/94 | Batch Loss: 3.4740\n",
      "Epoch 12 | Batch 80/94 | Batch Loss: 3.5054\n",
      "Epoch 12 | Batch 81/94 | Batch Loss: 3.5980\n",
      "Epoch 12 | Batch 82/94 | Batch Loss: 3.4432\n",
      "Epoch 12 | Batch 83/94 | Batch Loss: 3.5495\n",
      "Epoch 12 | Batch 84/94 | Batch Loss: 3.5052\n",
      "Epoch 12 | Batch 85/94 | Batch Loss: 3.4357\n",
      "Epoch 12 | Batch 86/94 | Batch Loss: 3.4698\n",
      "Epoch 12 | Batch 87/94 | Batch Loss: 3.5259\n",
      "Epoch 12 | Batch 88/94 | Batch Loss: 3.5130\n",
      "Epoch 12 | Batch 89/94 | Batch Loss: 3.5200\n",
      "Epoch 12 | Batch 90/94 | Batch Loss: 3.5916\n",
      "Epoch 12 | Batch 91/94 | Batch Loss: 3.5268\n",
      "Epoch 12 | Batch 92/94 | Batch Loss: 3.5444\n",
      "Epoch 12 | Batch 93/94 | Batch Loss: 3.5586\n",
      "Epoch 12 | Batch 94/94 | Batch Loss: 3.5117\n",
      "Epoch 12 Finished | Average Loss: 3.5079\n",
      "\n",
      "Epoch 13 Starting...\n",
      "Epoch 13 | Batch 1/94 | Batch Loss: 3.2732\n",
      "Epoch 13 | Batch 2/94 | Batch Loss: 3.3969\n",
      "Epoch 13 | Batch 3/94 | Batch Loss: 3.6303\n",
      "Epoch 13 | Batch 4/94 | Batch Loss: 3.4736\n",
      "Epoch 13 | Batch 5/94 | Batch Loss: 3.5300\n",
      "Epoch 13 | Batch 6/94 | Batch Loss: 3.5011\n",
      "Epoch 13 | Batch 7/94 | Batch Loss: 3.5219\n",
      "Epoch 13 | Batch 8/94 | Batch Loss: 3.5230\n",
      "Epoch 13 | Batch 9/94 | Batch Loss: 3.4395\n",
      "Epoch 13 | Batch 10/94 | Batch Loss: 3.3254\n",
      "Epoch 13 | Batch 11/94 | Batch Loss: 3.4591\n",
      "Epoch 13 | Batch 12/94 | Batch Loss: 3.4105\n",
      "Epoch 13 | Batch 13/94 | Batch Loss: 3.5295\n",
      "Epoch 13 | Batch 14/94 | Batch Loss: 3.4170\n",
      "Epoch 13 | Batch 15/94 | Batch Loss: 3.5500\n",
      "Epoch 13 | Batch 16/94 | Batch Loss: 3.4608\n",
      "Epoch 13 | Batch 17/94 | Batch Loss: 3.5034\n",
      "Epoch 13 | Batch 18/94 | Batch Loss: 3.4832\n",
      "Epoch 13 | Batch 19/94 | Batch Loss: 3.4519\n",
      "Epoch 13 | Batch 20/94 | Batch Loss: 3.4068\n",
      "Epoch 13 | Batch 21/94 | Batch Loss: 3.5052\n",
      "Epoch 13 | Batch 22/94 | Batch Loss: 3.5195\n",
      "Epoch 13 | Batch 23/94 | Batch Loss: 3.4283\n",
      "Epoch 13 | Batch 24/94 | Batch Loss: 3.4649\n",
      "Epoch 13 | Batch 25/94 | Batch Loss: 3.4934\n",
      "Epoch 13 | Batch 26/94 | Batch Loss: 3.4939\n",
      "Epoch 13 | Batch 27/94 | Batch Loss: 3.5213\n",
      "Epoch 13 | Batch 28/94 | Batch Loss: 3.4122\n",
      "Epoch 13 | Batch 29/94 | Batch Loss: 3.5599\n",
      "Epoch 13 | Batch 30/94 | Batch Loss: 3.4171\n",
      "Epoch 13 | Batch 31/94 | Batch Loss: 3.4175\n",
      "Epoch 13 | Batch 32/94 | Batch Loss: 3.3881\n",
      "Epoch 13 | Batch 33/94 | Batch Loss: 3.3791\n",
      "Epoch 13 | Batch 34/94 | Batch Loss: 3.5056\n",
      "Epoch 13 | Batch 35/94 | Batch Loss: 3.4409\n",
      "Epoch 13 | Batch 36/94 | Batch Loss: 3.5111\n",
      "Epoch 13 | Batch 37/94 | Batch Loss: 3.5339\n",
      "Epoch 13 | Batch 38/94 | Batch Loss: 3.3558\n",
      "Epoch 13 | Batch 39/94 | Batch Loss: 3.5800\n",
      "Epoch 13 | Batch 40/94 | Batch Loss: 3.4465\n",
      "Epoch 13 | Batch 41/94 | Batch Loss: 3.3938\n",
      "Epoch 13 | Batch 42/94 | Batch Loss: 3.5341\n",
      "Epoch 13 | Batch 43/94 | Batch Loss: 3.4491\n",
      "Epoch 13 | Batch 44/94 | Batch Loss: 3.5036\n",
      "Epoch 13 | Batch 45/94 | Batch Loss: 3.4800\n",
      "Epoch 13 | Batch 46/94 | Batch Loss: 3.3488\n",
      "Epoch 13 | Batch 47/94 | Batch Loss: 3.4288\n",
      "Epoch 13 | Batch 48/94 | Batch Loss: 3.4104\n",
      "Epoch 13 | Batch 49/94 | Batch Loss: 3.4845\n",
      "Epoch 13 | Batch 50/94 | Batch Loss: 3.4945\n",
      "Epoch 13 | Batch 51/94 | Batch Loss: 3.4626\n",
      "Epoch 13 | Batch 52/94 | Batch Loss: 3.5505\n",
      "Epoch 13 | Batch 53/94 | Batch Loss: 3.4119\n",
      "Epoch 13 | Batch 54/94 | Batch Loss: 3.4547\n",
      "Epoch 13 | Batch 55/94 | Batch Loss: 3.4276\n",
      "Epoch 13 | Batch 56/94 | Batch Loss: 3.4455\n",
      "Epoch 13 | Batch 57/94 | Batch Loss: 3.4434\n",
      "Epoch 13 | Batch 58/94 | Batch Loss: 3.3706\n",
      "Epoch 13 | Batch 59/94 | Batch Loss: 3.3503\n",
      "Epoch 13 | Batch 60/94 | Batch Loss: 3.4929\n",
      "Epoch 13 | Batch 61/94 | Batch Loss: 3.2791\n",
      "Epoch 13 | Batch 62/94 | Batch Loss: 3.4798\n",
      "Epoch 13 | Batch 63/94 | Batch Loss: 3.4598\n",
      "Epoch 13 | Batch 64/94 | Batch Loss: 3.4566\n",
      "Epoch 13 | Batch 65/94 | Batch Loss: 3.5008\n",
      "Epoch 13 | Batch 66/94 | Batch Loss: 3.4809\n",
      "Epoch 13 | Batch 67/94 | Batch Loss: 3.4341\n",
      "Epoch 13 | Batch 68/94 | Batch Loss: 3.4496\n",
      "Epoch 13 | Batch 69/94 | Batch Loss: 3.4222\n",
      "Epoch 13 | Batch 70/94 | Batch Loss: 3.4702\n",
      "Epoch 13 | Batch 71/94 | Batch Loss: 3.4177\n",
      "Epoch 13 | Batch 72/94 | Batch Loss: 3.5269\n",
      "Epoch 13 | Batch 73/94 | Batch Loss: 3.4298\n",
      "Epoch 13 | Batch 74/94 | Batch Loss: 3.4162\n",
      "Epoch 13 | Batch 75/94 | Batch Loss: 3.4349\n",
      "Epoch 13 | Batch 76/94 | Batch Loss: 3.4152\n",
      "Epoch 13 | Batch 77/94 | Batch Loss: 3.4401\n",
      "Epoch 13 | Batch 78/94 | Batch Loss: 3.3291\n",
      "Epoch 13 | Batch 79/94 | Batch Loss: 3.5215\n",
      "Epoch 13 | Batch 80/94 | Batch Loss: 3.3958\n",
      "Epoch 13 | Batch 81/94 | Batch Loss: 3.4529\n",
      "Epoch 13 | Batch 82/94 | Batch Loss: 3.3634\n",
      "Epoch 13 | Batch 83/94 | Batch Loss: 3.3693\n",
      "Epoch 13 | Batch 84/94 | Batch Loss: 3.4445\n",
      "Epoch 13 | Batch 85/94 | Batch Loss: 3.3831\n",
      "Epoch 13 | Batch 86/94 | Batch Loss: 3.4506\n",
      "Epoch 13 | Batch 87/94 | Batch Loss: 3.4575\n",
      "Epoch 13 | Batch 88/94 | Batch Loss: 3.5725\n",
      "Epoch 13 | Batch 89/94 | Batch Loss: 3.5049\n",
      "Epoch 13 | Batch 90/94 | Batch Loss: 3.3587\n",
      "Epoch 13 | Batch 91/94 | Batch Loss: 3.3178\n",
      "Epoch 13 | Batch 92/94 | Batch Loss: 3.3342\n",
      "Epoch 13 | Batch 93/94 | Batch Loss: 3.4449\n",
      "Epoch 13 | Batch 94/94 | Batch Loss: 3.4715\n",
      "Epoch 13 Finished | Average Loss: 3.4498\n",
      "\n",
      "Epoch 14 Starting...\n",
      "Epoch 14 | Batch 1/94 | Batch Loss: 3.3520\n",
      "Epoch 14 | Batch 2/94 | Batch Loss: 3.4507\n",
      "Epoch 14 | Batch 3/94 | Batch Loss: 3.5758\n",
      "Epoch 14 | Batch 4/94 | Batch Loss: 3.5206\n",
      "Epoch 14 | Batch 5/94 | Batch Loss: 3.3546\n",
      "Epoch 14 | Batch 6/94 | Batch Loss: 3.4161\n",
      "Epoch 14 | Batch 7/94 | Batch Loss: 3.4444\n",
      "Epoch 14 | Batch 8/94 | Batch Loss: 3.3096\n",
      "Epoch 14 | Batch 9/94 | Batch Loss: 3.4340\n",
      "Epoch 14 | Batch 10/94 | Batch Loss: 3.3879\n",
      "Epoch 14 | Batch 11/94 | Batch Loss: 3.4049\n",
      "Epoch 14 | Batch 12/94 | Batch Loss: 3.5562\n",
      "Epoch 14 | Batch 13/94 | Batch Loss: 3.5237\n",
      "Epoch 14 | Batch 14/94 | Batch Loss: 3.3290\n",
      "Epoch 14 | Batch 15/94 | Batch Loss: 3.2922\n",
      "Epoch 14 | Batch 16/94 | Batch Loss: 3.4366\n",
      "Epoch 14 | Batch 17/94 | Batch Loss: 3.2891\n",
      "Epoch 14 | Batch 18/94 | Batch Loss: 3.4962\n",
      "Epoch 14 | Batch 19/94 | Batch Loss: 3.4035\n",
      "Epoch 14 | Batch 20/94 | Batch Loss: 3.4029\n",
      "Epoch 14 | Batch 21/94 | Batch Loss: 3.4526\n",
      "Epoch 14 | Batch 22/94 | Batch Loss: 3.3438\n",
      "Epoch 14 | Batch 23/94 | Batch Loss: 3.4729\n",
      "Epoch 14 | Batch 24/94 | Batch Loss: 3.4494\n",
      "Epoch 14 | Batch 25/94 | Batch Loss: 3.2987\n",
      "Epoch 14 | Batch 26/94 | Batch Loss: 3.3704\n",
      "Epoch 14 | Batch 27/94 | Batch Loss: 3.3975\n",
      "Epoch 14 | Batch 28/94 | Batch Loss: 3.3405\n",
      "Epoch 14 | Batch 29/94 | Batch Loss: 3.4443\n",
      "Epoch 14 | Batch 30/94 | Batch Loss: 3.4269\n",
      "Epoch 14 | Batch 31/94 | Batch Loss: 3.4036\n",
      "Epoch 14 | Batch 32/94 | Batch Loss: 3.3617\n",
      "Epoch 14 | Batch 33/94 | Batch Loss: 3.3539\n",
      "Epoch 14 | Batch 34/94 | Batch Loss: 3.3203\n",
      "Epoch 14 | Batch 35/94 | Batch Loss: 3.3684\n",
      "Epoch 14 | Batch 36/94 | Batch Loss: 3.3821\n",
      "Epoch 14 | Batch 37/94 | Batch Loss: 3.3534\n",
      "Epoch 14 | Batch 38/94 | Batch Loss: 3.4937\n",
      "Epoch 14 | Batch 39/94 | Batch Loss: 3.3987\n",
      "Epoch 14 | Batch 40/94 | Batch Loss: 3.3222\n",
      "Epoch 14 | Batch 41/94 | Batch Loss: 3.3855\n",
      "Epoch 14 | Batch 42/94 | Batch Loss: 3.4820\n",
      "Epoch 14 | Batch 43/94 | Batch Loss: 3.4532\n",
      "Epoch 14 | Batch 44/94 | Batch Loss: 3.3961\n",
      "Epoch 14 | Batch 45/94 | Batch Loss: 3.3956\n",
      "Epoch 14 | Batch 46/94 | Batch Loss: 3.4286\n",
      "Epoch 14 | Batch 47/94 | Batch Loss: 3.4778\n",
      "Epoch 14 | Batch 48/94 | Batch Loss: 3.4444\n",
      "Epoch 14 | Batch 49/94 | Batch Loss: 3.3516\n",
      "Epoch 14 | Batch 50/94 | Batch Loss: 3.3141\n",
      "Epoch 14 | Batch 51/94 | Batch Loss: 3.3373\n",
      "Epoch 14 | Batch 52/94 | Batch Loss: 3.3735\n",
      "Epoch 14 | Batch 53/94 | Batch Loss: 3.2636\n",
      "Epoch 14 | Batch 54/94 | Batch Loss: 3.4228\n",
      "Epoch 14 | Batch 55/94 | Batch Loss: 3.4602\n",
      "Epoch 14 | Batch 56/94 | Batch Loss: 3.3033\n",
      "Epoch 14 | Batch 57/94 | Batch Loss: 3.2624\n",
      "Epoch 14 | Batch 58/94 | Batch Loss: 3.4945\n",
      "Epoch 14 | Batch 59/94 | Batch Loss: 3.3470\n",
      "Epoch 14 | Batch 60/94 | Batch Loss: 3.3684\n",
      "Epoch 14 | Batch 61/94 | Batch Loss: 3.4071\n",
      "Epoch 14 | Batch 62/94 | Batch Loss: 3.3642\n",
      "Epoch 14 | Batch 63/94 | Batch Loss: 3.3988\n",
      "Epoch 14 | Batch 64/94 | Batch Loss: 3.3436\n",
      "Epoch 14 | Batch 65/94 | Batch Loss: 3.3710\n",
      "Epoch 14 | Batch 66/94 | Batch Loss: 3.4377\n",
      "Epoch 14 | Batch 67/94 | Batch Loss: 3.4097\n",
      "Epoch 14 | Batch 68/94 | Batch Loss: 3.3704\n",
      "Epoch 14 | Batch 69/94 | Batch Loss: 3.4126\n",
      "Epoch 14 | Batch 70/94 | Batch Loss: 3.4365\n",
      "Epoch 14 | Batch 71/94 | Batch Loss: 3.3915\n",
      "Epoch 14 | Batch 72/94 | Batch Loss: 3.4063\n",
      "Epoch 14 | Batch 73/94 | Batch Loss: 3.3035\n",
      "Epoch 14 | Batch 74/94 | Batch Loss: 3.4211\n",
      "Epoch 14 | Batch 75/94 | Batch Loss: 3.2954\n",
      "Epoch 14 | Batch 76/94 | Batch Loss: 3.3437\n",
      "Epoch 14 | Batch 77/94 | Batch Loss: 3.4716\n",
      "Epoch 14 | Batch 78/94 | Batch Loss: 3.3980\n",
      "Epoch 14 | Batch 79/94 | Batch Loss: 3.3737\n",
      "Epoch 14 | Batch 80/94 | Batch Loss: 3.4994\n",
      "Epoch 14 | Batch 81/94 | Batch Loss: 3.3348\n",
      "Epoch 14 | Batch 82/94 | Batch Loss: 3.4315\n",
      "Epoch 14 | Batch 83/94 | Batch Loss: 3.5649\n",
      "Epoch 14 | Batch 84/94 | Batch Loss: 3.3260\n",
      "Epoch 14 | Batch 85/94 | Batch Loss: 3.3299\n",
      "Epoch 14 | Batch 86/94 | Batch Loss: 3.4707\n",
      "Epoch 14 | Batch 87/94 | Batch Loss: 3.4500\n",
      "Epoch 14 | Batch 88/94 | Batch Loss: 3.3553\n",
      "Epoch 14 | Batch 89/94 | Batch Loss: 3.3520\n",
      "Epoch 14 | Batch 90/94 | Batch Loss: 3.3270\n",
      "Epoch 14 | Batch 91/94 | Batch Loss: 3.4141\n",
      "Epoch 14 | Batch 92/94 | Batch Loss: 3.4384\n",
      "Epoch 14 | Batch 93/94 | Batch Loss: 3.3907\n",
      "Epoch 14 | Batch 94/94 | Batch Loss: 3.3050\n",
      "Epoch 14 Finished | Average Loss: 3.3962\n",
      "\n",
      "Epoch 15 Starting...\n",
      "Epoch 15 | Batch 1/94 | Batch Loss: 3.2583\n",
      "Epoch 15 | Batch 2/94 | Batch Loss: 3.3241\n",
      "Epoch 15 | Batch 3/94 | Batch Loss: 3.4410\n",
      "Epoch 15 | Batch 4/94 | Batch Loss: 3.2645\n",
      "Epoch 15 | Batch 5/94 | Batch Loss: 3.3832\n",
      "Epoch 15 | Batch 6/94 | Batch Loss: 3.4183\n",
      "Epoch 15 | Batch 7/94 | Batch Loss: 3.2904\n",
      "Epoch 15 | Batch 8/94 | Batch Loss: 3.3369\n",
      "Epoch 15 | Batch 9/94 | Batch Loss: 3.3674\n",
      "Epoch 15 | Batch 10/94 | Batch Loss: 3.4316\n",
      "Epoch 15 | Batch 11/94 | Batch Loss: 3.4106\n",
      "Epoch 15 | Batch 12/94 | Batch Loss: 3.4220\n",
      "Epoch 15 | Batch 13/94 | Batch Loss: 3.3554\n",
      "Epoch 15 | Batch 14/94 | Batch Loss: 3.3049\n",
      "Epoch 15 | Batch 15/94 | Batch Loss: 3.3184\n",
      "Epoch 15 | Batch 16/94 | Batch Loss: 3.4323\n",
      "Epoch 15 | Batch 17/94 | Batch Loss: 3.3077\n",
      "Epoch 15 | Batch 18/94 | Batch Loss: 3.2841\n",
      "Epoch 15 | Batch 19/94 | Batch Loss: 3.2994\n",
      "Epoch 15 | Batch 20/94 | Batch Loss: 3.3800\n",
      "Epoch 15 | Batch 21/94 | Batch Loss: 3.3456\n",
      "Epoch 15 | Batch 22/94 | Batch Loss: 3.3629\n",
      "Epoch 15 | Batch 23/94 | Batch Loss: 3.2142\n",
      "Epoch 15 | Batch 24/94 | Batch Loss: 3.3370\n",
      "Epoch 15 | Batch 25/94 | Batch Loss: 3.3467\n",
      "Epoch 15 | Batch 26/94 | Batch Loss: 3.3339\n",
      "Epoch 15 | Batch 27/94 | Batch Loss: 3.3799\n",
      "Epoch 15 | Batch 28/94 | Batch Loss: 3.3422\n",
      "Epoch 15 | Batch 29/94 | Batch Loss: 3.4204\n",
      "Epoch 15 | Batch 30/94 | Batch Loss: 3.2624\n",
      "Epoch 15 | Batch 31/94 | Batch Loss: 3.3302\n",
      "Epoch 15 | Batch 32/94 | Batch Loss: 3.3921\n",
      "Epoch 15 | Batch 33/94 | Batch Loss: 3.2799\n",
      "Epoch 15 | Batch 34/94 | Batch Loss: 3.4196\n",
      "Epoch 15 | Batch 35/94 | Batch Loss: 3.3599\n",
      "Epoch 15 | Batch 36/94 | Batch Loss: 3.3785\n",
      "Epoch 15 | Batch 37/94 | Batch Loss: 3.3877\n",
      "Epoch 15 | Batch 38/94 | Batch Loss: 3.3395\n",
      "Epoch 15 | Batch 39/94 | Batch Loss: 3.3298\n",
      "Epoch 15 | Batch 40/94 | Batch Loss: 3.4183\n",
      "Epoch 15 | Batch 41/94 | Batch Loss: 3.3820\n",
      "Epoch 15 | Batch 42/94 | Batch Loss: 3.4125\n",
      "Epoch 15 | Batch 43/94 | Batch Loss: 3.3914\n",
      "Epoch 15 | Batch 44/94 | Batch Loss: 3.4230\n",
      "Epoch 15 | Batch 45/94 | Batch Loss: 3.3952\n",
      "Epoch 15 | Batch 46/94 | Batch Loss: 3.3848\n",
      "Epoch 15 | Batch 47/94 | Batch Loss: 3.3311\n",
      "Epoch 15 | Batch 48/94 | Batch Loss: 3.3740\n",
      "Epoch 15 | Batch 49/94 | Batch Loss: 3.3159\n",
      "Epoch 15 | Batch 50/94 | Batch Loss: 3.2671\n",
      "Epoch 15 | Batch 51/94 | Batch Loss: 3.1662\n",
      "Epoch 15 | Batch 52/94 | Batch Loss: 3.4189\n",
      "Epoch 15 | Batch 53/94 | Batch Loss: 3.4484\n",
      "Epoch 15 | Batch 54/94 | Batch Loss: 3.3320\n",
      "Epoch 15 | Batch 55/94 | Batch Loss: 3.2630\n",
      "Epoch 15 | Batch 56/94 | Batch Loss: 3.3402\n",
      "Epoch 15 | Batch 57/94 | Batch Loss: 3.4332\n",
      "Epoch 15 | Batch 58/94 | Batch Loss: 3.2440\n",
      "Epoch 15 | Batch 59/94 | Batch Loss: 3.2626\n",
      "Epoch 15 | Batch 60/94 | Batch Loss: 3.3524\n",
      "Epoch 15 | Batch 61/94 | Batch Loss: 3.3615\n",
      "Epoch 15 | Batch 62/94 | Batch Loss: 3.2005\n",
      "Epoch 15 | Batch 63/94 | Batch Loss: 3.2774\n",
      "Epoch 15 | Batch 64/94 | Batch Loss: 3.3777\n",
      "Epoch 15 | Batch 65/94 | Batch Loss: 3.2793\n",
      "Epoch 15 | Batch 66/94 | Batch Loss: 3.3087\n",
      "Epoch 15 | Batch 67/94 | Batch Loss: 3.3260\n",
      "Epoch 15 | Batch 68/94 | Batch Loss: 3.3107\n",
      "Epoch 15 | Batch 69/94 | Batch Loss: 3.3359\n",
      "Epoch 15 | Batch 70/94 | Batch Loss: 3.4377\n",
      "Epoch 15 | Batch 71/94 | Batch Loss: 3.3674\n",
      "Epoch 15 | Batch 72/94 | Batch Loss: 3.2980\n",
      "Epoch 15 | Batch 73/94 | Batch Loss: 3.4740\n",
      "Epoch 15 | Batch 74/94 | Batch Loss: 3.3141\n",
      "Epoch 15 | Batch 75/94 | Batch Loss: 3.3381\n",
      "Epoch 15 | Batch 76/94 | Batch Loss: 3.4562\n",
      "Epoch 15 | Batch 77/94 | Batch Loss: 3.2996\n",
      "Epoch 15 | Batch 78/94 | Batch Loss: 3.3385\n",
      "Epoch 15 | Batch 79/94 | Batch Loss: 3.4342\n",
      "Epoch 15 | Batch 80/94 | Batch Loss: 3.3279\n",
      "Epoch 15 | Batch 81/94 | Batch Loss: 3.3183\n",
      "Epoch 15 | Batch 82/94 | Batch Loss: 3.3860\n",
      "Epoch 15 | Batch 83/94 | Batch Loss: 3.2464\n",
      "Epoch 15 | Batch 84/94 | Batch Loss: 3.2813\n",
      "Epoch 15 | Batch 85/94 | Batch Loss: 3.3121\n",
      "Epoch 15 | Batch 86/94 | Batch Loss: 3.3450\n",
      "Epoch 15 | Batch 87/94 | Batch Loss: 3.4078\n",
      "Epoch 15 | Batch 88/94 | Batch Loss: 3.3633\n",
      "Epoch 15 | Batch 89/94 | Batch Loss: 3.3133\n",
      "Epoch 15 | Batch 90/94 | Batch Loss: 3.2948\n",
      "Epoch 15 | Batch 91/94 | Batch Loss: 3.2476\n",
      "Epoch 15 | Batch 92/94 | Batch Loss: 3.3224\n",
      "Epoch 15 | Batch 93/94 | Batch Loss: 3.3222\n",
      "Epoch 15 | Batch 94/94 | Batch Loss: 3.3204\n",
      "Epoch 15 Finished | Average Loss: 3.3435\n",
      "\n",
      "Epoch 16 Starting...\n",
      "Epoch 16 | Batch 1/94 | Batch Loss: 3.2612\n",
      "Epoch 16 | Batch 2/94 | Batch Loss: 3.3430\n",
      "Epoch 16 | Batch 3/94 | Batch Loss: 3.2074\n",
      "Epoch 16 | Batch 4/94 | Batch Loss: 3.2659\n",
      "Epoch 16 | Batch 5/94 | Batch Loss: 3.2944\n",
      "Epoch 16 | Batch 6/94 | Batch Loss: 3.2640\n",
      "Epoch 16 | Batch 7/94 | Batch Loss: 3.3091\n",
      "Epoch 16 | Batch 8/94 | Batch Loss: 3.4074\n",
      "Epoch 16 | Batch 9/94 | Batch Loss: 3.2429\n",
      "Epoch 16 | Batch 10/94 | Batch Loss: 3.2886\n",
      "Epoch 16 | Batch 11/94 | Batch Loss: 3.2703\n",
      "Epoch 16 | Batch 12/94 | Batch Loss: 3.3638\n",
      "Epoch 16 | Batch 13/94 | Batch Loss: 3.2272\n",
      "Epoch 16 | Batch 14/94 | Batch Loss: 3.4010\n",
      "Epoch 16 | Batch 15/94 | Batch Loss: 3.2732\n",
      "Epoch 16 | Batch 16/94 | Batch Loss: 3.3491\n",
      "Epoch 16 | Batch 17/94 | Batch Loss: 3.2830\n",
      "Epoch 16 | Batch 18/94 | Batch Loss: 3.3535\n",
      "Epoch 16 | Batch 19/94 | Batch Loss: 3.2189\n",
      "Epoch 16 | Batch 20/94 | Batch Loss: 3.3737\n",
      "Epoch 16 | Batch 21/94 | Batch Loss: 3.3148\n",
      "Epoch 16 | Batch 22/94 | Batch Loss: 3.2855\n",
      "Epoch 16 | Batch 23/94 | Batch Loss: 3.2995\n",
      "Epoch 16 | Batch 24/94 | Batch Loss: 3.3722\n",
      "Epoch 16 | Batch 25/94 | Batch Loss: 3.3680\n",
      "Epoch 16 | Batch 26/94 | Batch Loss: 3.1400\n",
      "Epoch 16 | Batch 27/94 | Batch Loss: 3.3232\n",
      "Epoch 16 | Batch 28/94 | Batch Loss: 3.3088\n",
      "Epoch 16 | Batch 29/94 | Batch Loss: 3.3463\n",
      "Epoch 16 | Batch 30/94 | Batch Loss: 3.3404\n",
      "Epoch 16 | Batch 31/94 | Batch Loss: 3.2442\n",
      "Epoch 16 | Batch 32/94 | Batch Loss: 3.3412\n",
      "Epoch 16 | Batch 33/94 | Batch Loss: 3.4062\n",
      "Epoch 16 | Batch 34/94 | Batch Loss: 3.3960\n",
      "Epoch 16 | Batch 35/94 | Batch Loss: 3.4055\n",
      "Epoch 16 | Batch 36/94 | Batch Loss: 3.3125\n",
      "Epoch 16 | Batch 37/94 | Batch Loss: 3.3711\n",
      "Epoch 16 | Batch 38/94 | Batch Loss: 3.1759\n",
      "Epoch 16 | Batch 39/94 | Batch Loss: 3.2365\n",
      "Epoch 16 | Batch 40/94 | Batch Loss: 3.2889\n",
      "Epoch 16 | Batch 41/94 | Batch Loss: 3.1786\n",
      "Epoch 16 | Batch 42/94 | Batch Loss: 3.2461\n",
      "Epoch 16 | Batch 43/94 | Batch Loss: 3.3688\n",
      "Epoch 16 | Batch 44/94 | Batch Loss: 3.3332\n",
      "Epoch 16 | Batch 45/94 | Batch Loss: 3.2298\n",
      "Epoch 16 | Batch 46/94 | Batch Loss: 3.2522\n",
      "Epoch 16 | Batch 47/94 | Batch Loss: 3.2873\n",
      "Epoch 16 | Batch 48/94 | Batch Loss: 3.2944\n",
      "Epoch 16 | Batch 49/94 | Batch Loss: 3.3863\n",
      "Epoch 16 | Batch 50/94 | Batch Loss: 3.3410\n",
      "Epoch 16 | Batch 51/94 | Batch Loss: 3.2392\n",
      "Epoch 16 | Batch 52/94 | Batch Loss: 3.1948\n",
      "Epoch 16 | Batch 53/94 | Batch Loss: 3.3896\n",
      "Epoch 16 | Batch 54/94 | Batch Loss: 3.2275\n",
      "Epoch 16 | Batch 55/94 | Batch Loss: 3.3387\n",
      "Epoch 16 | Batch 56/94 | Batch Loss: 3.2706\n",
      "Epoch 16 | Batch 57/94 | Batch Loss: 3.2078\n",
      "Epoch 16 | Batch 58/94 | Batch Loss: 3.2892\n",
      "Epoch 16 | Batch 59/94 | Batch Loss: 3.2613\n",
      "Epoch 16 | Batch 60/94 | Batch Loss: 3.2783\n",
      "Epoch 16 | Batch 61/94 | Batch Loss: 3.2141\n",
      "Epoch 16 | Batch 62/94 | Batch Loss: 3.3444\n",
      "Epoch 16 | Batch 63/94 | Batch Loss: 3.2757\n",
      "Epoch 16 | Batch 64/94 | Batch Loss: 3.2521\n",
      "Epoch 16 | Batch 65/94 | Batch Loss: 3.2900\n",
      "Epoch 16 | Batch 66/94 | Batch Loss: 3.2959\n",
      "Epoch 16 | Batch 67/94 | Batch Loss: 3.1474\n",
      "Epoch 16 | Batch 68/94 | Batch Loss: 3.2984\n",
      "Epoch 16 | Batch 69/94 | Batch Loss: 3.3498\n",
      "Epoch 16 | Batch 70/94 | Batch Loss: 3.3415\n",
      "Epoch 16 | Batch 71/94 | Batch Loss: 3.3538\n",
      "Epoch 16 | Batch 72/94 | Batch Loss: 3.3218\n",
      "Epoch 16 | Batch 73/94 | Batch Loss: 3.2893\n",
      "Epoch 16 | Batch 74/94 | Batch Loss: 3.2793\n",
      "Epoch 16 | Batch 75/94 | Batch Loss: 3.3493\n",
      "Epoch 16 | Batch 76/94 | Batch Loss: 3.2502\n",
      "Epoch 16 | Batch 77/94 | Batch Loss: 3.3081\n",
      "Epoch 16 | Batch 78/94 | Batch Loss: 3.2330\n",
      "Epoch 16 | Batch 79/94 | Batch Loss: 3.2960\n",
      "Epoch 16 | Batch 80/94 | Batch Loss: 3.3737\n",
      "Epoch 16 | Batch 81/94 | Batch Loss: 3.2990\n",
      "Epoch 16 | Batch 82/94 | Batch Loss: 3.2643\n",
      "Epoch 16 | Batch 83/94 | Batch Loss: 3.2418\n",
      "Epoch 16 | Batch 84/94 | Batch Loss: 3.3240\n",
      "Epoch 16 | Batch 85/94 | Batch Loss: 3.2576\n",
      "Epoch 16 | Batch 86/94 | Batch Loss: 3.2986\n",
      "Epoch 16 | Batch 87/94 | Batch Loss: 3.2900\n",
      "Epoch 16 | Batch 88/94 | Batch Loss: 3.1710\n",
      "Epoch 16 | Batch 89/94 | Batch Loss: 3.2745\n",
      "Epoch 16 | Batch 90/94 | Batch Loss: 3.2490\n",
      "Epoch 16 | Batch 91/94 | Batch Loss: 3.3168\n",
      "Epoch 16 | Batch 92/94 | Batch Loss: 3.2788\n",
      "Epoch 16 | Batch 93/94 | Batch Loss: 3.2853\n",
      "Epoch 16 | Batch 94/94 | Batch Loss: 3.3566\n",
      "Epoch 16 Finished | Average Loss: 3.2943\n",
      "\n",
      "Epoch 17 Starting...\n",
      "Epoch 17 | Batch 1/94 | Batch Loss: 3.2819\n",
      "Epoch 17 | Batch 2/94 | Batch Loss: 3.2511\n",
      "Epoch 17 | Batch 3/94 | Batch Loss: 3.2814\n",
      "Epoch 17 | Batch 4/94 | Batch Loss: 3.2555\n",
      "Epoch 17 | Batch 5/94 | Batch Loss: 3.1858\n",
      "Epoch 17 | Batch 6/94 | Batch Loss: 3.2089\n",
      "Epoch 17 | Batch 7/94 | Batch Loss: 3.3241\n",
      "Epoch 17 | Batch 8/94 | Batch Loss: 3.3000\n",
      "Epoch 17 | Batch 9/94 | Batch Loss: 3.2003\n",
      "Epoch 17 | Batch 10/94 | Batch Loss: 3.2738\n",
      "Epoch 17 | Batch 11/94 | Batch Loss: 3.2245\n",
      "Epoch 17 | Batch 12/94 | Batch Loss: 3.3214\n",
      "Epoch 17 | Batch 13/94 | Batch Loss: 3.2013\n",
      "Epoch 17 | Batch 14/94 | Batch Loss: 3.1935\n",
      "Epoch 17 | Batch 15/94 | Batch Loss: 3.2883\n",
      "Epoch 17 | Batch 16/94 | Batch Loss: 3.1854\n",
      "Epoch 17 | Batch 17/94 | Batch Loss: 3.3631\n",
      "Epoch 17 | Batch 18/94 | Batch Loss: 3.1718\n",
      "Epoch 17 | Batch 19/94 | Batch Loss: 3.2329\n",
      "Epoch 17 | Batch 20/94 | Batch Loss: 3.1975\n",
      "Epoch 17 | Batch 21/94 | Batch Loss: 3.2965\n",
      "Epoch 17 | Batch 22/94 | Batch Loss: 3.2199\n",
      "Epoch 17 | Batch 23/94 | Batch Loss: 3.1673\n",
      "Epoch 17 | Batch 24/94 | Batch Loss: 3.2753\n",
      "Epoch 17 | Batch 25/94 | Batch Loss: 3.0801\n",
      "Epoch 17 | Batch 26/94 | Batch Loss: 3.1794\n",
      "Epoch 17 | Batch 27/94 | Batch Loss: 3.2107\n",
      "Epoch 17 | Batch 28/94 | Batch Loss: 3.2840\n",
      "Epoch 17 | Batch 29/94 | Batch Loss: 3.2560\n",
      "Epoch 17 | Batch 30/94 | Batch Loss: 3.3007\n",
      "Epoch 17 | Batch 31/94 | Batch Loss: 3.1805\n",
      "Epoch 17 | Batch 32/94 | Batch Loss: 3.2253\n",
      "Epoch 17 | Batch 33/94 | Batch Loss: 3.3330\n",
      "Epoch 17 | Batch 34/94 | Batch Loss: 3.2551\n",
      "Epoch 17 | Batch 35/94 | Batch Loss: 3.2320\n",
      "Epoch 17 | Batch 36/94 | Batch Loss: 3.2969\n",
      "Epoch 17 | Batch 37/94 | Batch Loss: 3.3096\n",
      "Epoch 17 | Batch 38/94 | Batch Loss: 3.3206\n",
      "Epoch 17 | Batch 39/94 | Batch Loss: 3.3036\n",
      "Epoch 17 | Batch 40/94 | Batch Loss: 3.2882\n",
      "Epoch 17 | Batch 41/94 | Batch Loss: 3.1998\n",
      "Epoch 17 | Batch 42/94 | Batch Loss: 3.1518\n",
      "Epoch 17 | Batch 43/94 | Batch Loss: 3.1922\n",
      "Epoch 17 | Batch 44/94 | Batch Loss: 3.2461\n",
      "Epoch 17 | Batch 45/94 | Batch Loss: 3.2250\n",
      "Epoch 17 | Batch 46/94 | Batch Loss: 3.2660\n",
      "Epoch 17 | Batch 47/94 | Batch Loss: 3.3147\n",
      "Epoch 17 | Batch 48/94 | Batch Loss: 3.1448\n",
      "Epoch 17 | Batch 49/94 | Batch Loss: 3.3515\n",
      "Epoch 17 | Batch 50/94 | Batch Loss: 3.2532\n",
      "Epoch 17 | Batch 51/94 | Batch Loss: 3.1113\n",
      "Epoch 17 | Batch 52/94 | Batch Loss: 3.1914\n",
      "Epoch 17 | Batch 53/94 | Batch Loss: 3.2609\n",
      "Epoch 17 | Batch 54/94 | Batch Loss: 3.1733\n",
      "Epoch 17 | Batch 55/94 | Batch Loss: 3.2116\n",
      "Epoch 17 | Batch 56/94 | Batch Loss: 3.1085\n",
      "Epoch 17 | Batch 57/94 | Batch Loss: 3.3191\n",
      "Epoch 17 | Batch 58/94 | Batch Loss: 3.2189\n",
      "Epoch 17 | Batch 59/94 | Batch Loss: 3.3419\n",
      "Epoch 17 | Batch 60/94 | Batch Loss: 3.2917\n",
      "Epoch 17 | Batch 61/94 | Batch Loss: 3.2184\n",
      "Epoch 17 | Batch 62/94 | Batch Loss: 3.1778\n",
      "Epoch 17 | Batch 63/94 | Batch Loss: 3.2415\n",
      "Epoch 17 | Batch 64/94 | Batch Loss: 3.1596\n",
      "Epoch 17 | Batch 65/94 | Batch Loss: 3.2395\n",
      "Epoch 17 | Batch 66/94 | Batch Loss: 3.2217\n",
      "Epoch 17 | Batch 67/94 | Batch Loss: 3.3693\n",
      "Epoch 17 | Batch 68/94 | Batch Loss: 3.2999\n",
      "Epoch 17 | Batch 69/94 | Batch Loss: 3.1929\n",
      "Epoch 17 | Batch 70/94 | Batch Loss: 3.2651\n",
      "Epoch 17 | Batch 71/94 | Batch Loss: 3.3286\n",
      "Epoch 17 | Batch 72/94 | Batch Loss: 3.2377\n",
      "Epoch 17 | Batch 73/94 | Batch Loss: 3.1613\n",
      "Epoch 17 | Batch 74/94 | Batch Loss: 3.2500\n",
      "Epoch 17 | Batch 75/94 | Batch Loss: 3.2596\n",
      "Epoch 17 | Batch 76/94 | Batch Loss: 3.2745\n",
      "Epoch 17 | Batch 77/94 | Batch Loss: 3.2844\n",
      "Epoch 17 | Batch 78/94 | Batch Loss: 3.3360\n",
      "Epoch 17 | Batch 79/94 | Batch Loss: 3.1917\n",
      "Epoch 17 | Batch 80/94 | Batch Loss: 3.2163\n",
      "Epoch 17 | Batch 81/94 | Batch Loss: 3.2798\n",
      "Epoch 17 | Batch 82/94 | Batch Loss: 3.2491\n",
      "Epoch 17 | Batch 83/94 | Batch Loss: 3.1679\n",
      "Epoch 17 | Batch 84/94 | Batch Loss: 3.2613\n",
      "Epoch 17 | Batch 85/94 | Batch Loss: 3.3576\n",
      "Epoch 17 | Batch 86/94 | Batch Loss: 3.3540\n",
      "Epoch 17 | Batch 87/94 | Batch Loss: 3.4356\n",
      "Epoch 17 | Batch 88/94 | Batch Loss: 3.2164\n",
      "Epoch 17 | Batch 89/94 | Batch Loss: 3.2492\n",
      "Epoch 17 | Batch 90/94 | Batch Loss: 3.2733\n",
      "Epoch 17 | Batch 91/94 | Batch Loss: 3.2549\n",
      "Epoch 17 | Batch 92/94 | Batch Loss: 3.3247\n",
      "Epoch 17 | Batch 93/94 | Batch Loss: 3.2571\n",
      "Epoch 17 | Batch 94/94 | Batch Loss: 3.2410\n",
      "Epoch 17 Finished | Average Loss: 3.2487\n",
      "\n",
      "Epoch 18 Starting...\n",
      "Epoch 18 | Batch 1/94 | Batch Loss: 3.1844\n",
      "Epoch 18 | Batch 2/94 | Batch Loss: 3.2346\n",
      "Epoch 18 | Batch 3/94 | Batch Loss: 3.2220\n",
      "Epoch 18 | Batch 4/94 | Batch Loss: 3.2710\n",
      "Epoch 18 | Batch 5/94 | Batch Loss: 3.3305\n",
      "Epoch 18 | Batch 6/94 | Batch Loss: 3.2865\n",
      "Epoch 18 | Batch 7/94 | Batch Loss: 3.3044\n",
      "Epoch 18 | Batch 8/94 | Batch Loss: 3.2113\n",
      "Epoch 18 | Batch 9/94 | Batch Loss: 3.2365\n",
      "Epoch 18 | Batch 10/94 | Batch Loss: 3.2422\n",
      "Epoch 18 | Batch 11/94 | Batch Loss: 3.2640\n",
      "Epoch 18 | Batch 12/94 | Batch Loss: 3.1522\n",
      "Epoch 18 | Batch 13/94 | Batch Loss: 3.1453\n",
      "Epoch 18 | Batch 14/94 | Batch Loss: 3.1601\n",
      "Epoch 18 | Batch 15/94 | Batch Loss: 3.2345\n",
      "Epoch 18 | Batch 16/94 | Batch Loss: 3.2062\n",
      "Epoch 18 | Batch 17/94 | Batch Loss: 3.2610\n",
      "Epoch 18 | Batch 18/94 | Batch Loss: 3.1827\n",
      "Epoch 18 | Batch 19/94 | Batch Loss: 3.0754\n",
      "Epoch 18 | Batch 20/94 | Batch Loss: 3.0819\n",
      "Epoch 18 | Batch 21/94 | Batch Loss: 3.2526\n",
      "Epoch 18 | Batch 22/94 | Batch Loss: 3.2398\n",
      "Epoch 18 | Batch 23/94 | Batch Loss: 3.1732\n",
      "Epoch 18 | Batch 24/94 | Batch Loss: 3.0800\n",
      "Epoch 18 | Batch 25/94 | Batch Loss: 3.1450\n",
      "Epoch 18 | Batch 26/94 | Batch Loss: 3.2003\n",
      "Epoch 18 | Batch 27/94 | Batch Loss: 3.2730\n",
      "Epoch 18 | Batch 28/94 | Batch Loss: 3.1498\n",
      "Epoch 18 | Batch 29/94 | Batch Loss: 3.2917\n",
      "Epoch 18 | Batch 30/94 | Batch Loss: 3.1852\n",
      "Epoch 18 | Batch 31/94 | Batch Loss: 3.2277\n",
      "Epoch 18 | Batch 32/94 | Batch Loss: 3.1266\n",
      "Epoch 18 | Batch 33/94 | Batch Loss: 3.1719\n",
      "Epoch 18 | Batch 34/94 | Batch Loss: 3.2592\n",
      "Epoch 18 | Batch 35/94 | Batch Loss: 3.2121\n",
      "Epoch 18 | Batch 36/94 | Batch Loss: 3.2022\n",
      "Epoch 18 | Batch 37/94 | Batch Loss: 3.1549\n",
      "Epoch 18 | Batch 38/94 | Batch Loss: 3.2567\n",
      "Epoch 18 | Batch 39/94 | Batch Loss: 3.1155\n",
      "Epoch 18 | Batch 40/94 | Batch Loss: 3.2552\n",
      "Epoch 18 | Batch 41/94 | Batch Loss: 3.1142\n",
      "Epoch 18 | Batch 42/94 | Batch Loss: 3.3551\n",
      "Epoch 18 | Batch 43/94 | Batch Loss: 3.1473\n",
      "Epoch 18 | Batch 44/94 | Batch Loss: 3.2280\n",
      "Epoch 18 | Batch 45/94 | Batch Loss: 3.1590\n",
      "Epoch 18 | Batch 46/94 | Batch Loss: 3.1621\n",
      "Epoch 18 | Batch 47/94 | Batch Loss: 3.1968\n",
      "Epoch 18 | Batch 48/94 | Batch Loss: 3.1881\n",
      "Epoch 18 | Batch 49/94 | Batch Loss: 3.2182\n",
      "Epoch 18 | Batch 50/94 | Batch Loss: 3.2278\n",
      "Epoch 18 | Batch 51/94 | Batch Loss: 3.2303\n",
      "Epoch 18 | Batch 52/94 | Batch Loss: 3.1713\n",
      "Epoch 18 | Batch 53/94 | Batch Loss: 3.2540\n",
      "Epoch 18 | Batch 54/94 | Batch Loss: 3.1823\n",
      "Epoch 18 | Batch 55/94 | Batch Loss: 3.2564\n",
      "Epoch 18 | Batch 56/94 | Batch Loss: 3.2090\n",
      "Epoch 18 | Batch 57/94 | Batch Loss: 3.1324\n",
      "Epoch 18 | Batch 58/94 | Batch Loss: 3.1659\n",
      "Epoch 18 | Batch 59/94 | Batch Loss: 3.2645\n",
      "Epoch 18 | Batch 60/94 | Batch Loss: 3.2742\n",
      "Epoch 18 | Batch 61/94 | Batch Loss: 3.3027\n",
      "Epoch 18 | Batch 62/94 | Batch Loss: 3.1984\n",
      "Epoch 18 | Batch 63/94 | Batch Loss: 3.1681\n",
      "Epoch 18 | Batch 64/94 | Batch Loss: 3.2011\n",
      "Epoch 18 | Batch 65/94 | Batch Loss: 3.2082\n",
      "Epoch 18 | Batch 66/94 | Batch Loss: 3.1017\n",
      "Epoch 18 | Batch 67/94 | Batch Loss: 3.2366\n",
      "Epoch 18 | Batch 68/94 | Batch Loss: 3.2641\n",
      "Epoch 18 | Batch 69/94 | Batch Loss: 3.1906\n",
      "Epoch 18 | Batch 70/94 | Batch Loss: 3.1597\n",
      "Epoch 18 | Batch 71/94 | Batch Loss: 3.2255\n",
      "Epoch 18 | Batch 72/94 | Batch Loss: 3.0905\n",
      "Epoch 18 | Batch 73/94 | Batch Loss: 3.2641\n",
      "Epoch 18 | Batch 74/94 | Batch Loss: 3.1388\n",
      "Epoch 18 | Batch 75/94 | Batch Loss: 3.1967\n",
      "Epoch 18 | Batch 76/94 | Batch Loss: 3.1363\n",
      "Epoch 18 | Batch 77/94 | Batch Loss: 3.1723\n",
      "Epoch 18 | Batch 78/94 | Batch Loss: 3.1547\n",
      "Epoch 18 | Batch 79/94 | Batch Loss: 3.1667\n",
      "Epoch 18 | Batch 80/94 | Batch Loss: 3.0932\n",
      "Epoch 18 | Batch 81/94 | Batch Loss: 3.3199\n",
      "Epoch 18 | Batch 82/94 | Batch Loss: 3.1264\n",
      "Epoch 18 | Batch 83/94 | Batch Loss: 3.1531\n",
      "Epoch 18 | Batch 84/94 | Batch Loss: 3.1447\n",
      "Epoch 18 | Batch 85/94 | Batch Loss: 3.1796\n",
      "Epoch 18 | Batch 86/94 | Batch Loss: 3.1718\n",
      "Epoch 18 | Batch 87/94 | Batch Loss: 3.3155\n",
      "Epoch 18 | Batch 88/94 | Batch Loss: 3.1725\n",
      "Epoch 18 | Batch 89/94 | Batch Loss: 3.1940\n",
      "Epoch 18 | Batch 90/94 | Batch Loss: 3.2479\n",
      "Epoch 18 | Batch 91/94 | Batch Loss: 3.1892\n",
      "Epoch 18 | Batch 92/94 | Batch Loss: 3.2635\n",
      "Epoch 18 | Batch 93/94 | Batch Loss: 3.3427\n",
      "Epoch 18 | Batch 94/94 | Batch Loss: 3.1913\n",
      "Epoch 18 Finished | Average Loss: 3.2030\n",
      "\n",
      "Epoch 19 Starting...\n",
      "Epoch 19 | Batch 1/94 | Batch Loss: 3.0919\n",
      "Epoch 19 | Batch 2/94 | Batch Loss: 3.0323\n",
      "Epoch 19 | Batch 3/94 | Batch Loss: 3.2613\n",
      "Epoch 19 | Batch 4/94 | Batch Loss: 3.1941\n",
      "Epoch 19 | Batch 5/94 | Batch Loss: 3.3306\n",
      "Epoch 19 | Batch 6/94 | Batch Loss: 3.1512\n",
      "Epoch 19 | Batch 7/94 | Batch Loss: 3.1471\n",
      "Epoch 19 | Batch 8/94 | Batch Loss: 3.1371\n",
      "Epoch 19 | Batch 9/94 | Batch Loss: 3.2291\n",
      "Epoch 19 | Batch 10/94 | Batch Loss: 3.2226\n",
      "Epoch 19 | Batch 11/94 | Batch Loss: 3.1778\n",
      "Epoch 19 | Batch 12/94 | Batch Loss: 3.1845\n",
      "Epoch 19 | Batch 13/94 | Batch Loss: 3.0816\n",
      "Epoch 19 | Batch 14/94 | Batch Loss: 3.3274\n",
      "Epoch 19 | Batch 15/94 | Batch Loss: 3.2347\n",
      "Epoch 19 | Batch 16/94 | Batch Loss: 3.1915\n",
      "Epoch 19 | Batch 17/94 | Batch Loss: 3.1718\n",
      "Epoch 19 | Batch 18/94 | Batch Loss: 3.2211\n",
      "Epoch 19 | Batch 19/94 | Batch Loss: 3.2931\n",
      "Epoch 19 | Batch 20/94 | Batch Loss: 3.1281\n",
      "Epoch 19 | Batch 21/94 | Batch Loss: 3.1502\n",
      "Epoch 19 | Batch 22/94 | Batch Loss: 3.1966\n",
      "Epoch 19 | Batch 23/94 | Batch Loss: 3.1850\n",
      "Epoch 19 | Batch 24/94 | Batch Loss: 3.0765\n",
      "Epoch 19 | Batch 25/94 | Batch Loss: 3.1138\n",
      "Epoch 19 | Batch 26/94 | Batch Loss: 3.1229\n",
      "Epoch 19 | Batch 27/94 | Batch Loss: 3.1138\n",
      "Epoch 19 | Batch 28/94 | Batch Loss: 3.0887\n",
      "Epoch 19 | Batch 29/94 | Batch Loss: 3.0634\n",
      "Epoch 19 | Batch 30/94 | Batch Loss: 3.1668\n",
      "Epoch 19 | Batch 31/94 | Batch Loss: 3.1710\n",
      "Epoch 19 | Batch 32/94 | Batch Loss: 3.0966\n",
      "Epoch 19 | Batch 33/94 | Batch Loss: 3.0366\n",
      "Epoch 19 | Batch 34/94 | Batch Loss: 3.2540\n",
      "Epoch 19 | Batch 35/94 | Batch Loss: 3.1473\n",
      "Epoch 19 | Batch 36/94 | Batch Loss: 3.1824\n",
      "Epoch 19 | Batch 37/94 | Batch Loss: 3.2254\n",
      "Epoch 19 | Batch 38/94 | Batch Loss: 3.0591\n",
      "Epoch 19 | Batch 39/94 | Batch Loss: 3.2485\n",
      "Epoch 19 | Batch 40/94 | Batch Loss: 3.1445\n",
      "Epoch 19 | Batch 41/94 | Batch Loss: 3.1118\n",
      "Epoch 19 | Batch 42/94 | Batch Loss: 3.2443\n",
      "Epoch 19 | Batch 43/94 | Batch Loss: 3.1604\n",
      "Epoch 19 | Batch 44/94 | Batch Loss: 3.2077\n",
      "Epoch 19 | Batch 45/94 | Batch Loss: 3.1971\n",
      "Epoch 19 | Batch 46/94 | Batch Loss: 3.2413\n",
      "Epoch 19 | Batch 47/94 | Batch Loss: 3.0942\n",
      "Epoch 19 | Batch 48/94 | Batch Loss: 3.3093\n",
      "Epoch 19 | Batch 49/94 | Batch Loss: 3.2089\n",
      "Epoch 19 | Batch 50/94 | Batch Loss: 3.1518\n",
      "Epoch 19 | Batch 51/94 | Batch Loss: 3.2361\n",
      "Epoch 19 | Batch 52/94 | Batch Loss: 3.2807\n",
      "Epoch 19 | Batch 53/94 | Batch Loss: 3.1920\n",
      "Epoch 19 | Batch 54/94 | Batch Loss: 3.1060\n",
      "Epoch 19 | Batch 55/94 | Batch Loss: 3.1833\n",
      "Epoch 19 | Batch 56/94 | Batch Loss: 3.1904\n",
      "Epoch 19 | Batch 57/94 | Batch Loss: 3.0915\n",
      "Epoch 19 | Batch 58/94 | Batch Loss: 3.1686\n",
      "Epoch 19 | Batch 59/94 | Batch Loss: 3.0926\n",
      "Epoch 19 | Batch 60/94 | Batch Loss: 3.1053\n",
      "Epoch 19 | Batch 61/94 | Batch Loss: 3.1043\n",
      "Epoch 19 | Batch 62/94 | Batch Loss: 3.1087\n",
      "Epoch 19 | Batch 63/94 | Batch Loss: 3.2330\n",
      "Epoch 19 | Batch 64/94 | Batch Loss: 3.0922\n",
      "Epoch 19 | Batch 65/94 | Batch Loss: 3.0787\n",
      "Epoch 19 | Batch 66/94 | Batch Loss: 3.1022\n",
      "Epoch 19 | Batch 67/94 | Batch Loss: 3.2623\n",
      "Epoch 19 | Batch 68/94 | Batch Loss: 3.1402\n",
      "Epoch 19 | Batch 69/94 | Batch Loss: 3.2188\n",
      "Epoch 19 | Batch 70/94 | Batch Loss: 2.9647\n",
      "Epoch 19 | Batch 71/94 | Batch Loss: 3.0708\n",
      "Epoch 19 | Batch 72/94 | Batch Loss: 3.1724\n",
      "Epoch 19 | Batch 73/94 | Batch Loss: 3.0707\n",
      "Epoch 19 | Batch 74/94 | Batch Loss: 3.2874\n",
      "Epoch 19 | Batch 75/94 | Batch Loss: 3.1665\n",
      "Epoch 19 | Batch 76/94 | Batch Loss: 3.1134\n",
      "Epoch 19 | Batch 77/94 | Batch Loss: 3.2538\n",
      "Epoch 19 | Batch 78/94 | Batch Loss: 3.1854\n",
      "Epoch 19 | Batch 79/94 | Batch Loss: 3.1723\n",
      "Epoch 19 | Batch 80/94 | Batch Loss: 3.2012\n",
      "Epoch 19 | Batch 81/94 | Batch Loss: 3.1306\n",
      "Epoch 19 | Batch 82/94 | Batch Loss: 3.1713\n",
      "Epoch 19 | Batch 83/94 | Batch Loss: 3.0841\n",
      "Epoch 19 | Batch 84/94 | Batch Loss: 3.1916\n",
      "Epoch 19 | Batch 85/94 | Batch Loss: 3.1234\n",
      "Epoch 19 | Batch 86/94 | Batch Loss: 3.2187\n",
      "Epoch 19 | Batch 87/94 | Batch Loss: 3.0201\n",
      "Epoch 19 | Batch 88/94 | Batch Loss: 3.2300\n",
      "Epoch 19 | Batch 89/94 | Batch Loss: 3.1222\n",
      "Epoch 19 | Batch 90/94 | Batch Loss: 3.1649\n",
      "Epoch 19 | Batch 91/94 | Batch Loss: 3.0634\n",
      "Epoch 19 | Batch 92/94 | Batch Loss: 3.1303\n",
      "Epoch 19 | Batch 93/94 | Batch Loss: 3.1542\n",
      "Epoch 19 | Batch 94/94 | Batch Loss: 3.1828\n",
      "Epoch 19 Finished | Average Loss: 3.1618\n",
      "\n",
      "Epoch 20 Starting...\n",
      "Epoch 20 | Batch 1/94 | Batch Loss: 3.1683\n",
      "Epoch 20 | Batch 2/94 | Batch Loss: 3.0553\n",
      "Epoch 20 | Batch 3/94 | Batch Loss: 3.1147\n",
      "Epoch 20 | Batch 4/94 | Batch Loss: 3.2205\n",
      "Epoch 20 | Batch 5/94 | Batch Loss: 3.1179\n",
      "Epoch 20 | Batch 6/94 | Batch Loss: 3.0435\n",
      "Epoch 20 | Batch 7/94 | Batch Loss: 3.0856\n",
      "Epoch 20 | Batch 8/94 | Batch Loss: 3.1116\n",
      "Epoch 20 | Batch 9/94 | Batch Loss: 3.0793\n",
      "Epoch 20 | Batch 10/94 | Batch Loss: 3.0492\n",
      "Epoch 20 | Batch 11/94 | Batch Loss: 3.0654\n",
      "Epoch 20 | Batch 12/94 | Batch Loss: 3.0544\n",
      "Epoch 20 | Batch 13/94 | Batch Loss: 3.1231\n",
      "Epoch 20 | Batch 14/94 | Batch Loss: 3.0998\n",
      "Epoch 20 | Batch 15/94 | Batch Loss: 3.0898\n",
      "Epoch 20 | Batch 16/94 | Batch Loss: 3.0971\n",
      "Epoch 20 | Batch 17/94 | Batch Loss: 3.0994\n",
      "Epoch 20 | Batch 18/94 | Batch Loss: 3.2060\n",
      "Epoch 20 | Batch 19/94 | Batch Loss: 3.0005\n",
      "Epoch 20 | Batch 20/94 | Batch Loss: 3.0922\n",
      "Epoch 20 | Batch 21/94 | Batch Loss: 3.0534\n",
      "Epoch 20 | Batch 22/94 | Batch Loss: 3.1684\n",
      "Epoch 20 | Batch 23/94 | Batch Loss: 3.1230\n",
      "Epoch 20 | Batch 24/94 | Batch Loss: 3.0816\n",
      "Epoch 20 | Batch 25/94 | Batch Loss: 3.0851\n",
      "Epoch 20 | Batch 26/94 | Batch Loss: 3.1091\n",
      "Epoch 20 | Batch 27/94 | Batch Loss: 3.1051\n",
      "Epoch 20 | Batch 28/94 | Batch Loss: 3.1679\n",
      "Epoch 20 | Batch 29/94 | Batch Loss: 3.1149\n",
      "Epoch 20 | Batch 30/94 | Batch Loss: 3.1120\n",
      "Epoch 20 | Batch 31/94 | Batch Loss: 3.0455\n",
      "Epoch 20 | Batch 32/94 | Batch Loss: 3.0988\n",
      "Epoch 20 | Batch 33/94 | Batch Loss: 3.0828\n",
      "Epoch 20 | Batch 34/94 | Batch Loss: 3.1727\n",
      "Epoch 20 | Batch 35/94 | Batch Loss: 3.0625\n",
      "Epoch 20 | Batch 36/94 | Batch Loss: 3.1326\n",
      "Epoch 20 | Batch 37/94 | Batch Loss: 3.1225\n",
      "Epoch 20 | Batch 38/94 | Batch Loss: 3.1508\n",
      "Epoch 20 | Batch 39/94 | Batch Loss: 3.2194\n",
      "Epoch 20 | Batch 40/94 | Batch Loss: 3.0895\n",
      "Epoch 20 | Batch 41/94 | Batch Loss: 3.1825\n",
      "Epoch 20 | Batch 42/94 | Batch Loss: 3.0245\n",
      "Epoch 20 | Batch 43/94 | Batch Loss: 3.1729\n",
      "Epoch 20 | Batch 44/94 | Batch Loss: 3.0336\n",
      "Epoch 20 | Batch 45/94 | Batch Loss: 3.0541\n",
      "Epoch 20 | Batch 46/94 | Batch Loss: 3.1740\n",
      "Epoch 20 | Batch 47/94 | Batch Loss: 3.1294\n",
      "Epoch 20 | Batch 48/94 | Batch Loss: 3.1957\n",
      "Epoch 20 | Batch 49/94 | Batch Loss: 3.0615\n",
      "Epoch 20 | Batch 50/94 | Batch Loss: 3.0177\n",
      "Epoch 20 | Batch 51/94 | Batch Loss: 3.1098\n",
      "Epoch 20 | Batch 52/94 | Batch Loss: 3.1063\n",
      "Epoch 20 | Batch 53/94 | Batch Loss: 3.2135\n",
      "Epoch 20 | Batch 54/94 | Batch Loss: 3.0685\n",
      "Epoch 20 | Batch 55/94 | Batch Loss: 3.1147\n",
      "Epoch 20 | Batch 56/94 | Batch Loss: 3.2407\n",
      "Epoch 20 | Batch 57/94 | Batch Loss: 3.2382\n",
      "Epoch 20 | Batch 58/94 | Batch Loss: 3.1457\n",
      "Epoch 20 | Batch 59/94 | Batch Loss: 3.1336\n",
      "Epoch 20 | Batch 60/94 | Batch Loss: 3.1456\n",
      "Epoch 20 | Batch 61/94 | Batch Loss: 3.1090\n",
      "Epoch 20 | Batch 62/94 | Batch Loss: 3.1003\n",
      "Epoch 20 | Batch 63/94 | Batch Loss: 3.2404\n",
      "Epoch 20 | Batch 64/94 | Batch Loss: 3.1967\n",
      "Epoch 20 | Batch 65/94 | Batch Loss: 3.1478\n",
      "Epoch 20 | Batch 66/94 | Batch Loss: 3.1694\n",
      "Epoch 20 | Batch 67/94 | Batch Loss: 3.1076\n",
      "Epoch 20 | Batch 68/94 | Batch Loss: 3.0808\n",
      "Epoch 20 | Batch 69/94 | Batch Loss: 3.1247\n",
      "Epoch 20 | Batch 70/94 | Batch Loss: 3.0124\n",
      "Epoch 20 | Batch 71/94 | Batch Loss: 3.0643\n",
      "Epoch 20 | Batch 72/94 | Batch Loss: 3.0651\n",
      "Epoch 20 | Batch 73/94 | Batch Loss: 3.1138\n",
      "Epoch 20 | Batch 74/94 | Batch Loss: 3.1302\n",
      "Epoch 20 | Batch 75/94 | Batch Loss: 3.2051\n",
      "Epoch 20 | Batch 76/94 | Batch Loss: 3.0974\n",
      "Epoch 20 | Batch 77/94 | Batch Loss: 3.1165\n",
      "Epoch 20 | Batch 78/94 | Batch Loss: 3.0120\n",
      "Epoch 20 | Batch 79/94 | Batch Loss: 3.2351\n",
      "Epoch 20 | Batch 80/94 | Batch Loss: 3.2545\n",
      "Epoch 20 | Batch 81/94 | Batch Loss: 3.1046\n",
      "Epoch 20 | Batch 82/94 | Batch Loss: 3.0592\n",
      "Epoch 20 | Batch 83/94 | Batch Loss: 3.1691\n",
      "Epoch 20 | Batch 84/94 | Batch Loss: 3.0323\n",
      "Epoch 20 | Batch 85/94 | Batch Loss: 3.1698\n",
      "Epoch 20 | Batch 86/94 | Batch Loss: 3.1876\n",
      "Epoch 20 | Batch 87/94 | Batch Loss: 3.1211\n",
      "Epoch 20 | Batch 88/94 | Batch Loss: 3.0809\n",
      "Epoch 20 | Batch 89/94 | Batch Loss: 3.0813\n",
      "Epoch 20 | Batch 90/94 | Batch Loss: 3.1860\n",
      "Epoch 20 | Batch 91/94 | Batch Loss: 3.0476\n",
      "Epoch 20 | Batch 92/94 | Batch Loss: 3.1816\n",
      "Epoch 20 | Batch 93/94 | Batch Loss: 3.1653\n",
      "Epoch 20 | Batch 94/94 | Batch Loss: 3.1373\n",
      "Epoch 20 Finished | Average Loss: 3.1185\n",
      "\n",
      "Epoch 21 Starting...\n",
      "Epoch 21 | Batch 1/94 | Batch Loss: 3.2062\n",
      "Epoch 21 | Batch 2/94 | Batch Loss: 3.1190\n",
      "Epoch 21 | Batch 3/94 | Batch Loss: 3.0836\n",
      "Epoch 21 | Batch 4/94 | Batch Loss: 3.0617\n",
      "Epoch 21 | Batch 5/94 | Batch Loss: 3.0678\n",
      "Epoch 21 | Batch 6/94 | Batch Loss: 3.0055\n",
      "Epoch 21 | Batch 7/94 | Batch Loss: 3.1049\n",
      "Epoch 21 | Batch 8/94 | Batch Loss: 3.0869\n",
      "Epoch 21 | Batch 9/94 | Batch Loss: 3.0448\n",
      "Epoch 21 | Batch 10/94 | Batch Loss: 3.0578\n",
      "Epoch 21 | Batch 11/94 | Batch Loss: 3.1259\n",
      "Epoch 21 | Batch 12/94 | Batch Loss: 3.1068\n",
      "Epoch 21 | Batch 13/94 | Batch Loss: 3.1471\n",
      "Epoch 21 | Batch 14/94 | Batch Loss: 3.1857\n",
      "Epoch 21 | Batch 15/94 | Batch Loss: 3.0336\n",
      "Epoch 21 | Batch 16/94 | Batch Loss: 3.0563\n",
      "Epoch 21 | Batch 17/94 | Batch Loss: 3.0362\n",
      "Epoch 21 | Batch 18/94 | Batch Loss: 3.0738\n",
      "Epoch 21 | Batch 19/94 | Batch Loss: 3.0974\n",
      "Epoch 21 | Batch 20/94 | Batch Loss: 2.9884\n",
      "Epoch 21 | Batch 21/94 | Batch Loss: 3.1000\n",
      "Epoch 21 | Batch 22/94 | Batch Loss: 3.0629\n",
      "Epoch 21 | Batch 23/94 | Batch Loss: 3.1086\n",
      "Epoch 21 | Batch 24/94 | Batch Loss: 3.1040\n",
      "Epoch 21 | Batch 25/94 | Batch Loss: 3.2071\n",
      "Epoch 21 | Batch 26/94 | Batch Loss: 3.0691\n",
      "Epoch 21 | Batch 27/94 | Batch Loss: 3.0503\n",
      "Epoch 21 | Batch 28/94 | Batch Loss: 3.1704\n",
      "Epoch 21 | Batch 29/94 | Batch Loss: 3.0512\n",
      "Epoch 21 | Batch 30/94 | Batch Loss: 3.0246\n",
      "Epoch 21 | Batch 31/94 | Batch Loss: 3.0819\n",
      "Epoch 21 | Batch 32/94 | Batch Loss: 3.1587\n",
      "Epoch 21 | Batch 33/94 | Batch Loss: 3.0919\n",
      "Epoch 21 | Batch 34/94 | Batch Loss: 2.9577\n",
      "Epoch 21 | Batch 35/94 | Batch Loss: 3.0817\n",
      "Epoch 21 | Batch 36/94 | Batch Loss: 3.1560\n",
      "Epoch 21 | Batch 37/94 | Batch Loss: 2.9859\n",
      "Epoch 21 | Batch 38/94 | Batch Loss: 3.0384\n",
      "Epoch 21 | Batch 39/94 | Batch Loss: 3.0954\n",
      "Epoch 21 | Batch 40/94 | Batch Loss: 3.0518\n",
      "Epoch 21 | Batch 41/94 | Batch Loss: 3.1131\n",
      "Epoch 21 | Batch 42/94 | Batch Loss: 3.0421\n",
      "Epoch 21 | Batch 43/94 | Batch Loss: 2.9639\n",
      "Epoch 21 | Batch 44/94 | Batch Loss: 3.0998\n",
      "Epoch 21 | Batch 45/94 | Batch Loss: 3.0565\n",
      "Epoch 21 | Batch 46/94 | Batch Loss: 3.0925\n",
      "Epoch 21 | Batch 47/94 | Batch Loss: 3.0459\n",
      "Epoch 21 | Batch 48/94 | Batch Loss: 3.0669\n",
      "Epoch 21 | Batch 49/94 | Batch Loss: 3.1679\n",
      "Epoch 21 | Batch 50/94 | Batch Loss: 3.1858\n",
      "Epoch 21 | Batch 51/94 | Batch Loss: 3.2206\n",
      "Epoch 21 | Batch 52/94 | Batch Loss: 3.0885\n",
      "Epoch 21 | Batch 53/94 | Batch Loss: 3.1702\n",
      "Epoch 21 | Batch 54/94 | Batch Loss: 3.1209\n",
      "Epoch 21 | Batch 55/94 | Batch Loss: 3.1646\n",
      "Epoch 21 | Batch 56/94 | Batch Loss: 3.1436\n",
      "Epoch 21 | Batch 57/94 | Batch Loss: 3.0013\n",
      "Epoch 21 | Batch 58/94 | Batch Loss: 3.1191\n",
      "Epoch 21 | Batch 59/94 | Batch Loss: 3.1448\n",
      "Epoch 21 | Batch 60/94 | Batch Loss: 2.9780\n",
      "Epoch 21 | Batch 61/94 | Batch Loss: 2.9875\n",
      "Epoch 21 | Batch 62/94 | Batch Loss: 3.0079\n",
      "Epoch 21 | Batch 63/94 | Batch Loss: 2.9475\n",
      "Epoch 21 | Batch 64/94 | Batch Loss: 3.1007\n",
      "Epoch 21 | Batch 65/94 | Batch Loss: 3.1342\n",
      "Epoch 21 | Batch 66/94 | Batch Loss: 3.0111\n",
      "Epoch 21 | Batch 67/94 | Batch Loss: 3.0899\n",
      "Epoch 21 | Batch 68/94 | Batch Loss: 3.1115\n",
      "Epoch 21 | Batch 69/94 | Batch Loss: 3.0300\n",
      "Epoch 21 | Batch 70/94 | Batch Loss: 2.9653\n",
      "Epoch 21 | Batch 71/94 | Batch Loss: 3.1480\n",
      "Epoch 21 | Batch 72/94 | Batch Loss: 3.0522\n",
      "Epoch 21 | Batch 73/94 | Batch Loss: 3.0218\n",
      "Epoch 21 | Batch 74/94 | Batch Loss: 3.1126\n",
      "Epoch 21 | Batch 75/94 | Batch Loss: 3.0284\n",
      "Epoch 21 | Batch 76/94 | Batch Loss: 3.0123\n",
      "Epoch 21 | Batch 77/94 | Batch Loss: 3.1541\n",
      "Epoch 21 | Batch 78/94 | Batch Loss: 3.1749\n",
      "Epoch 21 | Batch 79/94 | Batch Loss: 3.1669\n",
      "Epoch 21 | Batch 80/94 | Batch Loss: 2.9980\n",
      "Epoch 21 | Batch 81/94 | Batch Loss: 3.0907\n",
      "Epoch 21 | Batch 82/94 | Batch Loss: 3.0300\n",
      "Epoch 21 | Batch 83/94 | Batch Loss: 3.1056\n",
      "Epoch 21 | Batch 84/94 | Batch Loss: 3.0531\n",
      "Epoch 21 | Batch 85/94 | Batch Loss: 3.0682\n",
      "Epoch 21 | Batch 86/94 | Batch Loss: 3.1054\n",
      "Epoch 21 | Batch 87/94 | Batch Loss: 3.0347\n",
      "Epoch 21 | Batch 88/94 | Batch Loss: 3.0776\n",
      "Epoch 21 | Batch 89/94 | Batch Loss: 3.1232\n",
      "Epoch 21 | Batch 90/94 | Batch Loss: 3.0725\n",
      "Epoch 21 | Batch 91/94 | Batch Loss: 3.0357\n",
      "Epoch 21 | Batch 92/94 | Batch Loss: 2.9818\n",
      "Epoch 21 | Batch 93/94 | Batch Loss: 3.0870\n",
      "Epoch 21 | Batch 94/94 | Batch Loss: 3.1342\n",
      "Epoch 21 Finished | Average Loss: 3.0806\n",
      "\n",
      "Epoch 22 Starting...\n",
      "Epoch 22 | Batch 1/94 | Batch Loss: 2.9744\n",
      "Epoch 22 | Batch 2/94 | Batch Loss: 3.1238\n",
      "Epoch 22 | Batch 3/94 | Batch Loss: 2.9792\n",
      "Epoch 22 | Batch 4/94 | Batch Loss: 2.9738\n",
      "Epoch 22 | Batch 5/94 | Batch Loss: 3.1540\n",
      "Epoch 22 | Batch 6/94 | Batch Loss: 3.1197\n",
      "Epoch 22 | Batch 7/94 | Batch Loss: 2.9698\n",
      "Epoch 22 | Batch 8/94 | Batch Loss: 3.0357\n",
      "Epoch 22 | Batch 9/94 | Batch Loss: 3.0654\n",
      "Epoch 22 | Batch 10/94 | Batch Loss: 3.1073\n",
      "Epoch 22 | Batch 11/94 | Batch Loss: 2.9951\n",
      "Epoch 22 | Batch 12/94 | Batch Loss: 2.8879\n",
      "Epoch 22 | Batch 13/94 | Batch Loss: 2.9832\n",
      "Epoch 22 | Batch 14/94 | Batch Loss: 3.0669\n",
      "Epoch 22 | Batch 15/94 | Batch Loss: 2.9808\n",
      "Epoch 22 | Batch 16/94 | Batch Loss: 3.1315\n",
      "Epoch 22 | Batch 17/94 | Batch Loss: 3.0072\n",
      "Epoch 22 | Batch 18/94 | Batch Loss: 2.9810\n",
      "Epoch 22 | Batch 19/94 | Batch Loss: 3.1114\n",
      "Epoch 22 | Batch 20/94 | Batch Loss: 3.0068\n",
      "Epoch 22 | Batch 21/94 | Batch Loss: 3.0746\n",
      "Epoch 22 | Batch 22/94 | Batch Loss: 2.9878\n",
      "Epoch 22 | Batch 23/94 | Batch Loss: 3.0594\n",
      "Epoch 22 | Batch 24/94 | Batch Loss: 3.0551\n",
      "Epoch 22 | Batch 25/94 | Batch Loss: 3.0369\n",
      "Epoch 22 | Batch 26/94 | Batch Loss: 2.9760\n",
      "Epoch 22 | Batch 27/94 | Batch Loss: 3.0896\n",
      "Epoch 22 | Batch 28/94 | Batch Loss: 2.9919\n",
      "Epoch 22 | Batch 29/94 | Batch Loss: 3.0304\n",
      "Epoch 22 | Batch 30/94 | Batch Loss: 3.0931\n",
      "Epoch 22 | Batch 31/94 | Batch Loss: 2.9740\n",
      "Epoch 22 | Batch 32/94 | Batch Loss: 2.9636\n",
      "Epoch 22 | Batch 33/94 | Batch Loss: 3.0914\n",
      "Epoch 22 | Batch 34/94 | Batch Loss: 3.0290\n",
      "Epoch 22 | Batch 35/94 | Batch Loss: 3.0172\n",
      "Epoch 22 | Batch 36/94 | Batch Loss: 3.1625\n",
      "Epoch 22 | Batch 37/94 | Batch Loss: 3.0042\n",
      "Epoch 22 | Batch 38/94 | Batch Loss: 3.0471\n",
      "Epoch 22 | Batch 39/94 | Batch Loss: 3.0119\n",
      "Epoch 22 | Batch 40/94 | Batch Loss: 3.1769\n",
      "Epoch 22 | Batch 41/94 | Batch Loss: 3.0635\n",
      "Epoch 22 | Batch 42/94 | Batch Loss: 3.0347\n",
      "Epoch 22 | Batch 43/94 | Batch Loss: 3.0376\n",
      "Epoch 22 | Batch 44/94 | Batch Loss: 2.9843\n",
      "Epoch 22 | Batch 45/94 | Batch Loss: 3.1233\n",
      "Epoch 22 | Batch 46/94 | Batch Loss: 3.0173\n",
      "Epoch 22 | Batch 47/94 | Batch Loss: 2.9668\n",
      "Epoch 22 | Batch 48/94 | Batch Loss: 2.9760\n",
      "Epoch 22 | Batch 49/94 | Batch Loss: 3.0381\n",
      "Epoch 22 | Batch 50/94 | Batch Loss: 3.0767\n",
      "Epoch 22 | Batch 51/94 | Batch Loss: 3.1193\n",
      "Epoch 22 | Batch 52/94 | Batch Loss: 2.9458\n",
      "Epoch 22 | Batch 53/94 | Batch Loss: 3.0494\n",
      "Epoch 22 | Batch 54/94 | Batch Loss: 3.0388\n",
      "Epoch 22 | Batch 55/94 | Batch Loss: 3.0274\n",
      "Epoch 22 | Batch 56/94 | Batch Loss: 3.0471\n",
      "Epoch 22 | Batch 57/94 | Batch Loss: 2.9644\n",
      "Epoch 22 | Batch 58/94 | Batch Loss: 3.0583\n",
      "Epoch 22 | Batch 59/94 | Batch Loss: 3.0758\n",
      "Epoch 22 | Batch 60/94 | Batch Loss: 3.0799\n",
      "Epoch 22 | Batch 61/94 | Batch Loss: 3.0830\n",
      "Epoch 22 | Batch 62/94 | Batch Loss: 3.0283\n",
      "Epoch 22 | Batch 63/94 | Batch Loss: 3.0575\n",
      "Epoch 22 | Batch 64/94 | Batch Loss: 3.0077\n",
      "Epoch 22 | Batch 65/94 | Batch Loss: 2.9501\n",
      "Epoch 22 | Batch 66/94 | Batch Loss: 3.0526\n",
      "Epoch 22 | Batch 67/94 | Batch Loss: 3.0407\n",
      "Epoch 22 | Batch 68/94 | Batch Loss: 3.0821\n",
      "Epoch 22 | Batch 69/94 | Batch Loss: 2.9864\n",
      "Epoch 22 | Batch 70/94 | Batch Loss: 3.1021\n",
      "Epoch 22 | Batch 71/94 | Batch Loss: 3.0660\n",
      "Epoch 22 | Batch 72/94 | Batch Loss: 3.1645\n",
      "Epoch 22 | Batch 73/94 | Batch Loss: 3.0175\n",
      "Epoch 22 | Batch 74/94 | Batch Loss: 3.1452\n",
      "Epoch 22 | Batch 75/94 | Batch Loss: 3.1349\n",
      "Epoch 22 | Batch 76/94 | Batch Loss: 3.0146\n",
      "Epoch 22 | Batch 77/94 | Batch Loss: 3.0459\n",
      "Epoch 22 | Batch 78/94 | Batch Loss: 3.0426\n",
      "Epoch 22 | Batch 79/94 | Batch Loss: 3.0212\n",
      "Epoch 22 | Batch 80/94 | Batch Loss: 3.0713\n",
      "Epoch 22 | Batch 81/94 | Batch Loss: 3.0996\n",
      "Epoch 22 | Batch 82/94 | Batch Loss: 2.9275\n",
      "Epoch 22 | Batch 83/94 | Batch Loss: 3.0213\n",
      "Epoch 22 | Batch 84/94 | Batch Loss: 2.9525\n",
      "Epoch 22 | Batch 85/94 | Batch Loss: 2.9218\n",
      "Epoch 22 | Batch 86/94 | Batch Loss: 3.0272\n",
      "Epoch 22 | Batch 87/94 | Batch Loss: 3.1243\n",
      "Epoch 22 | Batch 88/94 | Batch Loss: 3.0466\n",
      "Epoch 22 | Batch 89/94 | Batch Loss: 3.0339\n",
      "Epoch 22 | Batch 90/94 | Batch Loss: 2.9537\n",
      "Epoch 22 | Batch 91/94 | Batch Loss: 3.1016\n",
      "Epoch 22 | Batch 92/94 | Batch Loss: 3.1248\n",
      "Epoch 22 | Batch 93/94 | Batch Loss: 2.9995\n",
      "Epoch 22 | Batch 94/94 | Batch Loss: 3.1234\n",
      "Epoch 22 Finished | Average Loss: 3.0407\n",
      "\n",
      "Epoch 23 Starting...\n",
      "Epoch 23 | Batch 1/94 | Batch Loss: 3.0676\n",
      "Epoch 23 | Batch 2/94 | Batch Loss: 3.0162\n",
      "Epoch 23 | Batch 3/94 | Batch Loss: 2.9964\n",
      "Epoch 23 | Batch 4/94 | Batch Loss: 2.8812\n",
      "Epoch 23 | Batch 5/94 | Batch Loss: 3.0653\n",
      "Epoch 23 | Batch 6/94 | Batch Loss: 3.0099\n",
      "Epoch 23 | Batch 7/94 | Batch Loss: 3.0892\n",
      "Epoch 23 | Batch 8/94 | Batch Loss: 2.9103\n",
      "Epoch 23 | Batch 9/94 | Batch Loss: 2.9915\n",
      "Epoch 23 | Batch 10/94 | Batch Loss: 3.1041\n",
      "Epoch 23 | Batch 11/94 | Batch Loss: 3.0450\n",
      "Epoch 23 | Batch 12/94 | Batch Loss: 2.8527\n",
      "Epoch 23 | Batch 13/94 | Batch Loss: 3.0836\n",
      "Epoch 23 | Batch 14/94 | Batch Loss: 3.0647\n",
      "Epoch 23 | Batch 15/94 | Batch Loss: 2.9659\n",
      "Epoch 23 | Batch 16/94 | Batch Loss: 3.0527\n",
      "Epoch 23 | Batch 17/94 | Batch Loss: 3.0429\n",
      "Epoch 23 | Batch 18/94 | Batch Loss: 3.0684\n",
      "Epoch 23 | Batch 19/94 | Batch Loss: 3.0109\n",
      "Epoch 23 | Batch 20/94 | Batch Loss: 3.0174\n",
      "Epoch 23 | Batch 21/94 | Batch Loss: 2.8927\n",
      "Epoch 23 | Batch 22/94 | Batch Loss: 3.0054\n",
      "Epoch 23 | Batch 23/94 | Batch Loss: 2.9653\n",
      "Epoch 23 | Batch 24/94 | Batch Loss: 3.0307\n",
      "Epoch 23 | Batch 25/94 | Batch Loss: 2.9378\n",
      "Epoch 23 | Batch 26/94 | Batch Loss: 2.8699\n",
      "Epoch 23 | Batch 27/94 | Batch Loss: 3.0941\n",
      "Epoch 23 | Batch 28/94 | Batch Loss: 3.0350\n",
      "Epoch 23 | Batch 29/94 | Batch Loss: 3.0650\n",
      "Epoch 23 | Batch 30/94 | Batch Loss: 2.9918\n",
      "Epoch 23 | Batch 31/94 | Batch Loss: 2.7697\n",
      "Epoch 23 | Batch 32/94 | Batch Loss: 3.0182\n",
      "Epoch 23 | Batch 33/94 | Batch Loss: 2.9544\n",
      "Epoch 23 | Batch 34/94 | Batch Loss: 3.0908\n",
      "Epoch 23 | Batch 35/94 | Batch Loss: 3.1429\n",
      "Epoch 23 | Batch 36/94 | Batch Loss: 2.9507\n",
      "Epoch 23 | Batch 37/94 | Batch Loss: 3.0171\n",
      "Epoch 23 | Batch 38/94 | Batch Loss: 3.0354\n",
      "Epoch 23 | Batch 39/94 | Batch Loss: 2.9860\n",
      "Epoch 23 | Batch 40/94 | Batch Loss: 3.0526\n",
      "Epoch 23 | Batch 41/94 | Batch Loss: 2.8941\n",
      "Epoch 23 | Batch 42/94 | Batch Loss: 3.0822\n",
      "Epoch 23 | Batch 43/94 | Batch Loss: 2.9900\n",
      "Epoch 23 | Batch 44/94 | Batch Loss: 2.9794\n",
      "Epoch 23 | Batch 45/94 | Batch Loss: 2.9944\n",
      "Epoch 23 | Batch 46/94 | Batch Loss: 3.1056\n",
      "Epoch 23 | Batch 47/94 | Batch Loss: 2.7964\n",
      "Epoch 23 | Batch 48/94 | Batch Loss: 2.8884\n",
      "Epoch 23 | Batch 49/94 | Batch Loss: 2.9929\n",
      "Epoch 23 | Batch 50/94 | Batch Loss: 3.0084\n",
      "Epoch 23 | Batch 51/94 | Batch Loss: 3.0210\n",
      "Epoch 23 | Batch 52/94 | Batch Loss: 3.1526\n",
      "Epoch 23 | Batch 53/94 | Batch Loss: 2.9628\n",
      "Epoch 23 | Batch 54/94 | Batch Loss: 3.0280\n",
      "Epoch 23 | Batch 55/94 | Batch Loss: 3.0103\n",
      "Epoch 23 | Batch 56/94 | Batch Loss: 2.9572\n",
      "Epoch 23 | Batch 57/94 | Batch Loss: 2.8711\n",
      "Epoch 23 | Batch 58/94 | Batch Loss: 2.9203\n",
      "Epoch 23 | Batch 59/94 | Batch Loss: 2.9552\n",
      "Epoch 23 | Batch 60/94 | Batch Loss: 3.1596\n",
      "Epoch 23 | Batch 61/94 | Batch Loss: 3.0386\n",
      "Epoch 23 | Batch 62/94 | Batch Loss: 2.9749\n",
      "Epoch 23 | Batch 63/94 | Batch Loss: 2.9693\n",
      "Epoch 23 | Batch 64/94 | Batch Loss: 2.9629\n",
      "Epoch 23 | Batch 65/94 | Batch Loss: 2.9480\n",
      "Epoch 23 | Batch 66/94 | Batch Loss: 2.9091\n",
      "Epoch 23 | Batch 67/94 | Batch Loss: 2.8892\n",
      "Epoch 23 | Batch 68/94 | Batch Loss: 3.1174\n",
      "Epoch 23 | Batch 69/94 | Batch Loss: 2.9508\n",
      "Epoch 23 | Batch 70/94 | Batch Loss: 3.0373\n",
      "Epoch 23 | Batch 71/94 | Batch Loss: 2.9276\n",
      "Epoch 23 | Batch 72/94 | Batch Loss: 2.9126\n",
      "Epoch 23 | Batch 73/94 | Batch Loss: 2.8920\n",
      "Epoch 23 | Batch 74/94 | Batch Loss: 3.0957\n",
      "Epoch 23 | Batch 75/94 | Batch Loss: 2.9404\n",
      "Epoch 23 | Batch 76/94 | Batch Loss: 3.0714\n",
      "Epoch 23 | Batch 77/94 | Batch Loss: 3.0759\n",
      "Epoch 23 | Batch 78/94 | Batch Loss: 3.0145\n",
      "Epoch 23 | Batch 79/94 | Batch Loss: 3.0425\n",
      "Epoch 23 | Batch 80/94 | Batch Loss: 3.1243\n",
      "Epoch 23 | Batch 81/94 | Batch Loss: 2.8245\n",
      "Epoch 23 | Batch 82/94 | Batch Loss: 3.0932\n",
      "Epoch 23 | Batch 83/94 | Batch Loss: 3.1104\n",
      "Epoch 23 | Batch 84/94 | Batch Loss: 3.1273\n",
      "Epoch 23 | Batch 85/94 | Batch Loss: 3.0183\n",
      "Epoch 23 | Batch 86/94 | Batch Loss: 3.0540\n",
      "Epoch 23 | Batch 87/94 | Batch Loss: 2.9435\n",
      "Epoch 23 | Batch 88/94 | Batch Loss: 3.0656\n",
      "Epoch 23 | Batch 89/94 | Batch Loss: 3.0599\n",
      "Epoch 23 | Batch 90/94 | Batch Loss: 2.9869\n",
      "Epoch 23 | Batch 91/94 | Batch Loss: 3.0382\n",
      "Epoch 23 | Batch 92/94 | Batch Loss: 2.9554\n",
      "Epoch 23 | Batch 93/94 | Batch Loss: 2.9412\n",
      "Epoch 23 | Batch 94/94 | Batch Loss: 2.9710\n",
      "Epoch 23 Finished | Average Loss: 3.0010\n",
      "\n",
      "Epoch 24 Starting...\n",
      "Epoch 24 | Batch 1/94 | Batch Loss: 2.9298\n",
      "Epoch 24 | Batch 2/94 | Batch Loss: 2.9920\n",
      "Epoch 24 | Batch 3/94 | Batch Loss: 3.0737\n",
      "Epoch 24 | Batch 4/94 | Batch Loss: 2.9880\n",
      "Epoch 24 | Batch 5/94 | Batch Loss: 2.9748\n",
      "Epoch 24 | Batch 6/94 | Batch Loss: 2.9632\n",
      "Epoch 24 | Batch 7/94 | Batch Loss: 3.0505\n",
      "Epoch 24 | Batch 8/94 | Batch Loss: 3.0176\n",
      "Epoch 24 | Batch 9/94 | Batch Loss: 2.9655\n",
      "Epoch 24 | Batch 10/94 | Batch Loss: 2.9313\n",
      "Epoch 24 | Batch 11/94 | Batch Loss: 2.8723\n",
      "Epoch 24 | Batch 12/94 | Batch Loss: 2.8914\n",
      "Epoch 24 | Batch 13/94 | Batch Loss: 2.9693\n",
      "Epoch 24 | Batch 14/94 | Batch Loss: 2.9789\n",
      "Epoch 24 | Batch 15/94 | Batch Loss: 2.9712\n",
      "Epoch 24 | Batch 16/94 | Batch Loss: 3.1207\n",
      "Epoch 24 | Batch 17/94 | Batch Loss: 3.0748\n",
      "Epoch 24 | Batch 18/94 | Batch Loss: 3.0133\n",
      "Epoch 24 | Batch 19/94 | Batch Loss: 2.9578\n",
      "Epoch 24 | Batch 20/94 | Batch Loss: 2.9398\n",
      "Epoch 24 | Batch 21/94 | Batch Loss: 2.9846\n",
      "Epoch 24 | Batch 22/94 | Batch Loss: 2.9126\n",
      "Epoch 24 | Batch 23/94 | Batch Loss: 3.1076\n",
      "Epoch 24 | Batch 24/94 | Batch Loss: 3.0150\n",
      "Epoch 24 | Batch 25/94 | Batch Loss: 2.9638\n",
      "Epoch 24 | Batch 26/94 | Batch Loss: 3.0064\n",
      "Epoch 24 | Batch 27/94 | Batch Loss: 2.9813\n",
      "Epoch 24 | Batch 28/94 | Batch Loss: 2.9895\n",
      "Epoch 24 | Batch 29/94 | Batch Loss: 2.9243\n",
      "Epoch 24 | Batch 30/94 | Batch Loss: 2.9288\n",
      "Epoch 24 | Batch 31/94 | Batch Loss: 2.9628\n",
      "Epoch 24 | Batch 32/94 | Batch Loss: 2.9649\n",
      "Epoch 24 | Batch 33/94 | Batch Loss: 2.9996\n",
      "Epoch 24 | Batch 34/94 | Batch Loss: 3.0531\n",
      "Epoch 24 | Batch 35/94 | Batch Loss: 2.8889\n",
      "Epoch 24 | Batch 36/94 | Batch Loss: 2.9964\n",
      "Epoch 24 | Batch 37/94 | Batch Loss: 2.9758\n",
      "Epoch 24 | Batch 38/94 | Batch Loss: 2.9549\n",
      "Epoch 24 | Batch 39/94 | Batch Loss: 3.0245\n",
      "Epoch 24 | Batch 40/94 | Batch Loss: 3.0400\n",
      "Epoch 24 | Batch 41/94 | Batch Loss: 3.0785\n",
      "Epoch 24 | Batch 42/94 | Batch Loss: 2.9198\n",
      "Epoch 24 | Batch 43/94 | Batch Loss: 2.9894\n",
      "Epoch 24 | Batch 44/94 | Batch Loss: 2.8401\n",
      "Epoch 24 | Batch 45/94 | Batch Loss: 2.9436\n",
      "Epoch 24 | Batch 46/94 | Batch Loss: 2.8690\n",
      "Epoch 24 | Batch 47/94 | Batch Loss: 3.0439\n",
      "Epoch 24 | Batch 48/94 | Batch Loss: 2.8652\n",
      "Epoch 24 | Batch 49/94 | Batch Loss: 2.9446\n",
      "Epoch 24 | Batch 50/94 | Batch Loss: 2.9125\n",
      "Epoch 24 | Batch 51/94 | Batch Loss: 2.8341\n",
      "Epoch 24 | Batch 52/94 | Batch Loss: 2.9514\n",
      "Epoch 24 | Batch 53/94 | Batch Loss: 2.8695\n",
      "Epoch 24 | Batch 54/94 | Batch Loss: 2.9429\n",
      "Epoch 24 | Batch 55/94 | Batch Loss: 2.8944\n",
      "Epoch 24 | Batch 56/94 | Batch Loss: 2.9556\n",
      "Epoch 24 | Batch 57/94 | Batch Loss: 2.9769\n",
      "Epoch 24 | Batch 58/94 | Batch Loss: 2.8797\n",
      "Epoch 24 | Batch 59/94 | Batch Loss: 2.9884\n",
      "Epoch 24 | Batch 60/94 | Batch Loss: 2.9316\n",
      "Epoch 24 | Batch 61/94 | Batch Loss: 2.9786\n",
      "Epoch 24 | Batch 62/94 | Batch Loss: 2.9351\n",
      "Epoch 24 | Batch 63/94 | Batch Loss: 3.0655\n",
      "Epoch 24 | Batch 64/94 | Batch Loss: 3.0145\n",
      "Epoch 24 | Batch 65/94 | Batch Loss: 2.9452\n",
      "Epoch 24 | Batch 66/94 | Batch Loss: 2.8933\n",
      "Epoch 24 | Batch 67/94 | Batch Loss: 2.9249\n",
      "Epoch 24 | Batch 68/94 | Batch Loss: 2.8581\n",
      "Epoch 24 | Batch 69/94 | Batch Loss: 3.0422\n",
      "Epoch 24 | Batch 70/94 | Batch Loss: 3.0158\n",
      "Epoch 24 | Batch 71/94 | Batch Loss: 3.0075\n",
      "Epoch 24 | Batch 72/94 | Batch Loss: 2.9479\n",
      "Epoch 24 | Batch 73/94 | Batch Loss: 2.9003\n",
      "Epoch 24 | Batch 74/94 | Batch Loss: 2.9773\n",
      "Epoch 24 | Batch 75/94 | Batch Loss: 2.9224\n",
      "Epoch 24 | Batch 76/94 | Batch Loss: 3.0307\n",
      "Epoch 24 | Batch 77/94 | Batch Loss: 2.9745\n",
      "Epoch 24 | Batch 78/94 | Batch Loss: 2.9484\n",
      "Epoch 24 | Batch 79/94 | Batch Loss: 2.8611\n",
      "Epoch 24 | Batch 80/94 | Batch Loss: 2.9859\n",
      "Epoch 24 | Batch 81/94 | Batch Loss: 2.9880\n",
      "Epoch 24 | Batch 82/94 | Batch Loss: 2.9903\n",
      "Epoch 24 | Batch 83/94 | Batch Loss: 3.0290\n",
      "Epoch 24 | Batch 84/94 | Batch Loss: 2.8631\n",
      "Epoch 24 | Batch 85/94 | Batch Loss: 3.0167\n",
      "Epoch 24 | Batch 86/94 | Batch Loss: 2.9350\n",
      "Epoch 24 | Batch 87/94 | Batch Loss: 2.9210\n",
      "Epoch 24 | Batch 88/94 | Batch Loss: 2.9880\n",
      "Epoch 24 | Batch 89/94 | Batch Loss: 2.9388\n",
      "Epoch 24 | Batch 90/94 | Batch Loss: 2.8780\n",
      "Epoch 24 | Batch 91/94 | Batch Loss: 3.0205\n",
      "Epoch 24 | Batch 92/94 | Batch Loss: 2.9725\n",
      "Epoch 24 | Batch 93/94 | Batch Loss: 2.9941\n",
      "Epoch 24 | Batch 94/94 | Batch Loss: 3.0354\n",
      "Epoch 24 Finished | Average Loss: 2.9658\n",
      "\n",
      "Epoch 25 Starting...\n",
      "Epoch 25 | Batch 1/94 | Batch Loss: 2.9256\n",
      "Epoch 25 | Batch 2/94 | Batch Loss: 2.8306\n",
      "Epoch 25 | Batch 3/94 | Batch Loss: 2.9499\n",
      "Epoch 25 | Batch 4/94 | Batch Loss: 2.9818\n",
      "Epoch 25 | Batch 5/94 | Batch Loss: 2.9632\n",
      "Epoch 25 | Batch 6/94 | Batch Loss: 3.1025\n",
      "Epoch 25 | Batch 7/94 | Batch Loss: 2.9158\n",
      "Epoch 25 | Batch 8/94 | Batch Loss: 2.9612\n",
      "Epoch 25 | Batch 9/94 | Batch Loss: 3.0639\n",
      "Epoch 25 | Batch 10/94 | Batch Loss: 2.9845\n",
      "Epoch 25 | Batch 11/94 | Batch Loss: 2.8204\n",
      "Epoch 25 | Batch 12/94 | Batch Loss: 2.8635\n",
      "Epoch 25 | Batch 13/94 | Batch Loss: 2.8083\n",
      "Epoch 25 | Batch 14/94 | Batch Loss: 2.9133\n",
      "Epoch 25 | Batch 15/94 | Batch Loss: 2.9198\n",
      "Epoch 25 | Batch 16/94 | Batch Loss: 2.7688\n",
      "Epoch 25 | Batch 17/94 | Batch Loss: 2.8761\n",
      "Epoch 25 | Batch 18/94 | Batch Loss: 2.9258\n",
      "Epoch 25 | Batch 19/94 | Batch Loss: 2.8804\n",
      "Epoch 25 | Batch 20/94 | Batch Loss: 2.8551\n",
      "Epoch 25 | Batch 21/94 | Batch Loss: 2.9860\n",
      "Epoch 25 | Batch 22/94 | Batch Loss: 2.9479\n",
      "Epoch 25 | Batch 23/94 | Batch Loss: 3.0174\n",
      "Epoch 25 | Batch 24/94 | Batch Loss: 3.0201\n",
      "Epoch 25 | Batch 25/94 | Batch Loss: 2.8561\n",
      "Epoch 25 | Batch 26/94 | Batch Loss: 2.9766\n",
      "Epoch 25 | Batch 27/94 | Batch Loss: 3.0497\n",
      "Epoch 25 | Batch 28/94 | Batch Loss: 2.9551\n",
      "Epoch 25 | Batch 29/94 | Batch Loss: 2.8794\n",
      "Epoch 25 | Batch 30/94 | Batch Loss: 2.9988\n",
      "Epoch 25 | Batch 31/94 | Batch Loss: 2.9101\n",
      "Epoch 25 | Batch 32/94 | Batch Loss: 2.9392\n",
      "Epoch 25 | Batch 33/94 | Batch Loss: 2.9476\n",
      "Epoch 25 | Batch 34/94 | Batch Loss: 3.0274\n",
      "Epoch 25 | Batch 35/94 | Batch Loss: 2.9336\n",
      "Epoch 25 | Batch 36/94 | Batch Loss: 2.9361\n",
      "Epoch 25 | Batch 37/94 | Batch Loss: 2.9783\n",
      "Epoch 25 | Batch 38/94 | Batch Loss: 2.9422\n",
      "Epoch 25 | Batch 39/94 | Batch Loss: 2.9729\n",
      "Epoch 25 | Batch 40/94 | Batch Loss: 2.9454\n",
      "Epoch 25 | Batch 41/94 | Batch Loss: 3.0423\n",
      "Epoch 25 | Batch 42/94 | Batch Loss: 2.8923\n",
      "Epoch 25 | Batch 43/94 | Batch Loss: 2.8467\n",
      "Epoch 25 | Batch 44/94 | Batch Loss: 2.9372\n",
      "Epoch 25 | Batch 45/94 | Batch Loss: 2.8894\n",
      "Epoch 25 | Batch 46/94 | Batch Loss: 2.9008\n",
      "Epoch 25 | Batch 47/94 | Batch Loss: 2.9305\n",
      "Epoch 25 | Batch 48/94 | Batch Loss: 2.9567\n",
      "Epoch 25 | Batch 49/94 | Batch Loss: 2.9502\n",
      "Epoch 25 | Batch 50/94 | Batch Loss: 2.9201\n",
      "Epoch 25 | Batch 51/94 | Batch Loss: 2.9586\n",
      "Epoch 25 | Batch 52/94 | Batch Loss: 2.9285\n",
      "Epoch 25 | Batch 53/94 | Batch Loss: 2.9202\n",
      "Epoch 25 | Batch 54/94 | Batch Loss: 2.8467\n",
      "Epoch 25 | Batch 55/94 | Batch Loss: 2.9844\n",
      "Epoch 25 | Batch 56/94 | Batch Loss: 2.9133\n",
      "Epoch 25 | Batch 57/94 | Batch Loss: 2.8780\n",
      "Epoch 25 | Batch 58/94 | Batch Loss: 2.9919\n",
      "Epoch 25 | Batch 59/94 | Batch Loss: 3.0038\n",
      "Epoch 25 | Batch 60/94 | Batch Loss: 3.0305\n",
      "Epoch 25 | Batch 61/94 | Batch Loss: 2.9812\n",
      "Epoch 25 | Batch 62/94 | Batch Loss: 2.8588\n",
      "Epoch 25 | Batch 63/94 | Batch Loss: 2.8954\n",
      "Epoch 25 | Batch 64/94 | Batch Loss: 2.8339\n",
      "Epoch 25 | Batch 65/94 | Batch Loss: 2.9580\n",
      "Epoch 25 | Batch 66/94 | Batch Loss: 2.9794\n",
      "Epoch 25 | Batch 67/94 | Batch Loss: 2.9473\n",
      "Epoch 25 | Batch 68/94 | Batch Loss: 2.7813\n",
      "Epoch 25 | Batch 69/94 | Batch Loss: 2.9374\n",
      "Epoch 25 | Batch 70/94 | Batch Loss: 2.9271\n",
      "Epoch 25 | Batch 71/94 | Batch Loss: 2.9504\n",
      "Epoch 25 | Batch 72/94 | Batch Loss: 2.9319\n",
      "Epoch 25 | Batch 73/94 | Batch Loss: 2.7678\n",
      "Epoch 25 | Batch 74/94 | Batch Loss: 2.8965\n",
      "Epoch 25 | Batch 75/94 | Batch Loss: 2.9119\n",
      "Epoch 25 | Batch 76/94 | Batch Loss: 2.9295\n",
      "Epoch 25 | Batch 77/94 | Batch Loss: 2.9519\n",
      "Epoch 25 | Batch 78/94 | Batch Loss: 2.9554\n",
      "Epoch 25 | Batch 79/94 | Batch Loss: 3.1031\n",
      "Epoch 25 | Batch 80/94 | Batch Loss: 2.9385\n",
      "Epoch 25 | Batch 81/94 | Batch Loss: 2.8843\n",
      "Epoch 25 | Batch 82/94 | Batch Loss: 2.8975\n",
      "Epoch 25 | Batch 83/94 | Batch Loss: 2.9613\n",
      "Epoch 25 | Batch 84/94 | Batch Loss: 3.0347\n",
      "Epoch 25 | Batch 85/94 | Batch Loss: 2.8630\n",
      "Epoch 25 | Batch 86/94 | Batch Loss: 2.8763\n",
      "Epoch 25 | Batch 87/94 | Batch Loss: 2.8369\n",
      "Epoch 25 | Batch 88/94 | Batch Loss: 2.9890\n",
      "Epoch 25 | Batch 89/94 | Batch Loss: 2.9088\n",
      "Epoch 25 | Batch 90/94 | Batch Loss: 2.9332\n",
      "Epoch 25 | Batch 91/94 | Batch Loss: 2.8707\n",
      "Epoch 25 | Batch 92/94 | Batch Loss: 2.9430\n",
      "Epoch 25 | Batch 93/94 | Batch Loss: 2.8738\n",
      "Epoch 25 | Batch 94/94 | Batch Loss: 2.9961\n",
      "Epoch 25 Finished | Average Loss: 2.9315\n",
      "\n",
      "Epoch 26 Starting...\n",
      "Epoch 26 | Batch 1/94 | Batch Loss: 2.9606\n",
      "Epoch 26 | Batch 2/94 | Batch Loss: 2.9050\n",
      "Epoch 26 | Batch 3/94 | Batch Loss: 2.8562\n",
      "Epoch 26 | Batch 4/94 | Batch Loss: 2.8181\n",
      "Epoch 26 | Batch 5/94 | Batch Loss: 2.8694\n",
      "Epoch 26 | Batch 6/94 | Batch Loss: 2.9007\n",
      "Epoch 26 | Batch 7/94 | Batch Loss: 2.9084\n",
      "Epoch 26 | Batch 8/94 | Batch Loss: 3.0125\n",
      "Epoch 26 | Batch 9/94 | Batch Loss: 2.8866\n",
      "Epoch 26 | Batch 10/94 | Batch Loss: 2.9557\n",
      "Epoch 26 | Batch 11/94 | Batch Loss: 2.8309\n",
      "Epoch 26 | Batch 12/94 | Batch Loss: 2.9165\n",
      "Epoch 26 | Batch 13/94 | Batch Loss: 2.8502\n",
      "Epoch 26 | Batch 14/94 | Batch Loss: 2.9627\n",
      "Epoch 26 | Batch 15/94 | Batch Loss: 2.8235\n",
      "Epoch 26 | Batch 16/94 | Batch Loss: 2.8289\n",
      "Epoch 26 | Batch 17/94 | Batch Loss: 3.0100\n",
      "Epoch 26 | Batch 18/94 | Batch Loss: 2.8477\n",
      "Epoch 26 | Batch 19/94 | Batch Loss: 2.9371\n",
      "Epoch 26 | Batch 20/94 | Batch Loss: 2.8225\n",
      "Epoch 26 | Batch 21/94 | Batch Loss: 2.9533\n",
      "Epoch 26 | Batch 22/94 | Batch Loss: 2.9533\n",
      "Epoch 26 | Batch 23/94 | Batch Loss: 2.8796\n",
      "Epoch 26 | Batch 24/94 | Batch Loss: 3.0011\n",
      "Epoch 26 | Batch 25/94 | Batch Loss: 2.8680\n",
      "Epoch 26 | Batch 26/94 | Batch Loss: 2.8801\n",
      "Epoch 26 | Batch 27/94 | Batch Loss: 2.9071\n",
      "Epoch 26 | Batch 28/94 | Batch Loss: 2.8161\n",
      "Epoch 26 | Batch 29/94 | Batch Loss: 2.8507\n",
      "Epoch 26 | Batch 30/94 | Batch Loss: 2.8111\n",
      "Epoch 26 | Batch 31/94 | Batch Loss: 3.0134\n",
      "Epoch 26 | Batch 32/94 | Batch Loss: 2.8846\n",
      "Epoch 26 | Batch 33/94 | Batch Loss: 2.9343\n",
      "Epoch 26 | Batch 34/94 | Batch Loss: 2.7916\n",
      "Epoch 26 | Batch 35/94 | Batch Loss: 2.8615\n",
      "Epoch 26 | Batch 36/94 | Batch Loss: 2.9159\n",
      "Epoch 26 | Batch 37/94 | Batch Loss: 2.9140\n",
      "Epoch 26 | Batch 38/94 | Batch Loss: 2.9081\n",
      "Epoch 26 | Batch 39/94 | Batch Loss: 2.8521\n",
      "Epoch 26 | Batch 40/94 | Batch Loss: 2.9111\n",
      "Epoch 26 | Batch 41/94 | Batch Loss: 2.8482\n",
      "Epoch 26 | Batch 42/94 | Batch Loss: 3.0165\n",
      "Epoch 26 | Batch 43/94 | Batch Loss: 2.7838\n",
      "Epoch 26 | Batch 44/94 | Batch Loss: 2.9287\n",
      "Epoch 26 | Batch 45/94 | Batch Loss: 2.9193\n",
      "Epoch 26 | Batch 46/94 | Batch Loss: 2.9481\n",
      "Epoch 26 | Batch 47/94 | Batch Loss: 2.8516\n",
      "Epoch 26 | Batch 48/94 | Batch Loss: 2.8541\n",
      "Epoch 26 | Batch 49/94 | Batch Loss: 2.8554\n",
      "Epoch 26 | Batch 50/94 | Batch Loss: 2.8096\n",
      "Epoch 26 | Batch 51/94 | Batch Loss: 3.0381\n",
      "Epoch 26 | Batch 52/94 | Batch Loss: 2.8172\n",
      "Epoch 26 | Batch 53/94 | Batch Loss: 2.8795\n",
      "Epoch 26 | Batch 54/94 | Batch Loss: 2.8213\n",
      "Epoch 26 | Batch 55/94 | Batch Loss: 2.8599\n",
      "Epoch 26 | Batch 56/94 | Batch Loss: 3.0019\n",
      "Epoch 26 | Batch 57/94 | Batch Loss: 2.8208\n",
      "Epoch 26 | Batch 58/94 | Batch Loss: 3.0244\n",
      "Epoch 26 | Batch 59/94 | Batch Loss: 3.0098\n",
      "Epoch 26 | Batch 60/94 | Batch Loss: 2.9156\n",
      "Epoch 26 | Batch 61/94 | Batch Loss: 3.0308\n",
      "Epoch 26 | Batch 62/94 | Batch Loss: 2.9884\n",
      "Epoch 26 | Batch 63/94 | Batch Loss: 2.8656\n",
      "Epoch 26 | Batch 64/94 | Batch Loss: 2.9200\n",
      "Epoch 26 | Batch 65/94 | Batch Loss: 2.8660\n",
      "Epoch 26 | Batch 66/94 | Batch Loss: 2.8595\n",
      "Epoch 26 | Batch 67/94 | Batch Loss: 2.8207\n",
      "Epoch 26 | Batch 68/94 | Batch Loss: 2.9551\n",
      "Epoch 26 | Batch 69/94 | Batch Loss: 2.9309\n",
      "Epoch 26 | Batch 70/94 | Batch Loss: 2.8481\n",
      "Epoch 26 | Batch 71/94 | Batch Loss: 2.9852\n",
      "Epoch 26 | Batch 72/94 | Batch Loss: 2.9727\n",
      "Epoch 26 | Batch 73/94 | Batch Loss: 2.9449\n",
      "Epoch 26 | Batch 74/94 | Batch Loss: 2.9128\n",
      "Epoch 26 | Batch 75/94 | Batch Loss: 2.9383\n",
      "Epoch 26 | Batch 76/94 | Batch Loss: 2.8981\n",
      "Epoch 26 | Batch 77/94 | Batch Loss: 2.8509\n",
      "Epoch 26 | Batch 78/94 | Batch Loss: 2.8716\n",
      "Epoch 26 | Batch 79/94 | Batch Loss: 2.9547\n",
      "Epoch 26 | Batch 80/94 | Batch Loss: 2.7542\n",
      "Epoch 26 | Batch 81/94 | Batch Loss: 2.8100\n",
      "Epoch 26 | Batch 82/94 | Batch Loss: 2.8637\n",
      "Epoch 26 | Batch 83/94 | Batch Loss: 2.8909\n",
      "Epoch 26 | Batch 84/94 | Batch Loss: 2.9638\n",
      "Epoch 26 | Batch 85/94 | Batch Loss: 2.8690\n",
      "Epoch 26 | Batch 86/94 | Batch Loss: 2.8774\n",
      "Epoch 26 | Batch 87/94 | Batch Loss: 2.7778\n",
      "Epoch 26 | Batch 88/94 | Batch Loss: 2.7883\n",
      "Epoch 26 | Batch 89/94 | Batch Loss: 2.9291\n",
      "Epoch 26 | Batch 90/94 | Batch Loss: 2.8252\n",
      "Epoch 26 | Batch 91/94 | Batch Loss: 2.8882\n",
      "Epoch 26 | Batch 92/94 | Batch Loss: 2.9556\n",
      "Epoch 26 | Batch 93/94 | Batch Loss: 2.9393\n",
      "Epoch 26 | Batch 94/94 | Batch Loss: 2.8408\n",
      "Epoch 26 Finished | Average Loss: 2.8958\n",
      "\n",
      "Epoch 27 Starting...\n",
      "Epoch 27 | Batch 1/94 | Batch Loss: 2.8602\n",
      "Epoch 27 | Batch 2/94 | Batch Loss: 2.9104\n",
      "Epoch 27 | Batch 3/94 | Batch Loss: 2.8983\n",
      "Epoch 27 | Batch 4/94 | Batch Loss: 2.8729\n",
      "Epoch 27 | Batch 5/94 | Batch Loss: 2.9912\n",
      "Epoch 27 | Batch 6/94 | Batch Loss: 2.9017\n",
      "Epoch 27 | Batch 7/94 | Batch Loss: 2.8433\n",
      "Epoch 27 | Batch 8/94 | Batch Loss: 2.9158\n",
      "Epoch 27 | Batch 9/94 | Batch Loss: 2.8498\n",
      "Epoch 27 | Batch 10/94 | Batch Loss: 2.8725\n",
      "Epoch 27 | Batch 11/94 | Batch Loss: 2.9145\n",
      "Epoch 27 | Batch 12/94 | Batch Loss: 2.8514\n",
      "Epoch 27 | Batch 13/94 | Batch Loss: 2.9874\n",
      "Epoch 27 | Batch 14/94 | Batch Loss: 2.8791\n",
      "Epoch 27 | Batch 15/94 | Batch Loss: 2.9026\n",
      "Epoch 27 | Batch 16/94 | Batch Loss: 2.9208\n",
      "Epoch 27 | Batch 17/94 | Batch Loss: 2.8285\n",
      "Epoch 27 | Batch 18/94 | Batch Loss: 2.9505\n",
      "Epoch 27 | Batch 19/94 | Batch Loss: 2.8782\n",
      "Epoch 27 | Batch 20/94 | Batch Loss: 2.8508\n",
      "Epoch 27 | Batch 21/94 | Batch Loss: 2.8676\n",
      "Epoch 27 | Batch 22/94 | Batch Loss: 2.9239\n",
      "Epoch 27 | Batch 23/94 | Batch Loss: 2.9364\n",
      "Epoch 27 | Batch 24/94 | Batch Loss: 2.8743\n",
      "Epoch 27 | Batch 25/94 | Batch Loss: 2.8275\n",
      "Epoch 27 | Batch 26/94 | Batch Loss: 2.7882\n",
      "Epoch 27 | Batch 27/94 | Batch Loss: 2.9390\n",
      "Epoch 27 | Batch 28/94 | Batch Loss: 2.8466\n",
      "Epoch 27 | Batch 29/94 | Batch Loss: 2.7925\n",
      "Epoch 27 | Batch 30/94 | Batch Loss: 2.7832\n",
      "Epoch 27 | Batch 31/94 | Batch Loss: 2.9151\n",
      "Epoch 27 | Batch 32/94 | Batch Loss: 2.8701\n",
      "Epoch 27 | Batch 33/94 | Batch Loss: 2.8487\n",
      "Epoch 27 | Batch 34/94 | Batch Loss: 2.7652\n",
      "Epoch 27 | Batch 35/94 | Batch Loss: 2.8193\n",
      "Epoch 27 | Batch 36/94 | Batch Loss: 2.8845\n",
      "Epoch 27 | Batch 37/94 | Batch Loss: 2.8883\n",
      "Epoch 27 | Batch 38/94 | Batch Loss: 2.9537\n",
      "Epoch 27 | Batch 39/94 | Batch Loss: 2.8366\n",
      "Epoch 27 | Batch 40/94 | Batch Loss: 2.7386\n",
      "Epoch 27 | Batch 41/94 | Batch Loss: 2.8621\n",
      "Epoch 27 | Batch 42/94 | Batch Loss: 2.7676\n",
      "Epoch 27 | Batch 43/94 | Batch Loss: 2.8312\n",
      "Epoch 27 | Batch 44/94 | Batch Loss: 2.8417\n",
      "Epoch 27 | Batch 45/94 | Batch Loss: 2.9070\n",
      "Epoch 27 | Batch 46/94 | Batch Loss: 2.8058\n",
      "Epoch 27 | Batch 47/94 | Batch Loss: 2.9081\n",
      "Epoch 27 | Batch 48/94 | Batch Loss: 2.9278\n",
      "Epoch 27 | Batch 49/94 | Batch Loss: 2.8192\n",
      "Epoch 27 | Batch 50/94 | Batch Loss: 2.9350\n",
      "Epoch 27 | Batch 51/94 | Batch Loss: 2.7823\n",
      "Epoch 27 | Batch 52/94 | Batch Loss: 2.8326\n",
      "Epoch 27 | Batch 53/94 | Batch Loss: 2.8161\n",
      "Epoch 27 | Batch 54/94 | Batch Loss: 2.7157\n",
      "Epoch 27 | Batch 55/94 | Batch Loss: 2.7704\n",
      "Epoch 27 | Batch 56/94 | Batch Loss: 2.7895\n",
      "Epoch 27 | Batch 57/94 | Batch Loss: 2.8974\n",
      "Epoch 27 | Batch 58/94 | Batch Loss: 2.7849\n",
      "Epoch 27 | Batch 59/94 | Batch Loss: 2.8898\n",
      "Epoch 27 | Batch 60/94 | Batch Loss: 2.7985\n",
      "Epoch 27 | Batch 61/94 | Batch Loss: 2.8398\n",
      "Epoch 27 | Batch 62/94 | Batch Loss: 2.8487\n",
      "Epoch 27 | Batch 63/94 | Batch Loss: 2.9019\n",
      "Epoch 27 | Batch 64/94 | Batch Loss: 2.6828\n",
      "Epoch 27 | Batch 65/94 | Batch Loss: 2.8004\n",
      "Epoch 27 | Batch 66/94 | Batch Loss: 2.9540\n",
      "Epoch 27 | Batch 67/94 | Batch Loss: 2.7767\n",
      "Epoch 27 | Batch 68/94 | Batch Loss: 2.9309\n",
      "Epoch 27 | Batch 69/94 | Batch Loss: 2.8077\n",
      "Epoch 27 | Batch 70/94 | Batch Loss: 2.8533\n",
      "Epoch 27 | Batch 71/94 | Batch Loss: 2.8177\n",
      "Epoch 27 | Batch 72/94 | Batch Loss: 2.8826\n",
      "Epoch 27 | Batch 73/94 | Batch Loss: 2.8400\n",
      "Epoch 27 | Batch 74/94 | Batch Loss: 2.9872\n",
      "Epoch 27 | Batch 75/94 | Batch Loss: 2.7924\n",
      "Epoch 27 | Batch 76/94 | Batch Loss: 2.8727\n",
      "Epoch 27 | Batch 77/94 | Batch Loss: 2.9574\n",
      "Epoch 27 | Batch 78/94 | Batch Loss: 2.8045\n",
      "Epoch 27 | Batch 79/94 | Batch Loss: 2.7883\n",
      "Epoch 27 | Batch 80/94 | Batch Loss: 2.8884\n",
      "Epoch 27 | Batch 81/94 | Batch Loss: 2.8444\n",
      "Epoch 27 | Batch 82/94 | Batch Loss: 2.8967\n",
      "Epoch 27 | Batch 83/94 | Batch Loss: 2.9011\n",
      "Epoch 27 | Batch 84/94 | Batch Loss: 2.8880\n",
      "Epoch 27 | Batch 85/94 | Batch Loss: 2.9000\n",
      "Epoch 27 | Batch 86/94 | Batch Loss: 2.8648\n",
      "Epoch 27 | Batch 87/94 | Batch Loss: 2.8546\n",
      "Epoch 27 | Batch 88/94 | Batch Loss: 2.7935\n",
      "Epoch 27 | Batch 89/94 | Batch Loss: 3.0146\n",
      "Epoch 27 | Batch 90/94 | Batch Loss: 2.8093\n",
      "Epoch 27 | Batch 91/94 | Batch Loss: 2.8986\n",
      "Epoch 27 | Batch 92/94 | Batch Loss: 2.8356\n",
      "Epoch 27 | Batch 93/94 | Batch Loss: 2.9192\n",
      "Epoch 27 | Batch 94/94 | Batch Loss: 2.7971\n",
      "Epoch 27 Finished | Average Loss: 2.8607\n",
      "\n",
      "Epoch 28 Starting...\n",
      "Epoch 28 | Batch 1/94 | Batch Loss: 2.8191\n",
      "Epoch 28 | Batch 2/94 | Batch Loss: 2.7135\n",
      "Epoch 28 | Batch 3/94 | Batch Loss: 2.8600\n",
      "Epoch 28 | Batch 4/94 | Batch Loss: 2.6824\n",
      "Epoch 28 | Batch 5/94 | Batch Loss: 2.7729\n",
      "Epoch 28 | Batch 6/94 | Batch Loss: 2.7626\n",
      "Epoch 28 | Batch 7/94 | Batch Loss: 2.8463\n",
      "Epoch 28 | Batch 8/94 | Batch Loss: 2.7508\n",
      "Epoch 28 | Batch 9/94 | Batch Loss: 2.8462\n",
      "Epoch 28 | Batch 10/94 | Batch Loss: 2.9140\n",
      "Epoch 28 | Batch 11/94 | Batch Loss: 2.8897\n",
      "Epoch 28 | Batch 12/94 | Batch Loss: 2.7849\n",
      "Epoch 28 | Batch 13/94 | Batch Loss: 2.8112\n",
      "Epoch 28 | Batch 14/94 | Batch Loss: 2.7561\n",
      "Epoch 28 | Batch 15/94 | Batch Loss: 2.7572\n",
      "Epoch 28 | Batch 16/94 | Batch Loss: 2.8816\n",
      "Epoch 28 | Batch 17/94 | Batch Loss: 2.8534\n",
      "Epoch 28 | Batch 18/94 | Batch Loss: 2.8095\n",
      "Epoch 28 | Batch 19/94 | Batch Loss: 2.8616\n",
      "Epoch 28 | Batch 20/94 | Batch Loss: 2.8821\n",
      "Epoch 28 | Batch 21/94 | Batch Loss: 2.7438\n",
      "Epoch 28 | Batch 22/94 | Batch Loss: 2.7923\n",
      "Epoch 28 | Batch 23/94 | Batch Loss: 2.8411\n",
      "Epoch 28 | Batch 24/94 | Batch Loss: 2.7744\n",
      "Epoch 28 | Batch 25/94 | Batch Loss: 2.7458\n",
      "Epoch 28 | Batch 26/94 | Batch Loss: 2.8948\n",
      "Epoch 28 | Batch 27/94 | Batch Loss: 2.7630\n",
      "Epoch 28 | Batch 28/94 | Batch Loss: 2.7432\n",
      "Epoch 28 | Batch 29/94 | Batch Loss: 2.8533\n",
      "Epoch 28 | Batch 30/94 | Batch Loss: 2.7600\n",
      "Epoch 28 | Batch 31/94 | Batch Loss: 2.8795\n",
      "Epoch 28 | Batch 32/94 | Batch Loss: 2.8927\n",
      "Epoch 28 | Batch 33/94 | Batch Loss: 2.8472\n",
      "Epoch 28 | Batch 34/94 | Batch Loss: 2.8352\n",
      "Epoch 28 | Batch 35/94 | Batch Loss: 2.8656\n",
      "Epoch 28 | Batch 36/94 | Batch Loss: 2.8331\n",
      "Epoch 28 | Batch 37/94 | Batch Loss: 2.8947\n",
      "Epoch 28 | Batch 38/94 | Batch Loss: 2.7985\n",
      "Epoch 28 | Batch 39/94 | Batch Loss: 2.8917\n",
      "Epoch 28 | Batch 40/94 | Batch Loss: 2.8513\n",
      "Epoch 28 | Batch 41/94 | Batch Loss: 2.9273\n",
      "Epoch 28 | Batch 42/94 | Batch Loss: 2.8861\n",
      "Epoch 28 | Batch 43/94 | Batch Loss: 2.8619\n",
      "Epoch 28 | Batch 44/94 | Batch Loss: 2.8639\n",
      "Epoch 28 | Batch 45/94 | Batch Loss: 2.8163\n",
      "Epoch 28 | Batch 46/94 | Batch Loss: 2.7982\n",
      "Epoch 28 | Batch 47/94 | Batch Loss: 2.8959\n",
      "Epoch 28 | Batch 48/94 | Batch Loss: 2.8677\n",
      "Epoch 28 | Batch 49/94 | Batch Loss: 2.8234\n",
      "Epoch 28 | Batch 50/94 | Batch Loss: 2.8116\n",
      "Epoch 28 | Batch 51/94 | Batch Loss: 2.7784\n",
      "Epoch 28 | Batch 52/94 | Batch Loss: 2.8492\n",
      "Epoch 28 | Batch 53/94 | Batch Loss: 2.8189\n",
      "Epoch 28 | Batch 54/94 | Batch Loss: 2.7882\n",
      "Epoch 28 | Batch 55/94 | Batch Loss: 2.8477\n",
      "Epoch 28 | Batch 56/94 | Batch Loss: 2.6925\n",
      "Epoch 28 | Batch 57/94 | Batch Loss: 2.8105\n",
      "Epoch 28 | Batch 58/94 | Batch Loss: 2.7677\n",
      "Epoch 28 | Batch 59/94 | Batch Loss: 2.8060\n",
      "Epoch 28 | Batch 60/94 | Batch Loss: 2.8224\n",
      "Epoch 28 | Batch 61/94 | Batch Loss: 2.9078\n",
      "Epoch 28 | Batch 62/94 | Batch Loss: 2.8192\n",
      "Epoch 28 | Batch 63/94 | Batch Loss: 2.8867\n",
      "Epoch 28 | Batch 64/94 | Batch Loss: 2.9056\n",
      "Epoch 28 | Batch 65/94 | Batch Loss: 2.8375\n",
      "Epoch 28 | Batch 66/94 | Batch Loss: 2.8708\n",
      "Epoch 28 | Batch 67/94 | Batch Loss: 2.8191\n",
      "Epoch 28 | Batch 68/94 | Batch Loss: 2.8485\n",
      "Epoch 28 | Batch 69/94 | Batch Loss: 2.8000\n",
      "Epoch 28 | Batch 70/94 | Batch Loss: 2.8729\n",
      "Epoch 28 | Batch 71/94 | Batch Loss: 2.8655\n",
      "Epoch 28 | Batch 72/94 | Batch Loss: 2.8405\n",
      "Epoch 28 | Batch 73/94 | Batch Loss: 2.7931\n",
      "Epoch 28 | Batch 74/94 | Batch Loss: 2.8287\n",
      "Epoch 28 | Batch 75/94 | Batch Loss: 2.7596\n",
      "Epoch 28 | Batch 76/94 | Batch Loss: 2.8322\n",
      "Epoch 28 | Batch 77/94 | Batch Loss: 2.8765\n",
      "Epoch 28 | Batch 78/94 | Batch Loss: 2.8437\n",
      "Epoch 28 | Batch 79/94 | Batch Loss: 2.8471\n",
      "Epoch 28 | Batch 80/94 | Batch Loss: 2.7480\n",
      "Epoch 28 | Batch 81/94 | Batch Loss: 2.8339\n",
      "Epoch 28 | Batch 82/94 | Batch Loss: 2.8990\n",
      "Epoch 28 | Batch 83/94 | Batch Loss: 2.8242\n",
      "Epoch 28 | Batch 84/94 | Batch Loss: 2.8497\n",
      "Epoch 28 | Batch 85/94 | Batch Loss: 2.8830\n",
      "Epoch 28 | Batch 86/94 | Batch Loss: 2.8189\n",
      "Epoch 28 | Batch 87/94 | Batch Loss: 2.9359\n",
      "Epoch 28 | Batch 88/94 | Batch Loss: 2.7786\n",
      "Epoch 28 | Batch 89/94 | Batch Loss: 2.7586\n",
      "Epoch 28 | Batch 90/94 | Batch Loss: 2.9341\n",
      "Epoch 28 | Batch 91/94 | Batch Loss: 2.8342\n",
      "Epoch 28 | Batch 92/94 | Batch Loss: 2.7376\n",
      "Epoch 28 | Batch 93/94 | Batch Loss: 2.8325\n",
      "Epoch 28 | Batch 94/94 | Batch Loss: 2.7905\n",
      "Epoch 28 Finished | Average Loss: 2.8277\n",
      "\n",
      "Epoch 29 Starting...\n",
      "Epoch 29 | Batch 1/94 | Batch Loss: 2.7402\n",
      "Epoch 29 | Batch 2/94 | Batch Loss: 2.8361\n",
      "Epoch 29 | Batch 3/94 | Batch Loss: 2.8127\n",
      "Epoch 29 | Batch 4/94 | Batch Loss: 2.8439\n",
      "Epoch 29 | Batch 5/94 | Batch Loss: 2.8187\n",
      "Epoch 29 | Batch 6/94 | Batch Loss: 2.6965\n",
      "Epoch 29 | Batch 7/94 | Batch Loss: 2.8710\n",
      "Epoch 29 | Batch 8/94 | Batch Loss: 2.6915\n",
      "Epoch 29 | Batch 9/94 | Batch Loss: 2.7780\n",
      "Epoch 29 | Batch 10/94 | Batch Loss: 2.7110\n",
      "Epoch 29 | Batch 11/94 | Batch Loss: 2.7327\n",
      "Epoch 29 | Batch 12/94 | Batch Loss: 2.7937\n",
      "Epoch 29 | Batch 13/94 | Batch Loss: 2.7668\n",
      "Epoch 29 | Batch 14/94 | Batch Loss: 2.7735\n",
      "Epoch 29 | Batch 15/94 | Batch Loss: 2.8713\n",
      "Epoch 29 | Batch 16/94 | Batch Loss: 2.8906\n",
      "Epoch 29 | Batch 17/94 | Batch Loss: 2.9334\n",
      "Epoch 29 | Batch 18/94 | Batch Loss: 2.7787\n",
      "Epoch 29 | Batch 19/94 | Batch Loss: 2.6843\n",
      "Epoch 29 | Batch 20/94 | Batch Loss: 2.7505\n",
      "Epoch 29 | Batch 21/94 | Batch Loss: 2.7152\n",
      "Epoch 29 | Batch 22/94 | Batch Loss: 2.8875\n",
      "Epoch 29 | Batch 23/94 | Batch Loss: 2.6645\n",
      "Epoch 29 | Batch 24/94 | Batch Loss: 2.7971\n",
      "Epoch 29 | Batch 25/94 | Batch Loss: 2.8724\n",
      "Epoch 29 | Batch 26/94 | Batch Loss: 2.7860\n",
      "Epoch 29 | Batch 27/94 | Batch Loss: 2.7781\n",
      "Epoch 29 | Batch 28/94 | Batch Loss: 2.7827\n",
      "Epoch 29 | Batch 29/94 | Batch Loss: 2.6791\n",
      "Epoch 29 | Batch 30/94 | Batch Loss: 2.7181\n",
      "Epoch 29 | Batch 31/94 | Batch Loss: 2.9272\n",
      "Epoch 29 | Batch 32/94 | Batch Loss: 2.8235\n",
      "Epoch 29 | Batch 33/94 | Batch Loss: 2.8293\n",
      "Epoch 29 | Batch 34/94 | Batch Loss: 2.9191\n",
      "Epoch 29 | Batch 35/94 | Batch Loss: 2.7529\n",
      "Epoch 29 | Batch 36/94 | Batch Loss: 2.7773\n",
      "Epoch 29 | Batch 37/94 | Batch Loss: 2.7301\n",
      "Epoch 29 | Batch 38/94 | Batch Loss: 2.9269\n",
      "Epoch 29 | Batch 39/94 | Batch Loss: 2.8086\n",
      "Epoch 29 | Batch 40/94 | Batch Loss: 2.9132\n",
      "Epoch 29 | Batch 41/94 | Batch Loss: 2.8889\n",
      "Epoch 29 | Batch 42/94 | Batch Loss: 2.9219\n",
      "Epoch 29 | Batch 43/94 | Batch Loss: 2.6890\n",
      "Epoch 29 | Batch 44/94 | Batch Loss: 2.8011\n",
      "Epoch 29 | Batch 45/94 | Batch Loss: 2.7263\n",
      "Epoch 29 | Batch 46/94 | Batch Loss: 2.8266\n",
      "Epoch 29 | Batch 47/94 | Batch Loss: 2.8428\n",
      "Epoch 29 | Batch 48/94 | Batch Loss: 2.8323\n",
      "Epoch 29 | Batch 49/94 | Batch Loss: 2.8433\n",
      "Epoch 29 | Batch 50/94 | Batch Loss: 2.7556\n",
      "Epoch 29 | Batch 51/94 | Batch Loss: 2.6647\n",
      "Epoch 29 | Batch 52/94 | Batch Loss: 2.6670\n",
      "Epoch 29 | Batch 53/94 | Batch Loss: 2.7793\n",
      "Epoch 29 | Batch 54/94 | Batch Loss: 2.7569\n",
      "Epoch 29 | Batch 55/94 | Batch Loss: 2.7846\n",
      "Epoch 29 | Batch 56/94 | Batch Loss: 2.8025\n",
      "Epoch 29 | Batch 57/94 | Batch Loss: 2.7675\n",
      "Epoch 29 | Batch 58/94 | Batch Loss: 2.7016\n",
      "Epoch 29 | Batch 59/94 | Batch Loss: 2.9079\n",
      "Epoch 29 | Batch 60/94 | Batch Loss: 2.7688\n",
      "Epoch 29 | Batch 61/94 | Batch Loss: 2.7675\n",
      "Epoch 29 | Batch 62/94 | Batch Loss: 2.8812\n",
      "Epoch 29 | Batch 63/94 | Batch Loss: 2.8093\n",
      "Epoch 29 | Batch 64/94 | Batch Loss: 2.7609\n",
      "Epoch 29 | Batch 65/94 | Batch Loss: 2.7885\n",
      "Epoch 29 | Batch 66/94 | Batch Loss: 2.7846\n",
      "Epoch 29 | Batch 67/94 | Batch Loss: 2.7386\n",
      "Epoch 29 | Batch 68/94 | Batch Loss: 2.7677\n",
      "Epoch 29 | Batch 69/94 | Batch Loss: 2.8294\n",
      "Epoch 29 | Batch 70/94 | Batch Loss: 2.7910\n",
      "Epoch 29 | Batch 71/94 | Batch Loss: 2.6804\n",
      "Epoch 29 | Batch 72/94 | Batch Loss: 2.8288\n",
      "Epoch 29 | Batch 73/94 | Batch Loss: 2.8295\n",
      "Epoch 29 | Batch 74/94 | Batch Loss: 2.7759\n",
      "Epoch 29 | Batch 75/94 | Batch Loss: 2.7858\n",
      "Epoch 29 | Batch 76/94 | Batch Loss: 2.7598\n",
      "Epoch 29 | Batch 77/94 | Batch Loss: 2.7289\n",
      "Epoch 29 | Batch 78/94 | Batch Loss: 2.7761\n",
      "Epoch 29 | Batch 79/94 | Batch Loss: 2.8147\n",
      "Epoch 29 | Batch 80/94 | Batch Loss: 2.7440\n",
      "Epoch 29 | Batch 81/94 | Batch Loss: 2.7500\n",
      "Epoch 29 | Batch 82/94 | Batch Loss: 2.6966\n",
      "Epoch 29 | Batch 83/94 | Batch Loss: 2.9217\n",
      "Epoch 29 | Batch 84/94 | Batch Loss: 2.8330\n",
      "Epoch 29 | Batch 85/94 | Batch Loss: 2.8982\n",
      "Epoch 29 | Batch 86/94 | Batch Loss: 2.9150\n",
      "Epoch 29 | Batch 87/94 | Batch Loss: 2.7457\n",
      "Epoch 29 | Batch 88/94 | Batch Loss: 2.7812\n",
      "Epoch 29 | Batch 89/94 | Batch Loss: 2.9262\n",
      "Epoch 29 | Batch 90/94 | Batch Loss: 2.7474\n",
      "Epoch 29 | Batch 91/94 | Batch Loss: 2.7850\n",
      "Epoch 29 | Batch 92/94 | Batch Loss: 2.7905\n",
      "Epoch 29 | Batch 93/94 | Batch Loss: 2.8073\n",
      "Epoch 29 | Batch 94/94 | Batch Loss: 2.8384\n",
      "Epoch 29 Finished | Average Loss: 2.7944\n",
      "\n",
      "Epoch 30 Starting...\n",
      "Epoch 30 | Batch 1/94 | Batch Loss: 2.9095\n",
      "Epoch 30 | Batch 2/94 | Batch Loss: 2.7171\n",
      "Epoch 30 | Batch 3/94 | Batch Loss: 2.7744\n",
      "Epoch 30 | Batch 4/94 | Batch Loss: 2.7095\n",
      "Epoch 30 | Batch 5/94 | Batch Loss: 2.8209\n",
      "Epoch 30 | Batch 6/94 | Batch Loss: 2.7867\n",
      "Epoch 30 | Batch 7/94 | Batch Loss: 2.8808\n",
      "Epoch 30 | Batch 8/94 | Batch Loss: 2.6195\n",
      "Epoch 30 | Batch 9/94 | Batch Loss: 2.6335\n",
      "Epoch 30 | Batch 10/94 | Batch Loss: 2.6806\n",
      "Epoch 30 | Batch 11/94 | Batch Loss: 2.7141\n",
      "Epoch 30 | Batch 12/94 | Batch Loss: 2.7450\n",
      "Epoch 30 | Batch 13/94 | Batch Loss: 2.6116\n",
      "Epoch 30 | Batch 14/94 | Batch Loss: 2.6687\n",
      "Epoch 30 | Batch 15/94 | Batch Loss: 2.8164\n",
      "Epoch 30 | Batch 16/94 | Batch Loss: 2.7447\n",
      "Epoch 30 | Batch 17/94 | Batch Loss: 2.7614\n",
      "Epoch 30 | Batch 18/94 | Batch Loss: 2.8430\n",
      "Epoch 30 | Batch 19/94 | Batch Loss: 2.7930\n",
      "Epoch 30 | Batch 20/94 | Batch Loss: 2.7606\n",
      "Epoch 30 | Batch 21/94 | Batch Loss: 2.7617\n",
      "Epoch 30 | Batch 22/94 | Batch Loss: 2.7075\n",
      "Epoch 30 | Batch 23/94 | Batch Loss: 2.7302\n",
      "Epoch 30 | Batch 24/94 | Batch Loss: 2.8122\n",
      "Epoch 30 | Batch 25/94 | Batch Loss: 2.9119\n",
      "Epoch 30 | Batch 26/94 | Batch Loss: 2.7965\n",
      "Epoch 30 | Batch 27/94 | Batch Loss: 2.7201\n",
      "Epoch 30 | Batch 28/94 | Batch Loss: 2.7841\n",
      "Epoch 30 | Batch 29/94 | Batch Loss: 2.8149\n",
      "Epoch 30 | Batch 30/94 | Batch Loss: 2.7285\n",
      "Epoch 30 | Batch 31/94 | Batch Loss: 2.7056\n",
      "Epoch 30 | Batch 32/94 | Batch Loss: 2.7800\n",
      "Epoch 30 | Batch 33/94 | Batch Loss: 2.7386\n",
      "Epoch 30 | Batch 34/94 | Batch Loss: 2.8572\n",
      "Epoch 30 | Batch 35/94 | Batch Loss: 2.7805\n",
      "Epoch 30 | Batch 36/94 | Batch Loss: 2.6404\n",
      "Epoch 30 | Batch 37/94 | Batch Loss: 2.7881\n",
      "Epoch 30 | Batch 38/94 | Batch Loss: 2.6996\n",
      "Epoch 30 | Batch 39/94 | Batch Loss: 2.6220\n",
      "Epoch 30 | Batch 40/94 | Batch Loss: 2.7313\n",
      "Epoch 30 | Batch 41/94 | Batch Loss: 2.6914\n",
      "Epoch 30 | Batch 42/94 | Batch Loss: 2.6791\n",
      "Epoch 30 | Batch 43/94 | Batch Loss: 2.8172\n",
      "Epoch 30 | Batch 44/94 | Batch Loss: 2.7851\n",
      "Epoch 30 | Batch 45/94 | Batch Loss: 2.7193\n",
      "Epoch 30 | Batch 46/94 | Batch Loss: 2.8814\n",
      "Epoch 30 | Batch 47/94 | Batch Loss: 2.7624\n",
      "Epoch 30 | Batch 48/94 | Batch Loss: 2.8103\n",
      "Epoch 30 | Batch 49/94 | Batch Loss: 2.7972\n",
      "Epoch 30 | Batch 50/94 | Batch Loss: 2.6551\n",
      "Epoch 30 | Batch 51/94 | Batch Loss: 2.7445\n",
      "Epoch 30 | Batch 52/94 | Batch Loss: 2.7662\n",
      "Epoch 30 | Batch 53/94 | Batch Loss: 2.6276\n",
      "Epoch 30 | Batch 54/94 | Batch Loss: 2.9060\n",
      "Epoch 30 | Batch 55/94 | Batch Loss: 2.7657\n",
      "Epoch 30 | Batch 56/94 | Batch Loss: 2.7479\n",
      "Epoch 30 | Batch 57/94 | Batch Loss: 2.8077\n",
      "Epoch 30 | Batch 58/94 | Batch Loss: 2.6830\n",
      "Epoch 30 | Batch 59/94 | Batch Loss: 2.7699\n",
      "Epoch 30 | Batch 60/94 | Batch Loss: 2.7306\n",
      "Epoch 30 | Batch 61/94 | Batch Loss: 2.9206\n",
      "Epoch 30 | Batch 62/94 | Batch Loss: 2.7522\n",
      "Epoch 30 | Batch 63/94 | Batch Loss: 2.7173\n",
      "Epoch 30 | Batch 64/94 | Batch Loss: 2.6504\n",
      "Epoch 30 | Batch 65/94 | Batch Loss: 2.8069\n",
      "Epoch 30 | Batch 66/94 | Batch Loss: 2.7099\n",
      "Epoch 30 | Batch 67/94 | Batch Loss: 2.7363\n",
      "Epoch 30 | Batch 68/94 | Batch Loss: 2.7753\n",
      "Epoch 30 | Batch 69/94 | Batch Loss: 2.7269\n",
      "Epoch 30 | Batch 70/94 | Batch Loss: 2.7873\n",
      "Epoch 30 | Batch 71/94 | Batch Loss: 2.7328\n",
      "Epoch 30 | Batch 72/94 | Batch Loss: 2.7820\n",
      "Epoch 30 | Batch 73/94 | Batch Loss: 2.6952\n",
      "Epoch 30 | Batch 74/94 | Batch Loss: 2.6807\n",
      "Epoch 30 | Batch 75/94 | Batch Loss: 2.8418\n",
      "Epoch 30 | Batch 76/94 | Batch Loss: 2.8812\n",
      "Epoch 30 | Batch 77/94 | Batch Loss: 2.9552\n",
      "Epoch 30 | Batch 78/94 | Batch Loss: 2.8286\n",
      "Epoch 30 | Batch 79/94 | Batch Loss: 2.7083\n",
      "Epoch 30 | Batch 80/94 | Batch Loss: 2.7839\n",
      "Epoch 30 | Batch 81/94 | Batch Loss: 2.7693\n",
      "Epoch 30 | Batch 82/94 | Batch Loss: 2.7422\n",
      "Epoch 30 | Batch 83/94 | Batch Loss: 2.8279\n",
      "Epoch 30 | Batch 84/94 | Batch Loss: 2.7674\n",
      "Epoch 30 | Batch 85/94 | Batch Loss: 2.7029\n",
      "Epoch 30 | Batch 86/94 | Batch Loss: 2.8803\n",
      "Epoch 30 | Batch 87/94 | Batch Loss: 2.9046\n",
      "Epoch 30 | Batch 88/94 | Batch Loss: 2.6399\n",
      "Epoch 30 | Batch 89/94 | Batch Loss: 2.7589\n",
      "Epoch 30 | Batch 90/94 | Batch Loss: 2.7012\n",
      "Epoch 30 | Batch 91/94 | Batch Loss: 2.8808\n",
      "Epoch 30 | Batch 92/94 | Batch Loss: 2.7771\n",
      "Epoch 30 | Batch 93/94 | Batch Loss: 2.7904\n",
      "Epoch 30 | Batch 94/94 | Batch Loss: 2.8551\n",
      "Epoch 30 Finished | Average Loss: 2.7630\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAHFCAYAAAAQU+iSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABTmUlEQVR4nO3deVxU9d4H8M+ZYRh2ZGdQUNSQTdxIwlwzcIvM5VYuZffWLc2619THm2apZeutrnUtbdHU1GuZLVYuaIpWLqigoYChIqCACMgiCAxwnj+QqZFtZpzhzAyf9+s1r6c5c86Z7/w6z+XTOb9FEEVRBBEREZGVkUldABEREZEpMOQQERGRVWLIISIiIqvEkENERERWiSGHiIiIrBJDDhEREVklhhwiIiKySgw5REREZJUYcoiIiMgqMeQQGYkgCDq9EhISbut7li5dCkEQDDo2ISHBKDXcznd/9dVX7f7dlmbdunUmvYZu18WLFyEIAt5++21J6yBqi43UBRBZi8OHD2u9f+WVV7B//37s27dPa3toaOhtfc8TTzyB0aNHG3Rs//79cfjw4duugdrHZ599huDg4Cbb+e+PSDcMOURGctddd2m99/Lygkwma7L9VpWVlXBwcND5e7p06YIuXboYVKOLi0ub9VD70OXfe3h4OCIjI9upIiLrw8dVRO1o+PDhCA8Px8GDBzFo0CA4ODjgb3/7GwDgiy++QGxsLFQqFezt7RESEoLnn38eFRUVWudo7nFVt27dcN9992HXrl3o378/7O3tERwcjLVr12rt19zjqsceewxOTk44d+4cxo4dCycnJ/j7+2PevHmorq7WOv7SpUuYPHkynJ2d0alTJ0ybNg3Hjh2DIAhYt26dUdro9OnTGD9+PNzc3GBnZ4e+ffti/fr1WvvU19dj+fLl6NWrF+zt7dGpUydERETgvffe0+xz9epVPPnkk/D394dSqYSXlxfuvvtu7N27t9Xvb2zf5ORkTJw4ES4uLnB1dcX06dNx9erVJvt/8cUXiI6OhqOjI5ycnDBq1CgkJydr7dPYxikpKYiNjYWzszNGjhx5G630B0EQ8Mwzz+Cjjz5CUFAQlEolQkNDsWXLlib76tK2AFBSUoJ58+ahe/fuUCqV8Pb2xtixY5Gent5k33fffReBgYFwcnJCdHQ0jhw5YpTfRWQMvJND1M7y8vIwffp0LFiwAK+99hpksob/1sjIyMDYsWMxZ84cODo6Ij09HW+++SYSExObPPJqzqlTpzBv3jw8//zz8PHxwaefforHH38cPXv2xNChQ1s9Vq1W4/7778fjjz+OefPm4eDBg3jllVfg6uqKl156CQBQUVGBESNGoLi4GG+++SZ69uyJXbt24aGHHrr9Rrnp7NmzGDRoELy9vfH+++/Dw8MDGzduxGOPPYYrV65gwYIFAIC33noLS5cuxeLFizF06FCo1Wqkp6ejpKREc65HHnkESUlJePXVVxEUFISSkhIkJSWhqKhIp1omTJiABx98EDNnzsSZM2fw4osvIjU1FUePHoVCoQAAvPbaa1i8eDH++te/YvHixaipqcG///1vDBkyBImJiVqPlWpqanD//ffjqaeewvPPP4/a2to2a6irq2uynyAIkMvlWtu2b9+O/fv34+WXX4ajoyM+/PBDTJkyBTY2Npg8ebJebVteXo7Bgwfj4sWL+Ne//oWoqChcv34dBw8eRF5entbjsw8++ADBwcFYsWIFAODFF1/E2LFjkZmZCVdXV53amcikRCIyiRkzZoiOjo5a24YNGyYCEH/66adWj62vrxfVarV44MABEYB46tQpzWdLliwRb/1/3a5du4p2dnZiVlaWZtuNGzdEd3d38amnntJs279/vwhA3L9/v1adAMQvv/xS65xjx44Ve/XqpXn/wQcfiADEnTt3au331FNPiQDEzz77rNXf1PjdW7dubXGfhx9+WFQqlWJ2drbW9jFjxogODg5iSUmJKIqieN9994l9+/Zt9fucnJzEOXPmtLpPcxrb97nnntPavmnTJhGAuHHjRlEURTE7O1u0sbERn332Wa39ysvLRV9fX/HBBx/UbGts47Vr1+pUw2effSYCaPYll8u19gUg2tvbi/n5+ZpttbW1YnBwsNizZ0/NNl3b9uWXXxYBiHv27GmxvszMTBGA2Lt3b7G2tlazPTExUQQg/u9//9PpdxKZGh9XEbUzNzc33HPPPU22X7hwAVOnToWvry/kcjkUCgWGDRsGAEhLS2vzvH379kVAQIDmvZ2dHYKCgpCVldXmsYIgIC4uTmtbRESE1rEHDhyAs7Nzk07PU6ZMafP8utq3bx9GjhwJf39/re2PPfYYKisrNZ27Bw4ciFOnTuHpp5/G7t27UVZW1uRcAwcOxLp167B8+XIcOXIEarVar1qmTZum9f7BBx+EjY0N9u/fDwDYvXs3amtr8eijj6K2tlbzsrOzw7Bhw5odATVp0iS9atiwYQOOHTum9Tp69GiT/UaOHAkfHx/Ne7lcjoceegjnzp3DpUuXAOjetjt37kRQUBDuvffeNusbN26c1l2liIgIANDpmiNqD3xcRdTOVCpVk23Xr1/HkCFDYGdnh+XLlyMoKAgODg7IycnBxIkTcePGjTbP6+Hh0WSbUqnU6VgHBwfY2dk1ObaqqkrzvqioSOsPaaPmthmqqKio2fbx8/PTfA4ACxcuhKOjIzZu3IjVq1dDLpdj6NChePPNNzUddb/44gssX74cn376KV588UU4OTlhwoQJeOutt+Dr69tmLbfuY2NjAw8PD00NV65cAQDceeedzR7f+BiykYODA1xcXNr83j8LCQnRqeNxc7+ncVtRURG6dOmic9tevXpVKyy35tZrTqlUAoBO1xxRe2DIIWpnzc1xs2/fPuTm5iIhIUFz9waAVh8TqXl4eCAxMbHJ9vz8fKN+R15eXpPtubm5AABPT08ADYFj7ty5mDt3LkpKSrB3714sWrQIo0aNQk5ODhwcHODp6YkVK1ZgxYoVyM7Oxvbt2/H888+joKAAu3btarOW/Px8dO7cWfO+trYWRUVFmj/sjbV89dVX6Nq1a5vnM3RuI1009++gcVtjvbq2rZeXl+buD5Gl4+MqIjPQ+Aew8b+EG3300UdSlNOsYcOGoby8HDt37tTa3twoHkONHDlSE/j+bMOGDXBwcGh2+HunTp0wefJkzJ49G8XFxbh48WKTfQICAvDMM88gJiYGSUlJOtWyadMmrfdffvklamtrMXz4cADAqFGjYGNjg/PnzyMyMrLZV3v56aefNHeWgIYOy1988QV69OihmW5A17YdM2YMfv/9d506uxOZO97JITIDgwYNgpubG2bOnIklS5ZAoVBg06ZNOHXqlNSlacyYMQP/+c9/MH36dCxfvhw9e/bEzp07sXv3bgBNH8+0pKUhxsOGDcOSJUvwww8/YMSIEXjppZfg7u6OTZs24ccff8Rbb72lGbETFxenmUPGy8sLWVlZWLFiBbp27Yo77rgDpaWlGDFiBKZOnYrg4GA4Ozvj2LFj2LVrFyZOnKhTnV9//TVsbGwQExOjGV3Vp08fPPjggwAahu2//PLLeOGFF3DhwgWMHj0abm5uuHLlChITE+Ho6Ihly5bp9F0tOX36dLOjsHr06AEvLy/Ne09PT9xzzz148cUXNaOr0tPTtQKorm07Z84cfPHFFxg/fjyef/55DBw4EDdu3MCBAwdw3333YcSIEbf1m4jaldQ9n4msVUujq8LCwprd/9ChQ2J0dLTo4OAgenl5iU888YSYlJTUZORSS6Orxo0b1+Scw4YNE4cNG6Z539LoqlvrbOl7srOzxYkTJ4pOTk6is7OzOGnSJHHHjh0iAPG7775rqSm0vrulV2NNKSkpYlxcnOjq6ira2tqKffr0aTJy65133hEHDRokenp6ira2tmJAQID4+OOPixcvXhRFURSrqqrEmTNnihEREaKLi4tob28v9urVS1yyZIlYUVHRap2Nv/vEiRNiXFyc5rdOmTJFvHLlSpP9v/32W3HEiBGii4uLqFQqxa5du4qTJ08W9+7d22Ybt6S10VUAxE8++USzLwBx9uzZ4ocffij26NFDVCgUYnBwsLhp06Ym59WlbUVRFK9duyb+85//FAMCAkSFQiF6e3uL48aNE9PT00VR/GN01b///e8mxwIQlyxZovNvJTIlQRRFsR0zFRFZmca5YrKzsw2eidmcLF26FMuWLcPVq1c1/VTMmSAImD17NlauXCl1KURmh4+riEhnjX9Ig4ODoVarsW/fPrz//vuYPn26VQQcIrIuDDlEpDMHBwf85z//wcWLF1FdXY2AgAD861//wuLFi6UujYioCT6uIiIiIqvEIeRERERklRhyiIiIyCox5BAREZFV6nAdj+vr65GbmwtnZ2eTTrNORERExiOKIsrLy+Hn56fz5KMdLuTk5uY2WYWXiIiILENOTo7OU1Z0uJDj7OwMoKGR9F0RuC1qtRrx8fGIjY2FQqEw6rmtGdtNf2wzw7DdDMN2MwzbTX+ttVlZWRn8/f01f8d10eFCTuMjKhcXF5OEHAcHB7i4uPCC1gPbTX9sM8Ow3QzDdjMM201/urSZPl1N2PGYiIiIrBJDDhEREVklhhwiIiKySgw5REREZJUYcoiIiMgqMeQQERGRVWLIISIiIqvEkENERERWiSGHiIiIrBJDjpHU1Ys4mlmME4UCjmYWo65elLokIiKiDq3DLetgCrtO52HZ96nIK60CIMeGjONQudphSVwoRoerpC6PiIioQ+KdnNu063QeZm1Muhlw/pBfWoVZG5Ow63SeRJURERF1bAw5t6GuXsSy71PR3IOpxm3Lvk/loysiIiIJMOTchsTM4iZ3cP5MBJBXWoXEzOL2K4qIiIgAMOTcloLylgOOIfsRERGR8TDk3AZvZzuj7kdERETGw5BzGwYGukPlagehhc8FACpXOwwMdG/PsoiIiAgMObdFLhOwJC4UAJoEncb3S+JCIZe1FIOIiIjIVBhybtPocBVWTe8PX1ftR1K+rnZYNb0/58khIiKSCCcDNILR4SrEhPpiV8plzP7fKQDArjlD4WqvkLgyIiKijot3coxELhMQG+oDV0XDnDjnCsolroiIiKhjY8gxss6ODSEnNbdM4kqIiIg6NoYcI/NzbPi/qXm8k0NERCQlhhwj6+Jw805OHu/kEBERSYkhx8gaH1edzS/jmlVEREQSYsgxMk87wF4hQ5W6HpmFFVKXQ0RE1GEx5BiZTAB6+ToDANL4yIqIiEgyDDkmEHwz5LBfDhERkXQYckwghHdyiIiIJMeQYwKNIYdz5RAREUmHIccEgnycIAhAQXk1Cq9XS10OERFRh8SQYwKOSht082iYFZCPrIiIiKTBkGMioSoXAAw5REREUmHIMZEQFfvlEBERSYkhx0RC/Rrv5HANKyIiIikw5JhIyM3HVeeuXkeVuk7iaoiIiDoehhwT8XWxg5uDAnX1Is4VXJe6HCIiog6HIcdEBEHQ3M1hvxwiIqL2x5BjQo0jrLi8AxERUftjyDGhEIYcIiIiyTDkmNAfI6zKIIqixNUQERF1LAw5JtTDywkKuYDyqlpcunZD6nKIiIg6FIYcE7K1keEO75uTAvKRFRERUbtiyDGxEC7vQEREJAmGHBNr7JfDYeRERETtiyHHxDQLdeYz5BAREbUnSUPO0qVLIQiC1svX17fF/RMSEprsLwgC0tPT27Fq/TSGnJziGyirUktcDRERUcdhI3UBYWFh2Lt3r+a9XC5v85izZ8/CxcVF897Ly8sktRmDq4MCnTvZ43LJDaTnlWNgoLvUJREREXUIkoccGxubVu/eNMfb2xudOnUyTUEmEKJyxuWSG0jNLWXIISIiaieSh5yMjAz4+flBqVQiKioKr732Grp3797qMf369UNVVRVCQ0OxePFijBgxosV9q6urUV1drXlfVtbQN0atVkOtNu7jo8bz3XreXj5O2JtWgDO5pUb/TmvQUrtRy9hmhmG7GYbtZhi2m/5aazND2lEQJZyKd+fOnaisrERQUBCuXLmC5cuXIz09HWfOnIGHh0eT/c+ePYuDBw9iwIABqK6uxueff47Vq1cjISEBQ4cObfY7li5dimXLljXZvnnzZjg4OBj9NzXnZJGAz36Xw99RxPyIunb5TiIiImtSWVmJqVOnorS0VKvLSmskDTm3qqioQI8ePbBgwQLMnTtXp2Pi4uIgCAK2b9/e7OfN3cnx9/dHYWGhzo2kK7VajT179iAmJgYKhUKzPau4Evf+5xfY2shwavE9sJFzUNuftdRu1DK2mWHYboZhuxmG7aa/1tqsrKwMnp6eeoUcyR9X/ZmjoyN69+6NjIwMnY+56667sHHjxhY/VyqVUCqVTbYrFAqTXXS3nru7lwscbeWoqKlDTmkNgnycTfK9ls6U/06sFdvMMGw3w7DdDMN2019zbWZIG5rVLYXq6mqkpaVBpVLpfExycrJe+0tBJhM48zEREVE7k/ROzvz58xEXF4eAgAAUFBRg+fLlKCsrw4wZMwAACxcuxOXLl7FhwwYAwIoVK9CtWzeEhYWhpqYGGzduxLZt27Bt2zYpf4ZOQlQuOJ51Dam5ZRjft7PU5RAREVk9SUPOpUuXMGXKFBQWFsLLywt33XUXjhw5gq5duwIA8vLykJ2drdm/pqYG8+fPx+XLl2Fvb4+wsDD8+OOPGDt2rFQ/QWea5R14J4eIiKhdSBpytmzZ0urn69at03q/YMECLFiwwIQVmQ4fVxEREbUvs+qTY816+ThDJgCF12tQUF4ldTlERERWjyGnndjbyhHo6QiAK5ITERG1B4acdhTq5wqA/XKIiIjaA0NOOwpRNcyPk5ZXLnElRERE1o8hpx2F3ux8nJpbKnElRERE1o8hpx01hpzMwgrcqOEaVkRERKbEkNOOvJyV8HSyRb0InL3CR1ZERESmxJDTjgSByzsQERG1F4acdvZHvxyGHCIiIlNiyGlnjcs78E4OERGRaTHktLM/P66qrxclroaIiMh6MeS0s+6ejrC1kaGipg451yqlLoeIiMhqMeS0Mxu5DL18GiYFZL8cIiIi02HIkUAoR1gRERGZHEOOBBqXd+AaVkRERKbDkCOBxoU6uYYVERGR6TDkSCD45p2cyyU3UFJZI3E1RERE1okhRwIudgr4u9sD4N0cIiIiU2HIkUiI782Zj9kvh4iIyCQYciTCmY+JiIhMiyFHIiFcw4qIiMikGHIk0jhXzrmC66iprZe4GiIiIuvDkCORLm72cLazQU1dPc5fvS51OURERFaHIUcigiDwkRUREZEJMeRIiMs7EBERmQ5DjoQaQw6HkRMRERkfQ46EQv50J0cURYmrISIisi4MORK6w8cJcpmAa5Vq5JdVSV0OERGRVWHIkZCdQo6eXk4A2C+HiIjI2BhyJBZyc7FOjrAiIiIyLoYcif2xvAMX6iQiIjImhhyJhXCEFRERkUkw5EisMeRcLKpARXWtxNUQERFZD4YciXk6KeHtrIQoAun5fGRFRERkLAw5ZuCPfjl8ZEVERGQsDDlmgP1yiIiIjI8hxwwE+zQMI//1XCEOny9CXT1nPyYiIrpdDDkS23U6D6/8mAoAyCqqxJRPjmDwm/uw63SexJURERFZNoYcCe06nYdZG5NQeL1Ga3t+aRVmbUxi0CEiIroNDDkSqasXsez7VDT3YKpx27LvU/noioiIyEAMORJJzCxGXmnLi3KKAPJKq5CYWdx+RREREVkRhhyJFJTrtuq4rvsRERGRNoYciXg72xl1PyIiItLGkCORgYHuULnaQWjhcwGAytUOAwPd27MsIiIiqyFpyFm6dCkEQdB6+fr6tnrMgQMHMGDAANjZ2aF79+5YvXp1O1VrXHKZgCVxoQDQYtBZEhcKuaylT4mIiKg1kt/JCQsLQ15enuaVkpLS4r6ZmZkYO3YshgwZguTkZCxatAj/+Mc/sG3btnas2HhGh6uwanp/+Lo2fST12sTeGB2ukqAqIiIi62AjeQE2Nm3evWm0evVqBAQEYMWKFQCAkJAQHD9+HG+//TYmTZpkwipNZ3S4CjGhvkjMLEZBeRU+2H8Ov1+5jloOHSciIrotkoecjIwM+Pn5QalUIioqCq+99hq6d+/e7L6HDx9GbGys1rZRo0ZhzZo1UKvVUCgUTY6prq5GdXW15n1ZWcP6UGq1Gmq12oi/BJrzGXLeyAAXAC64fK0Cb+3OwK6UPDw8wM+o9Zmr22m3joptZhi2m2HYboZhu+mvtTYzpB0FURQlu2Wwc+dOVFZWIigoCFeuXMHy5cuRnp6OM2fOwMPDo8n+QUFBeOyxx7Bo0SLNtkOHDuHuu+9Gbm4uVKqmj3eWLl2KZcuWNdm+efNmODg4GPcHGUHBDeDVkzaQCSJejayDg+QxlIiISHqVlZWYOnUqSktL4eLiotMxkv4JHTNmjOafe/fujejoaPTo0QPr16/H3Llzmz1GELQ74jZmtFu3N1q4cKHWucrKyuDv74/Y2FidG0lXarUae/bsQUxMTLN3lXT1xeVfce5qBWy79sPYPtbfL8dY7daRsM0Mw3YzDNvNMGw3/bXWZo1PYvRhVvcJHB0d0bt3b2RkZDT7ua+vL/Lz87W2FRQUwMbGptk7PwCgVCqhVCqbbFcoFCa76G733KPDVVi5/xx+OnsVkyIDjFiZeTPlvxNrxTYzDNvNMGw3w7Dd9NdcmxnShpKPrvqz6upqpKWlNfvYCQCio6OxZ88erW3x8fGIjIy0qgsoNswHAJBw9iqq1HUSV0NERGSZJA058+fPx4EDB5CZmYmjR49i8uTJKCsrw4wZMwA0PGp69NFHNfvPnDkTWVlZmDt3LtLS0rB27VqsWbMG8+fPl+onmETvzq7wdbFDZU0dDp0vlLocIiIiiyRpyLl06RKmTJmCXr16YeLEibC1tcWRI0fQtWtXAEBeXh6ys7M1+wcGBmLHjh1ISEhA37598corr+D999+32OHjLREEQXM3J/7MFYmrISIiskyS9snZsmVLq5+vW7euybZhw4YhKSnJRBWZj1FhvthwOAt7Uq/g1QkiZz4mIiLSk1n1yaE/DAx0h4udDYoqapCUfU3qcoiIiCwOQ46ZUshlGBnS+Mgqv429iYiI6FYMOWYsNvRmyEm9AgnnbCQiIrJIDDlmbFgvLyhtZMgqqsTZK+VSl0NERGRRGHLMmIOtDYbc4QmAo6yIiIj0xZBj5mJDG1Zoj09lvxwiIiJ9MOSYuZEh3pAJwOnLZbhcckPqcoiIiCwGQ46Z83BSIrKbOwCOsiIiItIHQ44F0IyyYr8cIiIinTHkWIDGfjmJF4txraJG4mqIiIgsA0OOBQjwcECwrzPq6kX8lF4gdTlEREQWgSHHQowKuznKiv1yiIiIdMKQYyEaVyU/mHEVN2rqJK6GiIjI/DHkWIhQlQs6d7JHlboeP2dclbocIiIis8eQYyEEQdDczdnNUVZERERtYsixII39cn5Kv4LaunqJqyEiIjJvDDkWJLKrG9wcFCipVOPYxWtSl0NERGTWGHIsiI1chpEhNycG5FpWRERErWLIsTB/DCW/AlEUJa6GiIjIfDHkWJghd3jCXiHH5ZIbOJNbJnU5REREZoshx8LYKeQYGuQJAIhP5SgrIiKiljDkWKDGtaw4+zEREVHLGHIs0MgQb8hlAtLzy5FdVCl1OURERGaJIccCdXKwRVSgOwCOsiIiImoJQ46Fig29OZScsx8TERE1iyHHQsXcHEp+PKsYhderJa6GiIjI/DDkWKjOnezRu7Mr6kXgpzTezSEiIroVQ44F4yMrIiKiljHkWLDYm4+sfj5XiIrqWomrISIiMi8MORYsyMcJXT0cUFNbjwO/X5W6HCIiIrPCkGPBBEHQrGW16UgWvjt5GYfPF6GunmtaERER2UhdAN0eF7uGf4W/ni/Cr+eLAAAqVzssiQvF6HCVlKURERFJindyLNiu03l4J/73JtvzS6swa2MSdp3Ok6AqIiIi88CQY6Hq6kUs+z4VzT2Yaty27PtUProiIqIOiyHHQiVmFiOvtKrFz0UAeaVVSMwsbr+iiIiIzAhDjoUqKG854BiyHxERkbVhyLFQ3s52Rt2PiIjI2jDkWKiBge5QudpBaOFzAQ2jrAbeXK2ciIioo2HIsVBymYAlcaEA0GLQWRIXCrmspU+JiIisG0OOBRsdrsKq6f3h69r0kdT0uwI4Tw4REXVonAzQwo0OVyEm1BeJmcUoKK/CsYvF2HgkG/vSr2JxbR2UNnKpSyQiIpIE7+RYAblMQHQPD4zv2xmLx4XC21mJyyU38OWxHKlLIyIikgxDjpWxU8jxzD09AQAr959DlbpO4oqIiIikYTYh5/XXX4cgCJgzZ06L+yQkJEAQhCav9PT09ivUAjx0pz/8XO1wpawaG49kSV0OERGRJMwi5Bw7dgwff/wxIiIidNr/7NmzyMvL07zuuOMOE1doWZQ2cvxjZEObrD5wHpU1tRJXRERE1P4kDznXr1/HtGnT8Mknn8DNzU2nY7y9veHr66t5yeXsXHurSQO6IMDdAYXXa7D+EO/mEBFRxyN5yJk9ezbGjRuHe++9V+dj+vXrB5VKhZEjR2L//v0mrM5yKeQy/PPm3ZyPDp5HeZVa4oqIiIjal6RDyLds2YKkpCQcO3ZMp/1VKhU+/vhjDBgwANXV1fj8888xcuRIJCQkYOjQoc0eU11djerqas37srIyAIBarYZabdw//I3nM/Z5DTUu3Bsf7HfEhcIKfHLwPJ4d0UPqkpplbu1mCdhmhmG7GYbtZhi2m/5aazND2lEQRVG87aoMkJOTg8jISMTHx6NPnz4AgOHDh6Nv375YsWKFzueJi4uDIAjYvn17s58vXboUy5Yta7J98+bNcHBwMKh2S5JUKGB9hhx2chEv9auDo0LqioiIiPRXWVmJqVOnorS0FC4uLjodI1nI+fbbbzFhwgSt/jR1dXUQBAEymQzV1dU69bV59dVXsXHjRqSlpTX7eXN3cvz9/VFYWKhzI+lKrVZjz549iImJgUJhHmmivl7E/R8extkr1zFraCDmxphfJ21zbDdzxzYzDNvNMGw3w7Dd9Ndam5WVlcHT01OvkCPZ46qRI0ciJSVFa9tf//pXBAcH41//+pfOnYmTk5OhUrW8fIFSqYRSqWyyXaFQmOyiM+W5DTE3thee+vwE1h/JxhNDe8DDqWl7mANzazdLwDYzDNvNMGw3w7Dd9NdcmxnShpKFHGdnZ4SHh2ttc3R0hIeHh2b7woULcfnyZWzYsAEAsGLFCnTr1g1hYWGoqanBxo0bsW3bNmzbtq3d67cksaE+6N3ZFSmXS7H6wHm8MC5U6pKIiIhMTvLRVa3Jy8tDdna25n1NTQ3mz5+PiIgIDBkyBL/88gt+/PFHTJw4UcIqzZ8gCJgbGwQA2HA4CwVlVRJXREREZHpmtUBnQkKC1vt169ZpvV+wYAEWLFjQfgVZkeFBXugf0AlJ2SX4YP85LBsf3vZBREREFsys7+SQ8QiCgPmxvQAA/0vMweWSGxJXREREZFoMOR3IoJ6euKu7O2rq6rFy3zmpyyEiIjIphpwOZt7Nuzlbj+cgu6hS4mqIiIhMhyGng7mzmzuGBnmhtl7Eez9lSF0OERGRyRgUcnJycnDp0iXN+8TERMyZMwcff/yx0Qoj05kb0zDS6pvkSzhXcF3iaoiIiEzDoJAzdepUzcKY+fn5iImJQWJiIhYtWoSXX37ZqAWS8fX174R7Q3xQL4J3c4iIyGoZFHJOnz6NgQMHAgC+/PJLhIeH49ChQ9i8eXOTYd9knhrv5vzwWy7S88skroaIiMj4DAo5arVas1TC3r17cf/99wMAgoODkZeXZ7zqyGRC/VwwrrcKogi8G38Wh88X4buTl3H4fBHq6iVZzoyIiMioDJoMMCwsDKtXr8a4ceOwZ88evPLKKwCA3NxceHh4GLVAMp05996BH1PyEJ9agPjUAs12lasdlsSFYnR4y2uCERERmTuD7uS8+eab+OijjzB8+HBMmTIFffr0AQBs375d8xiLzN/5q813Os4vrcKsjUnYdZp35YiIyHIZdCdn+PDhKCwsRFlZGdzc3DTbn3zySTg4OBitODKdunoRy75PbfYzEYAAYNn3qYgJ9YVcJrRrbURERMZg0J2cGzduoLq6WhNwsrKysGLFCpw9exbe3t5GLZBMIzGzGHmlLS/UKQLIK61CYmZx+xVFRERkRAaFnPHjx2PDhg0AgJKSEkRFReGdd97BAw88gFWrVhm1QDKNgnLdViLXdT8iIiJzY1DISUpKwpAhQwAAX331FXx8fJCVlYUNGzbg/fffN2qBZBreznZG3Y+IiMjcGBRyKisr4ezsDACIj4/HxIkTIZPJcNdddyErK8uoBZJpDAx0h8rVDq31tlG52mFgoHu71URERGRMBoWcnj174ttvv0VOTg52796N2NhYAEBBQQFcXFyMWiCZhlwmYElcKAC0GHT+NTqYnY6JiMhiGRRyXnrpJcyfPx/dunXDwIEDER0dDaDhrk6/fv2MWiCZzuhwFVZN7w9fV+1HUo25JuFsAUSREwMSEZFlMmgI+eTJkzF48GDk5eVp5sgBgJEjR2LChAlGK45Mb3S4CjGhvkjMLEZBeRW8nRseYU1bcxTfnsxFVHcPTBkYIHWZREREejMo5ACAr68vfH19cenSJQiCgM6dO3MiQAsllwmI7qE9U/X82F54c1c6lmw/g4gurgjzc5WoOiIiIsMY9Liqvr4eL7/8MlxdXdG1a1cEBASgU6dOeOWVV1BfX2/sGkkCTw3tjnuCvVFTW4/Zm5JQVqWWuiQiIiK9GBRyXnjhBaxcuRJvvPEGkpOTkZSUhNdeew3//e9/8eKLLxq7RpKATCbgnb/0QedO9rhYVInnt/3G/jlERGRRDAo569evx6effopZs2YhIiICffr0wdNPP41PPvkE69atM3KJJBU3R1usnNoPCrmAHSn5WH/ootQlERER6cygkFNcXIzg4OAm24ODg1FczGUArEm/ADcsHBMCAHh1RxpO5ZRIWxAREZGODAo5ffr0wcqVK5tsX7lyJSIiIm67KDIvf727G0aH+UJdJ+LpTUkorWT/HCIiMn8Gja566623MG7cOOzduxfR0dEQBAGHDh1CTk4OduzYYewaSWKCIOCtv0QgNa8M2cWVmLf1JD55NBKCwIkCiYjIfBl0J2fYsGH4/fffMWHCBJSUlKC4uBgTJ07EmTNn8Nlnnxm7RjIDLnYKfDitP2zlMuxNK8AnP1+QuiQiIqJWGTxPjp+fH1599VWtbadOncL69euxdu3a2y6MzE94Z1e8FBeKxd+expu7zqJ/gBsiu3FtKyIiMk8G3cmhjmtaVADu7+OHunoRz2xORkFZFQ6fL8J3Jy/j8Pki1NVzmDkREZkHg+/kUMckCAJem9gbp3NLceFqBQa/tR81tX9MAKlytcOSuFCMDldJWCURERHv5JABnJQ2mBrVsJ7VnwMOAOSXVmHWxiTsOp0nRWlEREQaet3JmThxYqufl5SU3E4tZCHq6kWs+Tmz2c9EAAKAZd+nIibUF3IZR2AREZE09Ao5rq6tL9Lo6uqKRx999LYKIvOXmFmMvNKqFj8XAeSVViExs7jJwp9ERETtRa+Qw+HhBAAF5S0HHEP2IyIiMgX2ySG9eTvbGXU/IiIiU2DIIb0NDHSHytUOrfW28XFRYmAg59AhIiLpMOSQ3uQyAUviQgGg1aBTVFHdPgURERE1gyGHDDI6XIVV0/vD11X7kZS3sxJuDgpcKavGtE+OovA6gw4REUmDkwGSwUaHqxAT6ovEzGIUlFfB29kOAwPdcelaJR766AgyCq5j+qdH8b+/3wU3R1upyyUiog6Gd3LotshlAqJ7eGB8386I7uEBuUxAVw9HbP57FLyclUjPL8f0NUdRWqmWulQiIupgGHLIJLp7OeF/f4+Cp5MtzuSW4ZG1R1F6g0GHiIjaD0MOmUxPb2dseuIuuDva4rdLpXjss0SUVzHoEBFR+2DIIZPq5euMjY9HwdVegeTsEvz1s2OoqK6VuiwiIuoAGHLI5EL9XLDx8Sg429ngeNY1/G3dMVTWMOgQEZFpMeRQu+jdxRWfPx4FZ6UNjmYW44n1x1GlrkNdvYijmcU4USjgaGYx6upFqUslIiIrYTYh5/XXX4cgCJgzZ06r+x04cAADBgyAnZ0dunfvjtWrV7dPgXTb+vp3wrq/3QlHWzkOnS/ChA9/xd1v/ITpa49jQ4Yc09cex+A392HX6TypSyUiIitgFiHn2LFj+PjjjxEREdHqfpmZmRg7diyGDBmC5ORkLFq0CP/4xz+wbdu2dqqUbteAru747K8DYSuXIS2vHPll2pMF5pdWYdbGJAYdIiK6bZKHnOvXr2PatGn45JNP4Obm1uq+q1evRkBAAFasWIGQkBA88cQT+Nvf/oa33367naolYxjQ1Q1Ods3PQ9n4sGrZ96l8dEVERLdF8hmPZ8+ejXHjxuHee+/F8uXLW9338OHDiI2N1do2atQorFmzBmq1GgqFoskx1dXVqK7+425BWVkZAECtVkOtNu5w5sbzGfu81uZoZjGKK2pa/FwEkFdahcPnChDFRT6bxWvNMGw3w7DdDMN2019rbWZIO0oacrZs2YKkpCQcO3ZMp/3z8/Ph4+Ojtc3Hxwe1tbUoLCyESqVqcszrr7+OZcuWNdkeHx8PBwcHwwpvw549e0xyXmtxolAAIG9zv/ifj6IojXdzWsNrzTBsN8Ow3QzDdtNfc21WWVmp93kkCzk5OTn45z//ifj4eNjZ2bV9wE2CoL3utSiKzW5vtHDhQsydO1fzvqysDP7+/oiNjYWLi4sBlbdMrVZjz549iImJafauEjXwyCzGhozjbe4XOySKd3JawGvNMGw3w7DdDMN2019rbdb4JEYfkoWcEydOoKCgAAMGDNBsq6urw8GDB7Fy5UpUV1dDLtf+r31fX1/k5+drbSsoKICNjQ08PDya/R6lUgmlUtlku0KhMNlFZ8pzW4Pont5Qudohv7QKLd2ncVTKMbC7FxQ2kncbM2u81gzDdjMM280wbDf9NddmhrShZH9BRo4ciZSUFJw8eVLzioyMxLRp03Dy5MkmAQcAoqOjm9zCio+PR2RkJC8gCyKXCVgSFwoAaP7+G1BRXYfH1x/DtVb67hAREbVGspDj7OyM8PBwrZejoyM8PDwQHh4OoOFR06OPPqo5ZubMmcjKysLcuXORlpaGtWvXYs2aNZg/f75UP4MMNDpchVXT+8PXVftRpcrVDo8PDoS9Qo6fMwpx/we/IDVX/1uUREREko+uak1eXh6ys7M17wMDA7Fjxw4899xz+OCDD+Dn54f3338fkyZNkrBKMtTocBViQn1x+FwB4n8+itghUYju6Q25TMDkAV3w1OcnkF1ciYmrfsWbkyIwvm9nqUsmIiILYlYhJyEhQev9unXrmuwzbNgwJCUltU9BZHJymYCoQHcUpYmICnSHXNbwACtE5YLtz9yNf2w5iYO/X8U/t5xEyqVSPD8mGDZy9tMhIqK28a8Fma1ODrb47LE7MXtEDwDAp79k4pE1iSi63jDvUV29iMPni/Ddycs4fL6IkwcSEZEWs7qTQ3QruUzA/40KRrifK+ZtPYXDF4pw/8pf8Wh0V6w7dBF5pVWafVWudlgSF4rR4U3nSyIioo6Hd3LIIozprcK3s+9GoKcjLpfcwOs707UCDsB1r4iISBtDDlmMIB9nbJs1CMoW5s7huldERPRnDDlkUc7ml6O6tr7FzxvXvUrMLG6/ooiIyCwx5JBFKSivansnPfYjIiLrxZBDFsXbWbd1znTdj4iIrBdDDlmUgYHuULnatbgcBNCwVERuSaVm8VYiIuqYGHLIouiy7pUIYN7W3/DU5ydwtby63WojIiLzwpBDFqe1da8+mNoP82KCoJALiE+9gtj/HMCPv3FIORFRR8TJAMkiNa57lZhZjILyKng722Hgn5aFGBnig3lbTyEtrwyzNydh52kVXh4fDndHW4krJyKi9sKQQxZLLhMQ3cOj2c9C/Vzw3ey7sXJfBj5IOI8ffsvDkQvFeG1COGLDfAE0LAvRUkgiIiLLx5BDVsvWRoa5sb1wb6gP5n15ChkF1/Hk5ycwsV9n3N3TE2/Hn+WyEEREVox9csjqRXTphO+fHYyZw3pAJgBfJ1/GvK2nuCwEEZGVY8ihDsFOIcfzY4Kx5cnoFh9JcVkIIiLrwpBDHUpdvdhqgOGyEERE1oMhhzoULgtBRNRxMORQh6Lrcg+dHBQmroSIiEyNIYc6FF2WhQCAF75Jwf70gnapiYiITIMhhzqU1paFaHzfyV6BS9eq8Nd1x/DU58dxueRGu9ZIRETGwZBDHU5Ly0L4utph9fT++OX5e/Dk0O6QywTsPnMF975zAKsPnEdNbb1m37p6EYfPF+G7k5dx+HwRR2MREZkhTgZIHVJby0IsGhuCSf27YPG3KTh28Rre2JmObScu4ZUHwlFSWYNl36dyIkEiIjPHkEMdVmvLQgBAL19nfPlUNLYlXcbrO9KQUXAdD398pNl9GycSXDW9P4MOEZGZ4OMqolYIgoDJA7pg37zhmBrl3+J+nEiQiMj8MOQQ6cDVQYG4iM6t7sOJBImIzAtDDpGOOJEgEZFlYcgh0pGuEwleq6gxcSVERKQLhhwiHek6keDS71Px6NpEJGdfa5e6iIioeQw5RDrSZSLBwT09YCMTcPD3q5jw4SH8bd0xpFwqbXIuzrNDRGR6HEJOpIfGiQRvnSfH90/z5GQXVeL9fRn4OukS9qUXYF96AWJDffBcTBBCVC7YdTqP8+wQEbUDhhwiPbU1kWCAhwPe/ksfPD28B97/KQPfncpFfOoVxKdeQf+ATkjKLmlyTs6zQ0RkfHxcRWSAxokEx/ftjOgeHpqA82fdvZyw4uF+2PPcUNwX0RBcmgs4AOfZISIyBYYcIhPr6e2MlVP7481JvVvdj/PsEBEZF0MOUTuxU8h12o/z7BARGQdDDlE70XWenZziStTzkRUR0W1jyCFqJ7rOs/N2/O8YteIgvjpxCeq6+iaf19WLOJpZjBOFAo5mFrMPDxFRCxhyiNqJLvPsxIb6wFlpg4yC65i/9RSGvbUfa37JREV1LQBg1+k8DH5zH6avPY4NGXJMX3scg9/ch12n89rvhxARWQgOISdqR7rMs1NWpcamI9lY80smckur8MoPqfjvvgwM6uGJnSl5uPW+DYefExE1jyGHqJ21Nc+Oi50Cs4b3wF/v7oavky7j44PncbGoEjtSmr9bI6LhTtCy71MRE+rb7HB2IqKOiCGHSAKN8+y0xk4hx9SoADx0pz/e/ykD7/2U0eK+fx5+3tZ5iYg6CvbJITJzcpmA7l6OOu3L4edERH9gyCGyALoOP0/OLtF0UiYi6ugYcogsgK7Dz9cduojo13/C6zvSkFtyo9l9uAI6EXUU7JNDZAEah5/P2pgEAdAaYdUYfB660x9HM4uRWViBjw5ewKe/ZGJMuC8eHxyIfgFuAMAV0ImoQ5H0Ts6qVasQEREBFxcXuLi4IDo6Gjt37mxx/4SEBAiC0OSVnp7ejlUTSaNx+Lmvq/ajK19XO6ya3h9vTIrAT3OHYc2MSAzq4YG6ehE//JaHCR8ewsQPf8WrP6Zi1sYkrYAD/DEEnXPtEJG1kfROTpcuXfDGG2+gZ8+eAID169dj/PjxSE5ORlhYWIvHnT17Fi4uLpr3Xl5eJq+VyBw0Dj8/fK4A8T8fReyQKET39NYMG5fJBIwM8cHIEB+k5pZhzS+Z+P5ULpKyS1pdAZ1D0InIGkl6JycuLg5jx45FUFAQgoKC8Oqrr8LJyQlHjhxp9Thvb2/4+vpqXnK5bgsfElkDuUxAVKA7BniKiPrT/Dq3CvVzwTsP9sEvz4/AxH5+rZ6TK6ATkTUymz45dXV12Lp1KyoqKhAdHd3qvv369UNVVRVCQ0OxePFijBgxosV9q6urUV1drXlfVlYGAFCr1VCr1cYp/qbG8xn7vNaO7aY/fdrMzU6OwT098HVybpv75pVUQK12aXM/S8VrzTBsN8Ow3fTXWpsZ0o6CKIqSDq1ISUlBdHQ0qqqq4OTkhM2bN2Ps2LHN7nv27FkcPHgQAwYMQHV1NT7//HOsXr0aCQkJGDp0aLPHLF26FMuWLWuyffPmzXBwcDDqbyEyVxmlAlamtn3Hs79HPe4LqIeHbiPWiYjaTWVlJaZOnYrS0lKtLiutkTzk1NTUIDs7GyUlJdi2bRs+/fRTHDhwAKGhoTodHxcXB0EQsH379mY/b+5Ojr+/PwoLC3VuJF2p1Wrs2bMHMTExUCgURj23NWO76U/fNqurFzH8nYO4UlbdZO2rWwkCMLSnJ6YM7ILhQV5NHofV1Ys4nnUNBeXV8HZWIrKrm8X04+G1Zhi2m2HYbvprrc3Kysrg6empV8iR/HGVra2tpuNxZGQkjh07hvfeew8fffSRTsffdddd2LhxY4ufK5VKKJXKJtsVCoXJLjpTntuasd30p2ubKQAsvT+s1SHoTw4NxJnccvxyrhAHMhpefq52mDIwAA8N9Ie3s53VDEHntWYYtpth2G76a67NDGlDyUPOrURR1Lrz0pbk5GSoVJbzP65EUtFlBXQAyCyswOajWdh64hJyS6vwzp7f8d5PGYjo4trsCC2ugk5E5krSkLNo0SKMGTMG/v7+KC8vx5YtW5CQkIBdu3YBABYuXIjLly9jw4YNAIAVK1agW7duCAsLQ01NDTZu3Iht27Zh27ZtUv4MIovR1groABDo6YgXxoViXmwv7EjJw8YjWRyCTkQWSdKQc+XKFTzyyCPIy8uDq6srIiIisGvXLsTExAAA8vLykJ2drdm/pqYG8+fPx+XLl2Fvb4+wsDD8+OOPLXZUJqKmdFkBHWhYBX1i/y6Y2L8LtiRm4/mvU1rcl6ugE5E5kjTkrFmzptXP161bp/V+wYIFWLBggQkrIqLm2NvqNhdVyuUShhwiMhtm1yeHiMyPrqugv7YjHTtS8vGXyC64L8IPrvbaHQXr6sVWH5URERkTQw4RtalxFfT80qoWh6ArbWRQ19XjZE4JTuaU4OXvUzEqzBd/ieyCQT08sSc13ypGZhGR5WDIIaI26bIK+nsP90X/rm74Nvkyth6/hIyC69h+KhfbT+XCzUGBa5VNZyvlyCwiMiVJ164iIsvR1iroo8NV8Ha2w5NDeyD+uaH4bvbdmH5XAJyV8mYDDvBHWFr2fSrq6iWdl5SIrBDv5BCRznQZgg4AgiCgj38n9PHvhJhQH8xYe6zFc3JkFhGZCkMOEelF1yHojUpauItzqzW/XICXsxI9vZ1a3Icdl4lIHww5RGRSuo7M2ptWgL1pBejj3wmT+3dGXB8/dHKw1XxuLUtKEFH7YcghIpNqa2SWAKCTgwJ9/TvhYEYhTuWU4FROCV75IQ33hnpjUv8uuKGuw7Obk5scz47LRNQahhwiMildRma9PrE3RoercLW8Gt+dvIyvTlxCen45dqTkY0dKPmQCmg1IXFKCiFrD0VVEZHK6jMwCAC9nJZ4Y0h275gzFjn8MweODA+FiZ4PWBl79ueMyEdGf8U4OEbULXUdmNQr1c0GoXyjC/Fww98tTbZ6/oLyqzX2IqGNhyCGidqPvyCwAULna67Tfml8yUVNbj1HhvnCxUzT5vK5exNHMYpwoFOCRWYzont58vEVk5RhyiMis6bKkBAD8dqkU//fVb3jh29MY0csL9/fpjJEh3rBTyG8ZmSXHhozjHJlF1AEw5BCRWdOl4/LS+8NQekON7adyca7gOnafuYLdZ67A0VaOMD8XJF681uS8HJlFZP3Y8ZiIzF5bHZdnDOqGf4y8A3ueG4qd/xyCWcN7oIubPSpq6poNOACXlCDqCHgnh4gsgi4dlwVBQIjKBSEqFywY1QsbDmdhyfYzLZ6TS0oQWTeGHCKyGPp0XBYEAZ0cmnZAbs7b8el4YnB3DOvlBQfb5v9nkUtKEFkehhwislq6LilxIqsEJ7KSoLSRYViQF0aH+2JksA9cb4YkLilBZJkYcojIaumypIS7ky0m9OuM3WfykVN8A/GpVxCfegU2N+8a+bs5YHNidpNj2XGZyPwx5BCR1dJlZNarD4RjdLgKL4wNQVpeOXadyceu03n4/cp1/JxR2OK5uaQEkfnj6Coismq6LikhCAJC/VwwNyYI8c8Nw0/zhuHhO/1bPTeXlCAyb7yTQ0RWr3Fk1uFzBYj/+Shih0S1OeNxDy8nRPfwwJZjOW2ef92hi3C1VyBE5QxBaHpOdlomkgZDDhF1CHKZgKhAdxSliYjSMWTo2nF595l87D6Tj64eDhgd5ovR4b7o06UTZDKBnZaJJMSQQ0TUAl06LrvaKxDZzQ0/ZxQiq6gSHx28gI8OXoCvix1CVM7Yf/Zqk+PYaZmofTDkEBG1QJeOy29M6o3R4SpUVNci4exV7DqTj31pV5BfVoX8suZXRmenZaL2wY7HRESt0LXjsqPSBuMiVPjvlH448WIM/m9UUKvn1afTcl29iMPni/Ddycs4fL6Iy1AQ6Yh3coiI2qDLkhJ/ZqeQo4ubg07nfu6LZIzv2xnDe3kjspsbFHLt//Zknx4iwzHkEBHpQJ8lJQDdOy3nl1Vr+vE4K20w+A5PjOjljeG9vJCUfQ2zNiY16Q/EPj1EumHIISIyAV06LXu7KLFoTAgOZFzFgbNXUVRRg52n87HzdD4AwEYmNHss+/QQ6YYhh4jIBHTptLzs/jCMDldhfL/OqK8XkXK5FPvPFmD/2as4lVOC2lb63nAFdaK2seMxEZGJ6NppGQBkMgF9/Dthzr1B+G723Vj+QLhO33H2Slmrn7PTMnVkvJNDRGRC+nZabtTDy0mn8y/dnoovj13CvaE+iAnxQXhnF82sy+y0TB0dQw4RkYnp22kZaLtPDwDYygWo60Sk5pUhNa8M7/+UAV8XO9wb6o1O9gp8sP88Oy1Th8bHVUREZqixTw/wRx+eRsLN1/s35+R55y99MCbcFw62cuSXVWHjkWysbCbgAH/0DVr2fSofXZHVY8ghIjJTuvTpcXe0xaQBXbBq+gAkvRiDdX+9E/eGeLd6Xn0nIjyaWYwThQKOZhYzGJFF4eMqIiIzpk+fHjuFHMN7eaP0hhp70wraPPfS789gYr/OGHyHJ0J8XSC75ZzafXrk2JBxnH16yKIw5BARmTlTTUR4Nr8cr+9MB3YC7o62GNTDA0Pu8MTdPT1x+nIpJyIki8eQQ0RkZXSZiNDTSYmnhnXHofNFOHKhCMUVNfjhtzz88FsegIZgxYkIydIx5BARWRldJiJ85YGGiQifGNIdNbX1OHWpBD9nFOLXc4VIzr7Wat8bTkRIloIdj4mIrJA+ExHa2shwZzd3zI0JwrZZg/D6xN46fcf3py6joKyqxc85ESFJjXdyiIislKETEQa4O+p0/s2JOdicmIMQlQuGBXlhWJAXBnR1g62NjBMRkllgyCEismKmmojQSWmD7p4OSMktQ1pew2v1gfNwtJWjh7cTfrtU2uQYdlqm9sbHVUREpEWXiQjf/ksEtj87BMdfuBfvPdwXE/t1hqeTLSpq6poNOAAnIqT2x5BDRERN6Nqnx8NJifF9O+Pdh/oicdG9eG1C6wuLNnZa/jrpEkSx9aDDPj10uyR9XLVq1SqsWrUKFy9eBACEhYXhpZdewpgxY1o85sCBA5g7dy7OnDkDPz8/LFiwADNnzmyniomIOo7GPj2HzxUg/uejiB0Sheie3i326ZHJBDgqdfuz8n9f/YZ39/yOoXd4YVgvL9zdwxOuDgrN5+zTQ8Ygacjp0qUL3njjDfTs2RMAsH79eowfPx7JyckICwtrsn9mZibGjh2Lv//979i4cSN+/fVXPP300/Dy8sKkSZPau3wiIqsnlwmICnRHUZqIKB06Les6EaFCJiCvtApfHM/BF8dzIBOAfgFuGHqHF2xtZHhrVzonIqTbJmnIiYuL03r/6quvYtWqVThy5EizIWf16tUICAjAihUrAAAhISE4fvw43n77bYYcIiIzoMtEhL6udtg7dxiOZ13DgbNXcTDjKs4VXMeJrGs4kXWtxXNzIkLSl9mMrqqrq8PWrVtRUVGB6OjoZvc5fPgwYmNjtbaNGjUKa9asgVqthkKhaHJMdXU1qqurNe/LysoAAGq1Gmq12oi/AJrzGfu81o7tpj+2mWHYbobRt91eGNMLz2451eJEhC+M6QVbmYhBgZ0wKLATFo6+A7klN/DzuSJ8ezIXx7NKWjx3Y5+ew+cKEBXo3moddfUijmddQ0F5NbydlYjs6tauwYjXm/5aazND2lEQ2+r5ZWIpKSmIjo5GVVUVnJycsHnzZowdO7bZfYOCgvDYY49h0aJFmm2HDh3C3XffjdzcXKhUTW9fLl26FMuWLWuyffPmzXBwcDDeDyEiIo1TRQK+vihDSc0foaKTrYiJ3erRx6PlPzsnCgVsyJC3eX5nGxFh7iJ6ujS83JTG+X4yX5WVlZg6dSpKS0vh4uKi0zGS38np1asXTp48iZKSEmzbtg0zZszAgQMHEBoa2uz+gqCdwhsz2q3bGy1cuBBz587VvC8rK4O/vz9iY2N1biRdqdVq7NmzBzExMc3eVaLmsd30xzYzDNvNMIa021gACwy4k+KRWYwNGcfbPH95rYAjBQKO3FxsvYubPaIC3TCwmxuqa+vx2eG0Jo/LSmsEfPa7HP99uA9Ghfno9DtuB683/bXWZo1PYvQhecixtbXVdDyOjIzEsWPH8N577+Gjjz5qsq+vry/y8/O1thUUFMDGxgYeHs1PdqVUKqFUKptsVygUJrvoTHlua8Z20x/bzDBsN8Po224KAIOD9AsT0T292+zT4+OixKsP9EbixWIcySzG6culuHTtBi5du4FtSbktnruxT8+rO89iTETnVgNXXb2o90zRLeH1pr/m2syQNpQ85NxKFEWtPjR/Fh0dje+//15rW3x8PCIjI3kBERFZAV0WF116fxhGhvpgZGhDgLpeXYvjF4tx5EIx9qbl41xBRYvn/2Nx0SJE9/Bsdh8OX7cekk4GuGjRIvz888+4ePEiUlJS8MILLyAhIQHTpk0D0PCo6dFHH9XsP3PmTGRlZWHu3LlIS0vD2rVrsWbNGsyfP1+qn0BEREamz+KiQMMSE8N7eeP5McF49p47dPqOJ9Yfx5MbjuPTny8g5VIpauvqATQEnFkbk7QCDvDH8PVdp/Nu45dRe5P0Ts6VK1fwyCOPIC8vD66uroiIiMCuXbsQExMDAMjLy0N2drZm/8DAQOzYsQPPPfccPvjgA/j5+eH999/n8HEiIitj6OKius7TU1FTh/jUK4hPvQKgISj1D+iEpOySZh+Tcfi6ZZI05KxZs6bVz9etW9dk27Bhw5CUlGSiioiIyFyYYnHRxnl63p/SDyeyriExsxjHLhajvKoWBzMKWz33H4+6itusq65exNHMYpwoFOCRWdzqTNFkOmbXJ4eIiMhQuvTpWRIXiju7uePObu6YOawH6upFpOeXYc0vmfg66XKb3/HruUL0C+gEO0XzQ921+/TIsSHjOPv0SIQLdBIRkVXRt0+PXCYgzM8Vfxngr9P5V+4/h4hl8Xj448NYsfd3HL1QhOraOgDs02NueCeHiIisjiF9etp61AUAdgoZnJQ2KLxegyMXGkZ0rUAGlDYy9PPvhNO5ZezTY0YYcoiIyCrp26dHl0ddKx7qi1FhvrhQWIEjF4pw5EIxDp8vQuH1ahzJLG71/Lr26THmHD0dHUMOERHRTY2Pum6dJ8f3lj41Pbyc0MPLCdOiukIURZy/WoGPDp7H1uOX2vyOX89dbbFPD+foMS6GHCIioj/R91GXIAjo6e2Eif266BRyVu4/j48PZqKvfydEdXdHVKAH+nfthIO/X8WsjUlNHnc19udprj8RtY4hh4iI6BamGL4ONPTpcbGzQUF5DRIvFiPxYjH+i3OQC4BMJrA/j5Ex5BARERmBPn16sooqceRCEY5mFuPohSLkllahrq7l1dH1naOHfXoaMOQQEREZia59erp5OqKbpyMeHhgAURSx7teLWPZDapvn/8/e35FV1BmR3dzRw8sRgqAdXtinRxtDDhERkRE19uk5fK4A8T8fReyQqFZnPBYEAcEqF53OnZhZjMSbo7jcHBQY0NUdkd3ccGc3N+SW3MA//neSfXr+hCGHiIjIyOQyAVGB7ihKExGlw+MiXZajcHO0xUN3dkFSVglO5pTgWqUae9OuYG/alVbPrU+fHmt71MWQQ0REJDFd+vO8NiFccyemprYeZ3JLcfziNRzPapirp6yqtsXz/9GnpwjRPTyb3ccaH3VxWQciIiIzoM9yFLY2MvQLcMPfh3bHR49E4pXx4Tp9x8yNSXhmcxLWH7qI1Nwy1NU3xClrXY6Cd3KIiIjMhCHLUQCAt4tdq583Kr2hxg+/5eGH3xpCi7PSBv0COiEpu8Qqh68z5BAREZkRU8zRIwDwcbHDO3/pg6TsaziWdQ1JWddQXl2LgxmFrZ7bkoevM+QQERFZOF369Cy9PxR33+GJu+9o6JNTVy8iPb8Ma3/JxLaky21+x5fHc2CnkCHUzwVKG8tYkoIhh4iIyAroOkdPI7lMQJifKyYP8Ncp5HyTfBnfJF+GrY0M4X4u6Bfghv4BbujftRNO5ZSY5ZIUDDlERERWwpA+PbosR+FsZ4M7u7rh5KVSFFfUICm7BEnZJViDTACATIBZ9ulhyCEiIrIi+vbp0eVR178nR2B0uAqiKCKrqBJJ2deQnF2CpOxrSMsrQ33LK1Lo1afH2BhyiIiIOjhdH3UJgqBZkmJi/y4AgK3Hc/B/X/3W5ncUlFe1uY+xMeQQERGRwcPXu7g56HR+b2fdhrkbE0MOERERATDd8HVf14bA1N444zEREREZrLFPD/BHH55Gje+XxIVKMl8OQw4RERHdFn2WpGhPfFxFREREt83QPj2mxJBDRERERmFInx5T4uMqIiIiskoMOURERGSVGHKIiIjIKjHkEBERkVViyCEiIiKrxJBDREREVokhh4iIiKwSQw4RERFZJYYcIiIiskodbsZjUWxYI7WsrMzo51ar1aisrERZWRkUCoXRz2+t2G76Y5sZhu1mGLabYdhu+mutzRr/bjf+HddFhws55eXlAAB/f3+JKyEiIiJ9lZeXw9XVVad9BVGfSGQF6uvrkZubC2dnZwiCcRcNKysrg7+/P3JycuDi4mLUc1sztpv+2GaGYbsZhu1mGLab/lprM1EUUV5eDj8/P8hkuvW26XB3cmQyGbp06WLS73BxceEFbQC2m/7YZoZhuxmG7WYYtpv+WmozXe/gNGLHYyIiIrJKDDlERERklRhyjEipVGLJkiVQKpVSl2JR2G76Y5sZhu1mGLabYdhu+jN2m3W4jsdERETUMfBODhEREVklhhwiIiKySgw5REREZJUYcoiIiMgqMeQYyYcffojAwEDY2dlhwIAB+Pnnn6UuyawtXboUgiBovXx9faUuy+wcPHgQcXFx8PPzgyAI+Pbbb7U+F0URS5cuhZ+fH+zt7TF8+HCcOXNGmmLNSFvt9thjjzW5/u666y5pijUTr7/+Ou688044OzvD29sbDzzwAM6ePau1D6+3pnRpN15vTa1atQoRERGaSf+io6Oxc+dOzefGutYYcozgiy++wJw5c/DCCy8gOTkZQ4YMwZgxY5CdnS11aWYtLCwMeXl5mldKSorUJZmdiooK9OnTBytXrmz287feegvvvvsuVq5ciWPHjsHX1xcxMTGaNdo6qrbaDQBGjx6tdf3t2LGjHSs0PwcOHMDs2bNx5MgR7NmzB7W1tYiNjUVFRYVmH15vTenSbgCvt1t16dIFb7zxBo4fP47jx4/jnnvuwfjx4zVBxmjXmki3beDAgeLMmTO1tgUHB4vPP/+8RBWZvyVLloh9+vSRugyLAkD85ptvNO/r6+tFX19f8Y033tBsq6qqEl1dXcXVq1dLUKF5urXdRFEUZ8yYIY4fP16SeixFQUGBCEA8cOCAKIq83nR1a7uJIq83Xbm5uYmffvqpUa813sm5TTU1NThx4gRiY2O1tsfGxuLQoUMSVWUZMjIy4Ofnh8DAQDz88MO4cOGC1CVZlMzMTOTn52tde0qlEsOGDeO1p4OEhAR4e3sjKCgIf//731FQUCB1SWaltLQUAODu7g6A15uubm23RrzeWlZXV4ctW7agoqIC0dHRRr3WGHJuU2FhIerq6uDj46O13cfHB/n5+RJVZf6ioqKwYcMG7N69G5988gny8/MxaNAgFBUVSV2axWi8vnjt6W/MmDHYtGkT9u3bh3feeQfHjh3DPffcg+rqaqlLMwuiKGLu3LkYPHgwwsPDAfB600Vz7QbwemtJSkoKnJycoFQqMXPmTHzzzTcIDQ016rXW4VYhNxVBELTei6LYZBv9YcyYMZp/7t27N6Kjo9GjRw+sX78ec+fOlbAyy8NrT38PPfSQ5p/Dw8MRGRmJrl274scff8TEiRMlrMw8PPPMM/jtt9/wyy+/NPmM11vLWmo3Xm/N69WrF06ePImSkhJs27YNM2bMwIEDBzSfG+Na452c2+Tp6Qm5XN4kXRYUFDRJodQyR0dH9O7dGxkZGVKXYjEaR6Px2rt9KpUKXbt25fUH4Nlnn8X27duxf/9+dOnSRbOd11vrWmq35vB6a2Bra4uePXsiMjISr7/+Ovr06YP33nvPqNcaQ85tsrW1xYABA7Bnzx6t7Xv27MGgQYMkqsryVFdXIy0tDSqVSupSLEZgYCB8fX21rr2amhocOHCA156eioqKkJOT06GvP1EU8cwzz+Drr7/Gvn37EBgYqPU5r7fmtdVuzeH11jxRFFFdXW3ca81InaI7tC1btogKhUJcs2aNmJqaKs6ZM0d0dHQUL168KHVpZmvevHliQkKCeOHCBfHIkSPifffdJzo7O7PNblFeXi4mJyeLycnJIgDx3XffFZOTk8WsrCxRFEXxjTfeEF1dXcWvv/5aTElJEadMmSKqVCqxrKxM4sql1Vq7lZeXi/PmzRMPHTokZmZmivv37xejo6PFzp07d+h2mzVrlujq6iomJCSIeXl5mldlZaVmH15vTbXVbrzemrdw4ULx4MGDYmZmpvjbb7+JixYtEmUymRgfHy+KovGuNYYcI/nggw/Erl27ira2tmL//v21hg9SUw899JCoUqlEhUIh+vn5iRMnThTPnDkjdVlmZ//+/SKAJq8ZM2aIotgwrHfJkiWir6+vqFQqxaFDh4opKSnSFm0GWmu3yspKMTY2VvTy8hIVCoUYEBAgzpgxQ8zOzpa6bEk1114AxM8++0yzD6+3ptpqN15vzfvb3/6m+Zvp5eUljhw5UhNwRNF415ogiqJo4J0lIiIiIrPFPjlERERklRhyiIiIyCox5BAREZFVYsghIiIiq8SQQ0RERFaJIYeIiIisEkMOERERWSWGHCIiNCwG+O2330pdBhEZEUMOEUnusccegyAITV6jR4+WujQismA2UhdARAQAo0ePxmeffaa1TalUSlQNEVkD3skhIrOgVCrh6+ur9XJzcwPQ8Chp1apVGDNmDOzt7REYGIitW7dqHZ+SkoJ77rkH9vb28PDwwJNPPonr169r7bN27VqEhYVBqVRCpVLhmWee0fq8sLAQEyZMgIODA+644w5s377dtD+aiEyKIYeILMKLL76ISZMm4dSpU5g+fTqmTJmCtLQ0AEBlZSVGjx4NNzc3HDt2DFu3bsXevXu1QsyqVaswe/ZsPPnkk0hJScH27dvRs2dPre9YtmwZHnzwQfz2228YO3Yspk2bhuLi4nb9nURkRMZbU5SIyDAzZswQ5XK56OjoqPV6+eWXRVFsWOl55syZWsdERUWJs2bNEkVRFD/++GPRzc1NvH79uubzH3/8UZTJZGJ+fr4oiqLo5+cnvvDCCy3WAEBcvHix5v3169dFQRDEnTt3Gu13ElH7Yp8cIjILI0aMwKpVq7S2ubu7a/45Ojpa67Po6GicPHkSAJCWloY+ffrA0dFR8/ndd9+N+vp6nD17FoIgIDc3FyNHjmy1hoiICM0/Ozo6wtnZGQUFBYb+JCKSGEMOEZkFR0fHJo+P2iIIAgBAFEXNPze3j729vU7nUygUTY6tr6/XqyYiMh/sk0NEFuHIkSNN3gcHBwMAQkNDcfLkSVRUVGg+//XXXyGTyRAUFARnZ2d069YNP/30U7vWTETS4p0cIjIL1dXVyM/P19pmY2MDT09PAMDWrVsRGRmJwYMHY9OmTUhMTMSaNWsAANOmTcOSJUswY8YMLF26FFevXsWzzz6LRx55BD4+PgCApUuXYubMmfD29saYMWNQXl6OX3/9Fc8++2z7/lAiajcMOURkFnbt2gWVSqW1rVevXkhPTwfQMPJpy5YtePrpp+Hr64tNmzYhNDQUAODg4IDdu3fjn//8J+688044ODhg0qRJePfddzXnmjFjBqqqqvCf//wH8+fPh6enJyZPntx+P5CI2p0giqIodRFERK0RBAHffPMNHnjgAalLISILwj45REREZJUYcoiIiMgqsU8OEZk9PlUnIkPwTg4RERFZJYYcIiIiskoMOURERGSVGHKIiIjIKjHkEBERkVViyCEiIiKrxJBDREREVokhh4iIiKwSQw4RERFZpf8HH22YEYh/q80AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aamir\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU Score: 0.88\n",
      "ROUGE-1: 0.00\n",
      "ROUGE-2: 0.00\n",
      "ROUGE-L: 0.00\n",
      "\n",
      "Model saved as 'transformer_translation_model.pth'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# 1. Load Tokenizers\n",
    "tokenizer_en = spm.SentencePieceProcessor(model_file='bpe_en.model')\n",
    "tokenizer_ur = spm.SentencePieceProcessor(model_file='bpe_ur.model')\n",
    "\n",
    "# 2. Dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_lines, tgt_lines, src_tokenizer, tgt_tokenizer, max_len=128):\n",
    "        self.src_lines = src_lines\n",
    "        self.tgt_lines = tgt_lines\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = [self.src_tokenizer.bos_id()] + self.src_tokenizer.encode(self.src_lines[idx], out_type=int) + [self.src_tokenizer.eos_id()]\n",
    "        tgt = [self.tgt_tokenizer.bos_id()] + self.tgt_tokenizer.encode(self.tgt_lines[idx], out_type=int) + [self.tgt_tokenizer.eos_id()]\n",
    "        \n",
    "        src = src[:self.max_len]\n",
    "        tgt = tgt[:self.max_len]\n",
    "\n",
    "        src += [0] * (self.max_len - len(src))\n",
    "        tgt += [0] * (self.max_len - len(tgt))\n",
    "        \n",
    "        return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n",
    "\n",
    "# 3. Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, nhead=8, num_layers=4, dim_feedforward=512, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, 5000, d_model), requires_grad=True)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.src_tok_emb(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        tgt_emb = self.tgt_tok_emb(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
    "\n",
    "        src_padding_mask = (src == 0)\n",
    "        tgt_padding_mask = (tgt == 0)\n",
    "\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n",
    "\n",
    "        output = self.transformer(\n",
    "            src_emb,\n",
    "            tgt_emb,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=tgt_mask\n",
    "        )\n",
    "\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# 4. Data (assuming 'data' dictionary is loaded correctly)\n",
    "train_src = data['quran']['train.en']\n",
    "train_tgt = data['quran']['train.ur']\n",
    "test_src = data['quran']['test.en']\n",
    "test_tgt = data['quran']['test.ur']\n",
    "\n",
    "train_dataset = TranslationDataset(train_src, train_tgt, tokenizer_en, tokenizer_ur)\n",
    "test_dataset = TranslationDataset(test_src, test_tgt, tokenizer_en, tokenizer_ur)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# 5. Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TransformerModel(\n",
    "    tokenizer_en.get_piece_size(),\n",
    "    tokenizer_ur.get_piece_size()\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "# 6. Training loop\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    print(f\"\\nEpoch {epoch+1} Starting...\")\n",
    "\n",
    "    for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input)\n",
    "        output = output.reshape(-1, output.shape[-1])\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "\n",
    "        loss = criterion(output, tgt_output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Batch {batch_idx+1}/{len(train_loader)} | Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1} Finished | Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 7. Visualize loss\n",
    "plt.plot(train_losses, marker='o')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 8. Evaluation (BLEU + ROUGE)\n",
    "model.eval()\n",
    "references = []\n",
    "hypotheses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for src, tgt in test_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        output = model(src, tgt_input)\n",
    "        output = output.argmax(dim=-1)\n",
    "\n",
    "        for i in range(output.size(0)):\n",
    "            pred_tokens = output[i].cpu().numpy().tolist()\n",
    "            true_tokens = tgt[i, 1:].cpu().numpy().tolist()\n",
    "\n",
    "            # Filter padding\n",
    "            pred_tokens = [tok for tok in pred_tokens if tok != 0]\n",
    "            true_tokens = [tok for tok in true_tokens if tok != 0]\n",
    "\n",
    "            pred_sentence = tokenizer_ur.decode(pred_tokens)\n",
    "            true_sentence = tokenizer_ur.decode(true_tokens)\n",
    "\n",
    "            hypotheses.append(pred_sentence.split())\n",
    "            references.append([true_sentence.split()])\n",
    "\n",
    "# BLEU score\n",
    "smoothie = SmoothingFunction().method4\n",
    "bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
    "print(f\"\\nBLEU Score: {bleu_score*100:.2f}\")\n",
    "\n",
    "# ROUGE score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge1_list, rouge2_list, rougel_list = [], [], []\n",
    "\n",
    "for ref, hyp in zip(references, hypotheses):\n",
    "    ref_sent = ' '.join(ref[0])\n",
    "    hyp_sent = ' '.join(hyp)\n",
    "    scores = scorer.score(ref_sent, hyp_sent)\n",
    "\n",
    "    rouge1_list.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_list.append(scores['rouge2'].fmeasure)\n",
    "    rougel_list.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "print(f\"ROUGE-1: {np.mean(rouge1_list)*100:.2f}\")\n",
    "print(f\"ROUGE-2: {np.mean(rouge2_list)*100:.2f}\")\n",
    "print(f\"ROUGE-L: {np.mean(rougel_list)*100:.2f}\")\n",
    "\n",
    "# 9. Save the model\n",
    "torch.save(model.state_dict(), \"transformer_translation_model.pth\")\n",
    "print(\"\\nModel saved as 'transformer_translation_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da7fa5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b321d088",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c64abd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13400 English training samples and 13400 Urdu training samples.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Base data folder\n",
    "base_path = 'C:\\\\Users\\\\aamir\\\\Downloads\\\\umc005-corpus'\n",
    "\n",
    "# Quran and Bible subfolders\n",
    "quran_folder = os.path.join(base_path, 'quran')\n",
    "bible_folder = os.path.join(base_path, 'bible')\n",
    "\n",
    "# Files to load\n",
    "quran_files = ['Quran-EN', 'Quran-UR', 'Quran-UR-normalized', 'test.ur', 'test.en', 'train.ur', 'train.en', 'dev.ur', 'dev.en']\n",
    "bible_files = ['Bible-EN', 'Bible-UR', 'Bible-EN-normalized', 'test.ur', 'test.en', 'train.ur', 'train.en', 'dev.ur', 'dev.en']\n",
    "\n",
    "# Function to load text from files\n",
    "def load_files(folder, filenames):\n",
    "    data = {}\n",
    "    for fname in filenames:\n",
    "        full_path = os.path.join(folder, fname)\n",
    "        if os.path.isfile(full_path):\n",
    "            with open(full_path, encoding='utf-8') as f:\n",
    "                data[fname] = f.read().splitlines()\n",
    "    return data\n",
    "\n",
    "# Load Quran and Bible data\n",
    "quran_data = load_files(quran_folder, quran_files)\n",
    "bible_data = load_files(bible_folder, bible_files)\n",
    "\n",
    "# Merge relevant files (example: train.en and train.ur from both Quran and Bible)\n",
    "train_en = quran_data.get('train.en', []) + bible_data.get('train.en', [])\n",
    "train_ur = quran_data.get('train.ur', []) + bible_data.get('train.ur', [])\n",
    "\n",
    "print(f\"Loaded {len(train_en)} English training samples and {len(train_ur)} Urdu training samples.\")\n",
    "\n",
    "# Basic text cleaning\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Zآ-ے0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "train_en = [clean_text(line) for line in train_en]\n",
    "train_ur = [clean_text(line) for line in train_ur]\n",
    "\n",
    "assert len(train_en) == len(train_ur), \"Mismatch between English and Urdu lines!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc3afe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13400 English training samples and 13400 Urdu training samples.\n",
      "Loaded 457 English test samples and 457 Urdu test samples.\n",
      "Loaded 13400 English training samples and 13400 Urdu training samples.\n",
      "Loaded 457 English test samples and 457 Urdu test samples.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Base data folder\n",
    "base_path = 'C:\\\\Users\\\\aamir\\\\Downloads\\\\umc005-corpus'\n",
    "\n",
    "# Quran and Bible subfolders\n",
    "quran_folder = os.path.join(base_path, 'quran')\n",
    "bible_folder = os.path.join(base_path, 'bible')\n",
    "\n",
    "# Files to load\n",
    "quran_files = ['Quran-EN', 'Quran-UR', 'Quran-UR-normalized', 'test.ur', 'test.en', 'train.ur', 'train.en', 'dev.ur', 'dev.en']\n",
    "bible_files = ['Bible-EN', 'Bible-UR', 'Bible-EN-normalized', 'test.ur', 'test.en', 'train.ur', 'train.en', 'dev.ur', 'dev.en']\n",
    "\n",
    "# Function to load text from files\n",
    "def load_files(folder, filenames):\n",
    "    data = {}\n",
    "    for fname in filenames:\n",
    "        full_path = os.path.join(folder, fname)\n",
    "        if os.path.isfile(full_path):\n",
    "            with open(full_path, encoding='utf-8') as f:\n",
    "                data[fname] = f.read().splitlines()\n",
    "    return data\n",
    "\n",
    "# Load Quran and Bible data\n",
    "quran_data = load_files(quran_folder, quran_files)\n",
    "bible_data = load_files(bible_folder, bible_files)\n",
    "\n",
    "# Merge relevant files (example: train.en and train.ur from both Quran and Bible)\n",
    "train_en = quran_data.get('train.en', []) + bible_data.get('train.en', [])\n",
    "train_ur = quran_data.get('train.ur', []) + bible_data.get('train.ur', [])\n",
    "\n",
    "test_en = quran_data.get('test.en', []) + bible_data.get('test.en', [])\n",
    "test_ur = quran_data.get('test.ur', []) + bible_data.get('test.ur', [])\n",
    "\n",
    "print(f\"Loaded {len(train_en)} English training samples and {len(train_ur)} Urdu training samples.\")\n",
    "print(f\"Loaded {len(test_en)} English test samples and {len(test_ur)} Urdu test samples.\")\n",
    "\n",
    "# 1. Tokenizers\n",
    "def en_tokenizer(text):\n",
    "    return text.split()  # Simple whitespace-based tokenization for English\n",
    "\n",
    "def ur_tokenizer(text):\n",
    "    return text.split()  # Simple whitespace-based tokenization for Urdu\n",
    "\n",
    "# 2. Build Vocabulary manually\n",
    "def build_vocab(sentences, tokenizer, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer(sentence)\n",
    "        counter.update(tokens)\n",
    "    # Filter tokens by min_freq and add special tokens\n",
    "    vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "    idx = 4\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    return vocab\n",
    "\n",
    "# 3. Encode Sentence\n",
    "def encode_sentence(sentence, vocab, tokenizer, max_len=30):\n",
    "    tokens = tokenizer(sentence)\n",
    "    tokens = ['<sos>'] + tokens[:max_len-2] + ['<eos>']\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "\n",
    "# 4. Pad Sequences\n",
    "def pad_sequence(seq, max_len, pad_idx):\n",
    "    return seq + [pad_idx] * (max_len - len(seq))\n",
    "\n",
    "# Prepare vocab\n",
    "en_vocab = build_vocab(train_en, en_tokenizer)\n",
    "ur_vocab = build_vocab(train_ur, ur_tokenizer)\n",
    "\n",
    "# Add padding token index\n",
    "pad_idx = ur_vocab[\"<pad>\"]\n",
    "\n",
    "# Dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src, tgt, src_vocab, tgt_vocab, src_tok, tgt_tok, max_len=30):\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.src_tok = src_tok\n",
    "        self.tgt_tok = tgt_tok\n",
    "        self.max_len = max_len\n",
    "        self.pad_idx = tgt_vocab['<pad>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Encode source and target sentences\n",
    "        src_seq = encode_sentence(self.src[idx], self.src_vocab, self.src_tok, self.max_len)\n",
    "        tgt_seq = encode_sentence(self.tgt[idx], self.tgt_vocab, self.tgt_tok, self.max_len)\n",
    "        # Pad sequences to ensure same length in a batch\n",
    "        src_seq = pad_sequence(src_seq, self.max_len, self.src_vocab['<pad>'])\n",
    "        tgt_seq = pad_sequence(tgt_seq, self.max_len, self.pad_idx)\n",
    "        return torch.tensor(src_seq), torch.tensor(tgt_seq)\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = TranslationDataset(train_en, train_ur, en_vocab, ur_vocab, en_tokenizer, ur_tokenizer)\n",
    "test_dataset = TranslationDataset(test_en, test_ur, en_vocab, ur_vocab, en_tokenizer, ur_tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "print(f\"Loaded {len(train_en)} English training samples and {len(train_ur)} Urdu training samples.\")\n",
    "print(f\"Loaded {len(test_en)} English test samples and {len(test_ur)} Urdu test samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a782fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Encoder with Dropout\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "# Decoder with Dropout\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(1)\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        predictions = self.fc(outputs.squeeze(1))\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features).to(DEVICE)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = tgt[:, 0]\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = tgt[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a0bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 10/210, Loss: 7.4192\n",
      "Epoch 1, Step 20/210, Loss: 6.4645\n",
      "Epoch 1, Step 30/210, Loss: 6.2476\n",
      "Epoch 1, Step 40/210, Loss: 6.3129\n",
      "Epoch 1, Step 50/210, Loss: 6.2007\n",
      "Epoch 1, Step 60/210, Loss: 6.0758\n",
      "Epoch 1, Step 70/210, Loss: 6.2062\n",
      "Epoch 1, Step 80/210, Loss: 6.1899\n",
      "Epoch 1, Step 90/210, Loss: 6.1429\n",
      "Epoch 1, Step 100/210, Loss: 6.0560\n",
      "Epoch 1, Step 110/210, Loss: 6.1122\n",
      "Epoch 1, Step 120/210, Loss: 6.0801\n",
      "Epoch 1, Step 130/210, Loss: 6.0714\n",
      "Epoch 1, Step 140/210, Loss: 6.0444\n",
      "Epoch 1, Step 150/210, Loss: 6.0051\n",
      "Epoch 1, Step 160/210, Loss: 6.0948\n",
      "Epoch 1, Step 170/210, Loss: 6.0094\n",
      "Epoch 1, Step 180/210, Loss: 6.1192\n",
      "Epoch 1, Step 190/210, Loss: 6.0260\n",
      "Epoch 1, Step 200/210, Loss: 6.0642\n",
      "Epoch 1, Step 210/210, Loss: 6.0246\n",
      "\n",
      "Epoch 1\n",
      "Train Loss: 6.2508\n",
      "BLEU: 0.9661, ROUGE-1 -> Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Model saved.\n",
      "Epoch 2, Step 10/210, Loss: 6.0382\n",
      "Epoch 2, Step 20/210, Loss: 5.9462\n",
      "Epoch 2, Step 30/210, Loss: 5.9942\n",
      "Epoch 2, Step 40/210, Loss: 6.0528\n",
      "Epoch 2, Step 50/210, Loss: 6.0702\n",
      "Epoch 2, Step 60/210, Loss: 5.9751\n",
      "Epoch 2, Step 70/210, Loss: 6.0098\n",
      "Epoch 2, Step 80/210, Loss: 6.0645\n",
      "Epoch 2, Step 90/210, Loss: 6.0588\n",
      "Epoch 2, Step 100/210, Loss: 5.9833\n",
      "Epoch 2, Step 110/210, Loss: 5.9322\n",
      "Epoch 2, Step 120/210, Loss: 6.1166\n",
      "Epoch 2, Step 130/210, Loss: 5.9997\n",
      "Epoch 2, Step 140/210, Loss: 5.9926\n",
      "Epoch 2, Step 150/210, Loss: 5.9677\n",
      "Epoch 2, Step 160/210, Loss: 5.9301\n",
      "Epoch 2, Step 170/210, Loss: 5.9224\n",
      "Epoch 2, Step 180/210, Loss: 5.9817\n",
      "Epoch 2, Step 190/210, Loss: 5.9073\n",
      "Epoch 2, Step 200/210, Loss: 5.9275\n",
      "Epoch 2, Step 210/210, Loss: 5.9942\n",
      "\n",
      "Epoch 2\n",
      "Train Loss: 5.9899\n",
      "BLEU: 0.9661, ROUGE-1 -> Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 3, Step 10/210, Loss: 5.9019\n",
      "Epoch 3, Step 20/210, Loss: 5.9390\n",
      "Epoch 3, Step 30/210, Loss: 5.8627\n",
      "Epoch 3, Step 40/210, Loss: 5.9246\n",
      "Epoch 3, Step 50/210, Loss: 5.8833\n",
      "Epoch 3, Step 60/210, Loss: 5.9778\n",
      "Epoch 3, Step 70/210, Loss: 5.8784\n",
      "Epoch 3, Step 80/210, Loss: 5.8376\n",
      "Epoch 3, Step 90/210, Loss: 5.8586\n",
      "Epoch 3, Step 100/210, Loss: 5.8631\n",
      "Epoch 3, Step 110/210, Loss: 5.9343\n",
      "Epoch 3, Step 120/210, Loss: 5.8454\n",
      "Epoch 3, Step 130/210, Loss: 5.8877\n",
      "Epoch 3, Step 140/210, Loss: 5.8180\n",
      "Epoch 3, Step 150/210, Loss: 5.8193\n",
      "Epoch 3, Step 160/210, Loss: 5.8747\n",
      "Epoch 3, Step 170/210, Loss: 5.9437\n",
      "Epoch 3, Step 180/210, Loss: 5.9866\n",
      "Epoch 3, Step 190/210, Loss: 5.8759\n",
      "Epoch 3, Step 200/210, Loss: 5.8574\n",
      "Epoch 3, Step 210/210, Loss: 5.9731\n",
      "\n",
      "Epoch 3\n",
      "Train Loss: 5.8960\n",
      "BLEU: 0.9661, ROUGE-1 -> Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 4, Step 10/210, Loss: 5.8557\n",
      "Epoch 4, Step 20/210, Loss: 5.7569\n",
      "Epoch 4, Step 30/210, Loss: 5.8303\n",
      "Epoch 4, Step 40/210, Loss: 5.7407\n",
      "Epoch 4, Step 50/210, Loss: 5.8892\n",
      "Epoch 4, Step 60/210, Loss: 5.8272\n",
      "Epoch 4, Step 70/210, Loss: 5.7128\n",
      "Epoch 4, Step 80/210, Loss: 5.8271\n",
      "Epoch 4, Step 90/210, Loss: 5.7141\n",
      "Epoch 4, Step 100/210, Loss: 5.7977\n",
      "Epoch 4, Step 110/210, Loss: 5.7413\n",
      "Epoch 4, Step 120/210, Loss: 5.7863\n",
      "Epoch 4, Step 130/210, Loss: 5.8160\n",
      "Epoch 4, Step 140/210, Loss: 5.7767\n",
      "Epoch 4, Step 150/210, Loss: 5.8497\n",
      "Epoch 4, Step 160/210, Loss: 5.7504\n",
      "Epoch 4, Step 170/210, Loss: 5.7655\n",
      "Epoch 4, Step 180/210, Loss: 5.7400\n",
      "Epoch 4, Step 190/210, Loss: 5.7795\n",
      "Epoch 4, Step 200/210, Loss: 5.7697\n",
      "Epoch 4, Step 210/210, Loss: 5.6742\n",
      "\n",
      "Epoch 4\n",
      "Train Loss: 5.7770\n",
      "BLEU: 0.9661, ROUGE-1 -> Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 5, Step 10/210, Loss: 5.6667\n",
      "Epoch 5, Step 20/210, Loss: 5.8155\n",
      "Epoch 5, Step 30/210, Loss: 5.6944\n",
      "Epoch 5, Step 40/210, Loss: 5.7577\n",
      "Epoch 5, Step 50/210, Loss: 5.5925\n",
      "Epoch 5, Step 60/210, Loss: 5.6511\n",
      "Epoch 5, Step 70/210, Loss: 5.7643\n",
      "Epoch 5, Step 80/210, Loss: 5.7620\n",
      "Epoch 5, Step 90/210, Loss: 5.7411\n",
      "Epoch 5, Step 100/210, Loss: 5.7009\n",
      "Epoch 5, Step 110/210, Loss: 5.7240\n",
      "Epoch 5, Step 120/210, Loss: 5.6317\n",
      "Epoch 5, Step 130/210, Loss: 5.7330\n",
      "Epoch 5, Step 140/210, Loss: 5.6335\n",
      "Epoch 5, Step 150/210, Loss: 5.6532\n",
      "Epoch 5, Step 160/210, Loss: 5.7075\n",
      "Epoch 5, Step 170/210, Loss: 5.6885\n",
      "Epoch 5, Step 180/210, Loss: 5.8028\n",
      "Epoch 5, Step 190/210, Loss: 5.7132\n",
      "Epoch 5, Step 200/210, Loss: 5.5325\n",
      "Epoch 5, Step 210/210, Loss: 5.8793\n",
      "\n",
      "Epoch 5\n",
      "Train Loss: 5.6819\n",
      "BLEU: 0.9661, ROUGE-1 -> Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 6, Step 10/210, Loss: 5.5280\n",
      "Epoch 6, Step 20/210, Loss: 5.5970\n",
      "Epoch 6, Step 30/210, Loss: 5.5176\n",
      "Epoch 6, Step 40/210, Loss: 5.6002\n",
      "Epoch 6, Step 50/210, Loss: 5.5857\n",
      "Epoch 6, Step 60/210, Loss: 5.7261\n",
      "Epoch 6, Step 70/210, Loss: 5.6121\n",
      "Epoch 6, Step 80/210, Loss: 5.6333\n",
      "Epoch 6, Step 90/210, Loss: 5.6544\n",
      "Epoch 6, Step 100/210, Loss: 5.5685\n",
      "Epoch 6, Step 110/210, Loss: 5.6254\n",
      "Epoch 6, Step 120/210, Loss: 5.6595\n",
      "Epoch 6, Step 130/210, Loss: 5.5216\n",
      "Epoch 6, Step 140/210, Loss: 5.6599\n",
      "Epoch 6, Step 150/210, Loss: 5.5573\n",
      "Epoch 6, Step 160/210, Loss: 5.4800\n",
      "Epoch 6, Step 170/210, Loss: 5.6112\n",
      "Epoch 6, Step 180/210, Loss: 5.6554\n",
      "Epoch 6, Step 190/210, Loss: 5.4725\n",
      "Epoch 6, Step 200/210, Loss: 5.5612\n",
      "Epoch 6, Step 210/210, Loss: 5.6320\n",
      "\n",
      "Epoch 6\n",
      "Train Loss: 5.5904\n",
      "BLEU: 0.9661, ROUGE-1 -> Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Early stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "learning_rate = 5e-4\n",
    "epochs = 20\n",
    "clip = 1.0\n",
    "\n",
    "# Instantiate model\n",
    "encoder = Encoder(len(en_vocab), embedding_dim, hidden_dim, num_layers, dropout).to(DEVICE)\n",
    "decoder = Decoder(len(ur_vocab), embedding_dim, hidden_dim, num_layers, dropout).to(DEVICE)\n",
    "model = Seq2Seq(encoder, decoder).to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=ur_vocab['<pad>'])\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, data_loader, tgt_vocab):\n",
    "    model.eval()\n",
    "    predictions, references = [], []\n",
    "    rouge = Rouge()\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data_loader:\n",
    "            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            for i in range(src.size(0)):\n",
    "                pred_tokens = output[i].argmax(1).cpu().numpy()\n",
    "                pred_sentence = [tgt_vocab.get(word, '<unk>') for word in pred_tokens]\n",
    "                pred_sentence = ' '.join(pred_sentence).replace('<eos>', '').strip()\n",
    "                ref_sentence = [tgt_vocab.get(word, '<unk>') for word in tgt[i].cpu().numpy()]\n",
    "                ref_sentence = ' '.join(ref_sentence).replace('<eos>', '').strip()\n",
    "                predictions.append(pred_sentence.split())\n",
    "                references.append([ref_sentence.split()])\n",
    "\n",
    "    bleu_score = np.mean([sentence_bleu(ref, pred) for ref, pred in zip(references, predictions)])\n",
    "    rouge_score = rouge.get_scores([' '.join(pred) for pred in predictions],\n",
    "                                   [' '.join(ref[0]) for ref in references], avg=True)\n",
    "    return bleu_score, rouge_score['rouge-1']['p'], rouge_score['rouge-1']['r'], rouge_score['rouge-1']['f']\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "best_bleu = 0\n",
    "patience = 5\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])\n",
    "\n",
    "        output = output.contiguous().view(-1, output.shape[-1])\n",
    "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Step {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    bleu, rouge_p, rouge_r, rouge_f = evaluate(model, test_loader, ur_vocab)\n",
    "    print(f\"\\nEpoch {epoch+1}\")\n",
    "    print(f\"Train Loss: {avg_loss:.4f}\")\n",
    "    print(f\"BLEU: {bleu:.4f}, ROUGE-1 -> Precision: {rouge_p:.4f}, Recall: {rouge_r:.4f}, F1: {rouge_f:.4f}\")\n",
    "\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_lstm_model.pth\")\n",
    "        print(\"Model saved.\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "# Plot\n",
    "plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8682373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f9065e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a0099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a626b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d77d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a3fd4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c345c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5159d2b1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
